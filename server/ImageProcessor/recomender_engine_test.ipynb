{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7195b1a9",
   "metadata": {},
   "source": [
    "# Code To test recomender engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea5315",
   "metadata": {},
   "source": [
    "## Feature Extraction Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631bb520",
   "metadata": {},
   "source": [
    "### Load Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b18a9",
   "metadata": {},
   "source": [
    "#### Load configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b21b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bda834",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load Configurations ####\n",
    "import json\n",
    "\n",
    "# Load configuration from JSON file\n",
    "def load_config(config_file='imageconfig.json'):\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "# Example of loading and using the configuration\n",
    "config = load_config()\n",
    "\n",
    "vision_uri = config['AzureVisionService']['Uri']\n",
    "vision_key = config['AzureVisionService']['Key']\n",
    "\n",
    "\n",
    "print(f\"Vision URI: {vision_uri}\")\n",
    "print(f\"Vision Key: {vision_key}\")\n",
    "\n",
    "azure_openai_uri = config['AzureOpenAIService']['Uri']\n",
    "azure_openai_key = config['AzureOpenAIService']['Key']\n",
    "\n",
    "\n",
    "print(f\"azure open URI: {azure_openai_uri}\")\n",
    "print(f\"azure open Key: {azure_openai_key}\")\n",
    "\n",
    "# Load OpnAI configuration\n",
    "openai_key = config['OpenAIService']['Key']\n",
    "\n",
    "print(f\"OpenAI Key: {openai_key}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11d6d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Function to read image from path and analyze using Azure ImageAnalysisClient\n",
    "def analyze_image(image_path):\n",
    "    # Read image as bytes\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_data = image_file.read()\n",
    "\n",
    "    # Create ImageAnalysisClient\n",
    "    client = ImageAnalysisClient(\n",
    "        endpoint=vision_uri,\n",
    "        credential=AzureKeyCredential(vision_key)\n",
    "    )\n",
    "\n",
    "    # Analyze image (example: describe)\n",
    "       # visual_features=[\"Description\"]\n",
    "    result = client.analyze(\n",
    "        image_data=image_data,\n",
    "    \n",
    "       visual_features =[\n",
    "        VisualFeatures.TAGS,\n",
    "        VisualFeatures.OBJECTS,\n",
    "        VisualFeatures.CAPTION,\n",
    "        VisualFeatures.DENSE_CAPTIONS,\n",
    "        VisualFeatures.READ,\n",
    "       # VisualFeatures.SMART_CROPS,\n",
    "        VisualFeatures.PEOPLE,\n",
    "    ]\n",
    "    )\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cae3d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\dinesh_image_trails\\men_grey_suit.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a suit with a tie', 'confidence': 0.7532808184623718}, 'denseCaptionsResult': {'values': [{'text': 'a suit with a tie', 'confidence': 0.7532808184623718, 'boundingBox': {'x': 0, 'y': 0, 'w': 600, 'h': 700}}, {'text': 'a suit with a tie', 'confidence': 0.7210917472839355, 'boundingBox': {'x': 69, 'y': 15, 'w': 440, 'h': 669}}, {'text': 'a close up of a tie', 'confidence': 0.860759437084198, 'boundingBox': {'x': 267, 'y': 106, 'w': 62, 'h': 213}}]}, 'metadata': {'width': 600, 'height': 700}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9997585415840149}, {'name': 'coat', 'confidence': 0.9702774286270142}, {'name': 'collar', 'confidence': 0.9564415216445923}, {'name': 'outerwear', 'confidence': 0.9557503461837769}, {'name': 'blazer', 'confidence': 0.9508758187294006}, {'name': 'button', 'confidence': 0.9468828439712524}, {'name': 'person', 'confidence': 0.9405521154403687}, {'name': 'formal wear', 'confidence': 0.8835344314575195}, {'name': 'pocket', 'confidence': 0.8797683715820312}, {'name': 'shirt', 'confidence': 0.843208372592926}, {'name': 'wearing', 'confidence': 0.6651028990745544}, {'name': 'suit', 'confidence': 0.6221878528594971}, {'name': 'fabric', 'confidence': 0.5588575005531311}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 263, 'y': 102, 'w': 72, 'h': 225}, 'tags': [{'name': 'tie', 'confidence': 0.638}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 78, 'y': 10, 'w': 445, 'h': 689}, 'confidence': 0.9236836433410645}, {'boundingBox': {'x': 190, 'y': 263, 'w': 247, 'h': 175}, 'confidence': 0.001563498517498374}]}}\n",
      "Caption: a suit with a tie (Confidence: 0.7532808184623718)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9997585415840149)\n",
      "- coat (Confidence: 0.9702774286270142)\n",
      "- collar (Confidence: 0.9564415216445923)\n",
      "- outerwear (Confidence: 0.9557503461837769)\n",
      "- blazer (Confidence: 0.9508758187294006)\n",
      "- button (Confidence: 0.9468828439712524)\n",
      "- person (Confidence: 0.9405521154403687)\n",
      "- formal wear (Confidence: 0.8835344314575195)\n",
      "- pocket (Confidence: 0.8797683715820312)\n",
      "- shirt (Confidence: 0.843208372592926)\n",
      "- wearing (Confidence: 0.6651028990745544)\n",
      "- suit (Confidence: 0.6221878528594971)\n",
      "- fabric (Confidence: 0.5588575005531311)\n",
      "Dense Captions:\n",
      "- a suit with a tie (Confidence: 0.7532808184623718)\n",
      "- a suit with a tie (Confidence: 0.7210917472839355)\n",
      "- a close up of a tie (Confidence: 0.860759437084198)\n",
      "People:\n",
      "- BoundingBox: {'x': 78, 'y': 10, 'w': 445, 'h': 689}, Confidence: 0.9236836433410645\n",
      "- BoundingBox: {'x': 190, 'y': 263, 'w': 247, 'h': 175}, Confidence: 0.001563498517498374\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# All tag extraction good except the category tag as per indian context\n",
    "#image_path = r\"D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 1.jpg\"\n",
    "image_path = r\"D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\dinesh_image_trails\\men_grey_suit.jpg\"\n",
    "\n",
    "print(f\"Analyzing image: {image_path}\")\n",
    "result = analyze_image(image_path=image_path)\n",
    "print(result)\n",
    "# Print caption\n",
    "if 'captionResult' in result:\n",
    "    caption = result['captionResult']\n",
    "    print(f\"Caption: {caption['text']} (Confidence: {caption['confidence']})\")\n",
    "\n",
    "# Print tags\n",
    "if 'tagsResult' in result and 'values' in result['tagsResult']:\n",
    "    print(\"Tags:\")\n",
    "    for tag in result['tagsResult']['values']:\n",
    "        print(f\"- {tag['name']} (Confidence: {tag['confidence']})\")\n",
    "\n",
    "# Print dense captions\n",
    "if 'denseCaptionsResult' in result and 'values' in result['denseCaptionsResult']:\n",
    "    print(\"Dense Captions:\")\n",
    "    for dense_caption in result['denseCaptionsResult']['values']:\n",
    "        print(f\"- {dense_caption['text']} (Confidence: {dense_caption['confidence']})\")\n",
    "\n",
    "# Print people\n",
    "if 'peopleResult' in result and 'values' in result['peopleResult']:\n",
    "    print(\"People:\")\n",
    "    for person in result['peopleResult']['values']:\n",
    "        bbox = person['boundingBox']\n",
    "        print(f\"- BoundingBox: {bbox}, Confidence: {person['confidence']}\")\n",
    "# Generate a caption for the image\n",
    "# if result.caption:\n",
    "#     print(f\"Caption: {result.caption.text} (Confidence: {result.caption.confidence})\")\n",
    "# # Print tags\n",
    "# if result.tags:\n",
    "#     print(\"Tags:\")\n",
    "#     for tag in result.tags:\n",
    "#         print(f\"- {tag.name} (Confidence: {tag.confidence})\")\n",
    "# # Print objects detected in the image\n",
    "# if result.objects:\n",
    "#     print(\"Objects:\")\n",
    "#     for obj in result.objects:\n",
    "#         print(f\"- {obj.object_property} (Confidence: {obj.confidence})\")\n",
    "# # Print dense captions\n",
    "# if result.dense_captions:\n",
    "#     print(\"Dense Captions:\")\n",
    "#     for dense_caption in result.dense_captions:\n",
    "#         print(f\"- {dense_caption.text} (Confidence: {dense_caption.confidence})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "479b2400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images from the specified directory\n",
    "def read_images_from_directory(directory):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "\n",
    "    image_files = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "    return [join(directory, f) for f in image_files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a817cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through images in a directory and analyze each one and extract tags\n",
    "def analyze_images_in_directory(directory):\n",
    "    image_paths = read_images_from_directory(directory)\n",
    "    for image_path in image_paths:\n",
    "        print(f\"Analyzing image: {image_path}\")\n",
    "        result = analyze_image(image_path=image_path)\n",
    "        print(result)\n",
    "        # Print caption\n",
    "        if 'captionResult' in result:\n",
    "            caption = result['captionResult']\n",
    "            print(f\"Caption: {caption['text']} (Confidence: {caption['confidence']})\")\n",
    "\n",
    "        # Print tags\n",
    "        if 'tagsResult' in result and 'values' in result['tagsResult']:\n",
    "            print(\"Tags:\")\n",
    "            for tag in result['tagsResult']['values']:\n",
    "                print(f\"- {tag['name']} (Confidence: {tag['confidence']})\")\n",
    "\n",
    "        if 'denseCaptionsResult' in result and 'values' in result['denseCaptionsResult']:\n",
    "            print(\"Dense Captions:\")\n",
    "            for dense_caption in result['denseCaptionsResult']['values']:\n",
    "                print(f\"- {dense_caption['text']} (Confidence: {dense_caption['confidence']})\")               "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5c782d",
   "metadata": {},
   "source": [
    "### Tag extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd53633",
   "metadata": {},
   "source": [
    "### Code Snippet to rename iamges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d3648b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F 1.jpg to D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F_1.jpg\n",
      "Renamed D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F 10.jpg to D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F_10.jpg\n",
      "Renamed D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F 11.jpg to D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F_11.jpg\n",
      "Renamed D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F 12.jpg to D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F_12.jpg\n",
      "Renamed D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F 13.jpg to D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F_13.jpg\n",
      "Renamed D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F 14.jpg to D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F_14.jpg\n",
      "Renamed D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F 2.jpg to D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F_2.jpg\n",
      "Renamed D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F 3.jpg to D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F_3.jpg\n",
      "Renamed D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F 4.jpg to D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F_4.jpg\n",
      "Renamed D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F 5.jpg to D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F_5.jpg\n",
      "Renamed D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F 6.jpg to D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F_6.jpg\n",
      "Renamed D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F 7.jpg to D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F_7.jpg\n",
      "Renamed D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F 8.jpg to D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F_8.jpg\n",
      "Renamed D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F 9.jpg to D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\F_9.jpg\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# ### Rename the image name M 9.jpg as M_9.jpg\n",
    "# def rename_image_file(image_path):    \n",
    "#     base, ext = os.path.splitext(image_path)\n",
    "#     new_name = base.replace(\" \", \"_\") + ext\n",
    "#     os.rename(image_path, new_name)\n",
    "#     print(f\"Renamed {image_path} to {new_name}\")\n",
    "#     return new_name\n",
    "# image_collection = read_images_from_directory(r\"D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\")\n",
    "# for image_path in image_collection:\n",
    "#     renamed_image_path = rename_image_file(image_path)\n",
    "    #analyze_image(renamed_image_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5fe00a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Error processing Backup: [Errno 13] Permission denied: 'D:\\\\Dinesh\\\\Tech\\\\GitHub\\\\virtual-assistant\\\\images\\\\American\\\\resized_images\\\\women\\\\Run1\\\\Backup'\n",
      "✅ Resized: F_1.jpg\n",
      "✅ Resized: F_10.jpg\n",
      "✅ Resized: F_11.jpg\n",
      "✅ Resized: F_12.jpg\n",
      "✅ Resized: F_13.png\n",
      "✅ Resized: F_14.png\n",
      "✅ Resized: F_15.jpg\n",
      "✅ Resized: F_16.jpg\n",
      "✅ Resized: F_17.jpg\n",
      "✅ Resized: F_18.png\n",
      "✅ Resized: F_2.jpg\n",
      "✅ Resized: F_3.png\n",
      "✅ Resized: F_4.png\n",
      "✅ Resized: F_5.png\n",
      "✅ Resized: F_6.png\n",
      "✅ Resized: F_7.png\n",
      "✅ Resized: F_8.png\n",
      "✅ Resized: F_9.png\n",
      "⚠️ Error processing resized_images: [Errno 13] Permission denied: 'D:\\\\Dinesh\\\\Tech\\\\GitHub\\\\virtual-assistant\\\\images\\\\American\\\\resized_images\\\\women\\\\Run1\\\\resized_images'\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "input_folder = r\"D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\"  # use consistent plural naming\n",
    "output_base = \"resized_images\"   # use consistent plural naming\n",
    "\n",
    "\n",
    "\n",
    "# Target size (width, height)\n",
    "target_size = (400, 800)\n",
    "\n",
    "# Loop through both categories\n",
    "\n",
    "output_folder = os.path.join(input_folder,output_base)\n",
    "\n",
    "# Check if input folder exists\n",
    "if not os.path.exists(input_folder):\n",
    "    print(f\"❌ Folder not found: {input_folder}\")\n",
    "    exit(1)\n",
    "\n",
    "# Create the output folder if it doesn't exist\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# Process each image in the folder\n",
    "for filename in os.listdir(input_folder):\n",
    "    file_path = os.path.join(input_folder, filename)\n",
    "\n",
    "    try:\n",
    "        with Image.open(file_path) as img:\n",
    "            img = img.resize(target_size)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            img.save(output_path)\n",
    "            print(f\"✅ Resized: {filename}\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error processing {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f438bd27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_1.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a blue dress', 'confidence': 0.8804439902305603}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a blue dress', 'confidence': 0.8804439902305603, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a blue dress', 'confidence': 0.8130084872245789, 'boundingBox': {'x': 93, 'y': 25, 'w': 159, 'h': 757}}, {'text': 'a blurry image of a plant', 'confidence': 0.8195161819458008, 'boundingBox': {'x': 32, 'y': 164, 'w': 84, 'h': 228}}, {'text': 'a blurry image of a plant', 'confidence': 0.7601820826530457, 'boundingBox': {'x': 340, 'y': 165, 'w': 57, 'h': 254}}, {'text': 'a woman wearing a blue jacket', 'confidence': 0.7569583654403687, 'boundingBox': {'x': 143, 'y': 35, 'w': 123, 'h': 177}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9992886781692505}, {'name': 'person', 'confidence': 0.9831652641296387}, {'name': 'fashion', 'confidence': 0.971450686454773}, {'name': 'fashion design', 'confidence': 0.954893946647644}, {'name': 'fashion model', 'confidence': 0.9480499029159546}, {'name': 'day dress', 'confidence': 0.9210357666015625}, {'name': 'shoulder', 'confidence': 0.9145808219909668}, {'name': 'high heels', 'confidence': 0.9068188667297363}, {'name': 'footwear', 'confidence': 0.8796181678771973}, {'name': 'pattern (fashion design)', 'confidence': 0.8651552796363831}, {'name': 'fashion show', 'confidence': 0.8567705750465393}, {'name': 'lady', 'confidence': 0.8438880443572998}, {'name': 'haute couture', 'confidence': 0.8418262004852295}, {'name': 'indoor', 'confidence': 0.8003585934638977}, {'name': 'woman', 'confidence': 0.7872868776321411}, {'name': 'fabric', 'confidence': 0.7778888940811157}, {'name': 'blue', 'confidence': 0.7557581067085266}, {'name': 'wall', 'confidence': 0.7519873380661011}, {'name': 'floor', 'confidence': 0.692084550857544}, {'name': 'dress', 'confidence': 0.681490421295166}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 95, 'y': 35, 'w': 173, 'h': 760}, 'confidence': 0.19061504304409027}, {'boundingBox': {'x': 113, 'y': 302, 'w': 146, 'h': 201}, 'confidence': 0.0010549298021942377}]}}\n",
      "Caption: a woman in a blue dress (Confidence: 0.8804439902305603)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9992886781692505)\n",
      "- person (Confidence: 0.9831652641296387)\n",
      "- fashion (Confidence: 0.971450686454773)\n",
      "- fashion design (Confidence: 0.954893946647644)\n",
      "- fashion model (Confidence: 0.9480499029159546)\n",
      "- day dress (Confidence: 0.9210357666015625)\n",
      "- shoulder (Confidence: 0.9145808219909668)\n",
      "- high heels (Confidence: 0.9068188667297363)\n",
      "- footwear (Confidence: 0.8796181678771973)\n",
      "- pattern (fashion design) (Confidence: 0.8651552796363831)\n",
      "- fashion show (Confidence: 0.8567705750465393)\n",
      "- lady (Confidence: 0.8438880443572998)\n",
      "- haute couture (Confidence: 0.8418262004852295)\n",
      "- indoor (Confidence: 0.8003585934638977)\n",
      "- woman (Confidence: 0.7872868776321411)\n",
      "- fabric (Confidence: 0.7778888940811157)\n",
      "- blue (Confidence: 0.7557581067085266)\n",
      "- wall (Confidence: 0.7519873380661011)\n",
      "- floor (Confidence: 0.692084550857544)\n",
      "- dress (Confidence: 0.681490421295166)\n",
      "Dense Captions:\n",
      "- a woman in a blue dress (Confidence: 0.8804439902305603)\n",
      "- a woman wearing a blue dress (Confidence: 0.8130084872245789)\n",
      "- a blurry image of a plant (Confidence: 0.8195161819458008)\n",
      "- a blurry image of a plant (Confidence: 0.7601820826530457)\n",
      "- a woman wearing a blue jacket (Confidence: 0.7569583654403687)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_10.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a red dress', 'confidence': 0.8708924055099487}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a red dress', 'confidence': 0.8708924055099487, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman in a red dress holding a purse', 'confidence': 0.8180763125419617, 'boundingBox': {'x': 93, 'y': 92, 'w': 175, 'h': 694}}, {'text': 'a close up of a bag', 'confidence': 0.7533065676689148, 'boundingBox': {'x': 168, 'y': 400, 'w': 58, 'h': 82}}, {'text': 'a close-up of a christmas tree', 'confidence': 0.8652535080909729, 'boundingBox': {'x': 0, 'y': 3, 'w': 138, 'h': 525}}, {'text': \"a close up of a woman's feet\", 'confidence': 0.7676438093185425, 'boundingBox': {'x': 181, 'y': 699, 'w': 57, 'h': 98}}, {'text': 'a close-up of a handbag', 'confidence': 0.8259629607200623, 'boundingBox': {'x': 160, 'y': 354, 'w': 78, 'h': 138}}, {'text': 'a woman with a pearl earring', 'confidence': 0.7156115174293518, 'boundingBox': {'x': 171, 'y': 103, 'w': 74, 'h': 106}}, {'text': \"a close up of a person's hand\", 'confidence': 0.7566566467285156, 'boundingBox': {'x': 189, 'y': 348, 'w': 65, 'h': 66}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'christmas tree', 'confidence': 0.9876245260238647}, {'name': 'clothing', 'confidence': 0.9666926264762878}, {'name': 'person', 'confidence': 0.9358536005020142}, {'name': 'christmas', 'confidence': 0.9012216329574585}, {'name': 'fashion', 'confidence': 0.8790127635002136}, {'name': 'woman', 'confidence': 0.8110399842262268}, {'name': 'dress', 'confidence': 0.7397493124008179}, {'name': 'red', 'confidence': 0.733142614364624}, {'name': 'indoor', 'confidence': 0.7316449284553528}, {'name': 'floor', 'confidence': 0.6470171809196472}, {'name': 'standing', 'confidence': 0.6257574558258057}, {'name': 'plant', 'confidence': 0.6064904928207397}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 99, 'y': 107, 'w': 183, 'h': 691}, 'confidence': 0.9205418825149536}, {'boundingBox': {'x': 107, 'y': 372, 'w': 166, 'h': 199}, 'confidence': 0.0026920188684016466}]}}\n",
      "Caption: a woman in a red dress (Confidence: 0.8708924055099487)\n",
      "Tags:\n",
      "- christmas tree (Confidence: 0.9876245260238647)\n",
      "- clothing (Confidence: 0.9666926264762878)\n",
      "- person (Confidence: 0.9358536005020142)\n",
      "- christmas (Confidence: 0.9012216329574585)\n",
      "- fashion (Confidence: 0.8790127635002136)\n",
      "- woman (Confidence: 0.8110399842262268)\n",
      "- dress (Confidence: 0.7397493124008179)\n",
      "- red (Confidence: 0.733142614364624)\n",
      "- indoor (Confidence: 0.7316449284553528)\n",
      "- floor (Confidence: 0.6470171809196472)\n",
      "- standing (Confidence: 0.6257574558258057)\n",
      "- plant (Confidence: 0.6064904928207397)\n",
      "Dense Captions:\n",
      "- a woman in a red dress (Confidence: 0.8708924055099487)\n",
      "- a woman in a red dress holding a purse (Confidence: 0.8180763125419617)\n",
      "- a close up of a bag (Confidence: 0.7533065676689148)\n",
      "- a close-up of a christmas tree (Confidence: 0.8652535080909729)\n",
      "- a close up of a woman's feet (Confidence: 0.7676438093185425)\n",
      "- a close-up of a handbag (Confidence: 0.8259629607200623)\n",
      "- a woman with a pearl earring (Confidence: 0.7156115174293518)\n",
      "- a close up of a person's hand (Confidence: 0.7566566467285156)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_11.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman standing in front of a white wall', 'confidence': 0.8714901804924011}, 'denseCaptionsResult': {'values': [{'text': 'a woman standing in front of a white wall', 'confidence': 0.8714901804924011, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a blue shirt and white shorts', 'confidence': 0.7695636749267578, 'boundingBox': {'x': 141, 'y': 190, 'w': 145, 'h': 540}}, {'text': \"a close up of a woman's waist\", 'confidence': 0.754126787185669, 'boundingBox': {'x': 157, 'y': 380, 'w': 98, 'h': 121}}, {'text': 'a woman with a hand in her pocket', 'confidence': 0.7491388916969299, 'boundingBox': {'x': 139, 'y': 263, 'w': 142, 'h': 233}}, {'text': \"a person's feet with brown toenails\", 'confidence': 0.7098212838172913, 'boundingBox': {'x': 160, 'y': 657, 'w': 69, 'h': 101}}, {'text': 'a woman with long hair', 'confidence': 0.779208779335022, 'boundingBox': {'x': 164, 'y': 181, 'w': 85, 'h': 141}}, {'text': 'a woman standing in front of a white wall', 'confidence': 0.8556535243988037, 'boundingBox': {'x': 0, 'y': 0, 'w': 388, 'h': 640}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9937949776649475}, {'name': 'person', 'confidence': 0.9839099645614624}, {'name': 'wall', 'confidence': 0.9832321405410767}, {'name': 'human face', 'confidence': 0.9432544112205505}, {'name': 'shoulder', 'confidence': 0.9355468153953552}, {'name': 'footwear', 'confidence': 0.9237754344940186}, {'name': 'fashion', 'confidence': 0.923584520816803}, {'name': 'waist', 'confidence': 0.9215418696403503}, {'name': 'sandal', 'confidence': 0.9211342334747314}, {'name': 'fashion design', 'confidence': 0.9154194593429565}, {'name': 'day dress', 'confidence': 0.9056682586669922}, {'name': 'high heels', 'confidence': 0.8966624140739441}, {'name': 'blouse', 'confidence': 0.8761774301528931}, {'name': 'indoor', 'confidence': 0.8684285879135132}, {'name': 'fashion model', 'confidence': 0.8636276125907898}, {'name': 'joint', 'confidence': 0.8589683771133423}, {'name': 'casual dress', 'confidence': 0.8409060835838318}, {'name': 'woman', 'confidence': 0.7831000685691833}, {'name': 'standing', 'confidence': 0.7315313816070557}, {'name': 'dress', 'confidence': 0.6000102758407593}, {'name': 'wearing', 'confidence': 0.5385799407958984}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 165, 'y': 382, 'w': 100, 'h': 124}, 'tags': [{'name': 'Miniskirt', 'confidence': 0.568}]}, {'boundingBox': {'x': 142, 'y': 193, 'w': 143, 'h': 500}, 'tags': [{'name': 'person', 'confidence': 0.691}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 141, 'y': 184, 'w': 149, 'h': 575}, 'confidence': 0.9463921189308167}, {'boundingBox': {'x': 151, 'y': 375, 'w': 139, 'h': 206}, 'confidence': 0.0014841316733509302}]}}\n",
      "Caption: a woman standing in front of a white wall (Confidence: 0.8714901804924011)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9937949776649475)\n",
      "- person (Confidence: 0.9839099645614624)\n",
      "- wall (Confidence: 0.9832321405410767)\n",
      "- human face (Confidence: 0.9432544112205505)\n",
      "- shoulder (Confidence: 0.9355468153953552)\n",
      "- footwear (Confidence: 0.9237754344940186)\n",
      "- fashion (Confidence: 0.923584520816803)\n",
      "- waist (Confidence: 0.9215418696403503)\n",
      "- sandal (Confidence: 0.9211342334747314)\n",
      "- fashion design (Confidence: 0.9154194593429565)\n",
      "- day dress (Confidence: 0.9056682586669922)\n",
      "- high heels (Confidence: 0.8966624140739441)\n",
      "- blouse (Confidence: 0.8761774301528931)\n",
      "- indoor (Confidence: 0.8684285879135132)\n",
      "- fashion model (Confidence: 0.8636276125907898)\n",
      "- joint (Confidence: 0.8589683771133423)\n",
      "- casual dress (Confidence: 0.8409060835838318)\n",
      "- woman (Confidence: 0.7831000685691833)\n",
      "- standing (Confidence: 0.7315313816070557)\n",
      "- dress (Confidence: 0.6000102758407593)\n",
      "- wearing (Confidence: 0.5385799407958984)\n",
      "Dense Captions:\n",
      "- a woman standing in front of a white wall (Confidence: 0.8714901804924011)\n",
      "- a woman wearing a blue shirt and white shorts (Confidence: 0.7695636749267578)\n",
      "- a close up of a woman's waist (Confidence: 0.754126787185669)\n",
      "- a woman with a hand in her pocket (Confidence: 0.7491388916969299)\n",
      "- a person's feet with brown toenails (Confidence: 0.7098212838172913)\n",
      "- a woman with long hair (Confidence: 0.779208779335022)\n",
      "- a woman standing in front of a white wall (Confidence: 0.8556535243988037)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_12.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman wearing sunglasses and a skirt', 'confidence': 0.7680467367172241}, 'denseCaptionsResult': {'values': [{'text': 'a woman wearing sunglasses and a skirt', 'confidence': 0.7680467367172241, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman in a skirt and a green shirt', 'confidence': 0.7352137565612793, 'boundingBox': {'x': 122, 'y': 183, 'w': 213, 'h': 604}}, {'text': 'a close up of a white skirt', 'confidence': 0.7458731532096863, 'boundingBox': {'x': 150, 'y': 422, 'w': 99, 'h': 176}}, {'text': 'a woman with long hair wearing sunglasses', 'confidence': 0.6929612159729004, 'boundingBox': {'x': 152, 'y': 190, 'w': 83, 'h': 158}}, {'text': 'a woman wearing a green top and white skirt', 'confidence': 0.7896580100059509, 'boundingBox': {'x': 124, 'y': 297, 'w': 178, 'h': 284}}, {'text': 'a close-up of a plant', 'confidence': 0.7646339535713196, 'boundingBox': {'x': 56, 'y': 374, 'w': 83, 'h': 130}}, {'text': 'a close up of a metal object', 'confidence': 0.6877428889274597, 'boundingBox': {'x': 299, 'y': 0, 'w': 95, 'h': 781}}, {'text': 'a woman wearing sunglasses and a skirt', 'confidence': 0.7600184082984924, 'boundingBox': {'x': 0, 'y': 0, 'w': 385, 'h': 790}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9925472140312195}, {'name': 'person', 'confidence': 0.9593212604522705}, {'name': 'outdoor', 'confidence': 0.9295374751091003}, {'name': 'building', 'confidence': 0.9152774214744568}, {'name': 'woman', 'confidence': 0.9083626866340637}, {'name': 'fashion accessory', 'confidence': 0.901688277721405}, {'name': 'footwear', 'confidence': 0.8918161392211914}, {'name': 'fashion', 'confidence': 0.8619606494903564}, {'name': 'waist', 'confidence': 0.861922025680542}, {'name': 'lady', 'confidence': 0.8465598821640015}, {'name': 'window', 'confidence': 0.7353109121322632}, {'name': 'girl', 'confidence': 0.6054539680480957}, {'name': 'standing', 'confidence': 0.5948991179466248}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': [{'lines': [{'text': '1/6', 'boundingPolygon': [{'x': 357, 'y': 21}, {'x': 380, 'y': 22}, {'x': 380, 'y': 44}, {'x': 356, 'y': 44}], 'words': [{'text': '1/6', 'boundingPolygon': [{'x': 356, 'y': 21}, {'x': 377, 'y': 21}, {'x': 377, 'y': 44}, {'x': 356, 'y': 43}], 'confidence': 0.965}]}]}]}, 'peopleResult': {'values': [{'boundingBox': {'x': 127, 'y': 194, 'w': 224, 'h': 605}, 'confidence': 0.7865033149719238}, {'boundingBox': {'x': 136, 'y': 377, 'w': 203, 'h': 224}, 'confidence': 0.0019430926768109202}]}}\n",
      "Caption: a woman wearing sunglasses and a skirt (Confidence: 0.7680467367172241)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9925472140312195)\n",
      "- person (Confidence: 0.9593212604522705)\n",
      "- outdoor (Confidence: 0.9295374751091003)\n",
      "- building (Confidence: 0.9152774214744568)\n",
      "- woman (Confidence: 0.9083626866340637)\n",
      "- fashion accessory (Confidence: 0.901688277721405)\n",
      "- footwear (Confidence: 0.8918161392211914)\n",
      "- fashion (Confidence: 0.8619606494903564)\n",
      "- waist (Confidence: 0.861922025680542)\n",
      "- lady (Confidence: 0.8465598821640015)\n",
      "- window (Confidence: 0.7353109121322632)\n",
      "- girl (Confidence: 0.6054539680480957)\n",
      "- standing (Confidence: 0.5948991179466248)\n",
      "Dense Captions:\n",
      "- a woman wearing sunglasses and a skirt (Confidence: 0.7680467367172241)\n",
      "- a woman in a skirt and a green shirt (Confidence: 0.7352137565612793)\n",
      "- a close up of a white skirt (Confidence: 0.7458731532096863)\n",
      "- a woman with long hair wearing sunglasses (Confidence: 0.6929612159729004)\n",
      "- a woman wearing a green top and white skirt (Confidence: 0.7896580100059509)\n",
      "- a close-up of a plant (Confidence: 0.7646339535713196)\n",
      "- a close up of a metal object (Confidence: 0.6877428889274597)\n",
      "- a woman wearing sunglasses and a skirt (Confidence: 0.7600184082984924)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_13.png\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman wearing a brown dress', 'confidence': 0.7996203899383545}, 'denseCaptionsResult': {'values': [{'text': 'a woman wearing a brown dress', 'confidence': 0.7996203899383545, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a brown dress', 'confidence': 0.7318982481956482, 'boundingBox': {'x': 92, 'y': 0, 'w': 176, 'h': 777}}, {'text': 'a close-up of a woman', 'confidence': 0.9193321466445923, 'boundingBox': {'x': 146, 'y': 14, 'w': 112, 'h': 181}}, {'text': \"a close-up of a woman's feet\", 'confidence': 0.8628923296928406, 'boundingBox': {'x': 156, 'y': 625, 'w': 60, 'h': 158}}, {'text': 'a close up of a dress', 'confidence': 0.7889321446418762, 'boundingBox': {'x': 108, 'y': 279, 'w': 150, 'h': 319}}, {'text': 'a blurry green bush', 'confidence': 0.668538510799408, 'boundingBox': {'x': 268, 'y': 263, 'w': 62, 'h': 81}}, {'text': 'blur a blurry image of a tree', 'confidence': 0.748283326625824, 'boundingBox': {'x': 270, 'y': 0, 'w': 123, 'h': 270}}, {'text': 'a woman wearing a brown skirt and sandals', 'confidence': 0.7244333624839783, 'boundingBox': {'x': 0, 'y': 289, 'w': 388, 'h': 496}}, {'text': 'a close up of a leg', 'confidence': 0.7041520476341248, 'boundingBox': {'x': 174, 'y': 622, 'w': 51, 'h': 87}}, {'text': 'a woman wearing a brown dress', 'confidence': 0.7331585884094238, 'boundingBox': {'x': 99, 'y': 9, 'w': 181, 'h': 394}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9944460391998291}, {'name': 'fashion', 'confidence': 0.9714424014091492}, {'name': 'person', 'confidence': 0.9660764932632446}, {'name': 'day dress', 'confidence': 0.9639536738395691}, {'name': 'fashion model', 'confidence': 0.9595026969909668}, {'name': 'fashion design', 'confidence': 0.9563249349594116}, {'name': 'pattern (fashion design)', 'confidence': 0.955929160118103}, {'name': 'footwear', 'confidence': 0.9097524881362915}, {'name': 'polka dot', 'confidence': 0.8922989368438721}, {'name': 'casual dress', 'confidence': 0.8848531246185303}, {'name': 'cocktail dress', 'confidence': 0.8822378516197205}, {'name': 'photo shoot', 'confidence': 0.8765221238136292}, {'name': 'model', 'confidence': 0.8754985332489014}, {'name': 'waist', 'confidence': 0.8754206895828247}, {'name': 'dress', 'confidence': 0.8662847280502319}, {'name': 'shoulder', 'confidence': 0.8603095412254333}, {'name': 'sandal', 'confidence': 0.857479453086853}, {'name': 'fashion show', 'confidence': 0.8407131433486938}, {'name': 'outdoor', 'confidence': 0.7020701169967651}, {'name': 'standing', 'confidence': 0.6811429858207703}, {'name': 'woman', 'confidence': 0.582347571849823}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 98, 'y': 7, 'w': 179, 'h': 785}, 'confidence': 0.9181420803070068}, {'boundingBox': {'x': 107, 'y': 294, 'w': 162, 'h': 201}, 'confidence': 0.0017102111596614122}]}}\n",
      "Caption: a woman wearing a brown dress (Confidence: 0.7996203899383545)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9944460391998291)\n",
      "- fashion (Confidence: 0.9714424014091492)\n",
      "- person (Confidence: 0.9660764932632446)\n",
      "- day dress (Confidence: 0.9639536738395691)\n",
      "- fashion model (Confidence: 0.9595026969909668)\n",
      "- fashion design (Confidence: 0.9563249349594116)\n",
      "- pattern (fashion design) (Confidence: 0.955929160118103)\n",
      "- footwear (Confidence: 0.9097524881362915)\n",
      "- polka dot (Confidence: 0.8922989368438721)\n",
      "- casual dress (Confidence: 0.8848531246185303)\n",
      "- cocktail dress (Confidence: 0.8822378516197205)\n",
      "- photo shoot (Confidence: 0.8765221238136292)\n",
      "- model (Confidence: 0.8754985332489014)\n",
      "- waist (Confidence: 0.8754206895828247)\n",
      "- dress (Confidence: 0.8662847280502319)\n",
      "- shoulder (Confidence: 0.8603095412254333)\n",
      "- sandal (Confidence: 0.857479453086853)\n",
      "- fashion show (Confidence: 0.8407131433486938)\n",
      "- outdoor (Confidence: 0.7020701169967651)\n",
      "- standing (Confidence: 0.6811429858207703)\n",
      "- woman (Confidence: 0.582347571849823)\n",
      "Dense Captions:\n",
      "- a woman wearing a brown dress (Confidence: 0.7996203899383545)\n",
      "- a woman wearing a brown dress (Confidence: 0.7318982481956482)\n",
      "- a close-up of a woman (Confidence: 0.9193321466445923)\n",
      "- a close-up of a woman's feet (Confidence: 0.8628923296928406)\n",
      "- a close up of a dress (Confidence: 0.7889321446418762)\n",
      "- a blurry green bush (Confidence: 0.668538510799408)\n",
      "- blur a blurry image of a tree (Confidence: 0.748283326625824)\n",
      "- a woman wearing a brown skirt and sandals (Confidence: 0.7244333624839783)\n",
      "- a close up of a leg (Confidence: 0.7041520476341248)\n",
      "- a woman wearing a brown dress (Confidence: 0.7331585884094238)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_14.png\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman wearing a blue dress', 'confidence': 0.8691132664680481}, 'denseCaptionsResult': {'values': [{'text': 'a woman wearing a blue dress', 'confidence': 0.8691132664680481, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a blue dress', 'confidence': 0.8528799414634705, 'boundingBox': {'x': 84, 'y': 0, 'w': 209, 'h': 785}}, {'text': 'a close up of a bag', 'confidence': 0.847114086151123, 'boundingBox': {'x': 64, 'y': 478, 'w': 66, 'h': 208}}, {'text': 'a woman with long brown hair', 'confidence': 0.7906026244163513, 'boundingBox': {'x': 131, 'y': 9, 'w': 124, 'h': 218}}, {'text': 'a woman wearing a blue dress', 'confidence': 0.7932541966438293, 'boundingBox': {'x': 114, 'y': 318, 'w': 178, 'h': 311}}, {'text': 'a blurry picture of a window', 'confidence': 0.717437207698822, 'boundingBox': {'x': 1, 'y': 2, 'w': 107, 'h': 284}}, {'text': 'a close up of a foot', 'confidence': 0.7826892137527466, 'boundingBox': {'x': 163, 'y': 719, 'w': 55, 'h': 79}}, {'text': 'a woman wearing a blue dress and sandals holding a brown bag', 'confidence': 0.7409933805465698, 'boundingBox': {'x': 0, 'y': 423, 'w': 388, 'h': 366}}, {'text': 'a person standing in front of a bush', 'confidence': 0.7797672748565674, 'boundingBox': {'x': 268, 'y': 291, 'w': 126, 'h': 159}}, {'text': 'a close up of a bag', 'confidence': 0.8011611104011536, 'boundingBox': {'x': 45, 'y': 455, 'w': 104, 'h': 287}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9941908121109009}, {'name': 'person', 'confidence': 0.9916039705276489}, {'name': 'dress', 'confidence': 0.9753078818321228}, {'name': 'fashion', 'confidence': 0.9712661504745483}, {'name': 'day dress', 'confidence': 0.9664942026138306}, {'name': 'fashion design', 'confidence': 0.94410240650177}, {'name': 'shoulder', 'confidence': 0.9342482089996338}, {'name': 'waist', 'confidence': 0.9271707534790039}, {'name': 'street fashion', 'confidence': 0.9244189262390137}, {'name': 'casual dress', 'confidence': 0.9234451055526733}, {'name': 'pattern (fashion design)', 'confidence': 0.9201693534851074}, {'name': 'handbag', 'confidence': 0.911948025226593}, {'name': 'fashion model', 'confidence': 0.9077636003494263}, {'name': 'cocktail dress', 'confidence': 0.8979761600494385}, {'name': 'woman', 'confidence': 0.8847976922988892}, {'name': 'fashion accessory', 'confidence': 0.8819758296012878}, {'name': 'lady', 'confidence': 0.8737606406211853}, {'name': 'female person', 'confidence': 0.8566533327102661}, {'name': 'blouse', 'confidence': 0.8499457836151123}, {'name': 'outdoor', 'confidence': 0.794875979423523}, {'name': 'ground', 'confidence': 0.7735491394996643}, {'name': 'girl', 'confidence': 0.5260487794876099}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 72, 'y': 473, 'w': 62, 'h': 204}, 'tags': [{'name': 'handbag', 'confidence': 0.682}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 96, 'y': 13, 'w': 204, 'h': 786}, 'confidence': 0.9562084078788757}, {'boundingBox': {'x': 103, 'y': 303, 'w': 181, 'h': 194}, 'confidence': 0.0023609481286257505}]}}\n",
      "Caption: a woman wearing a blue dress (Confidence: 0.8691132664680481)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9941908121109009)\n",
      "- person (Confidence: 0.9916039705276489)\n",
      "- dress (Confidence: 0.9753078818321228)\n",
      "- fashion (Confidence: 0.9712661504745483)\n",
      "- day dress (Confidence: 0.9664942026138306)\n",
      "- fashion design (Confidence: 0.94410240650177)\n",
      "- shoulder (Confidence: 0.9342482089996338)\n",
      "- waist (Confidence: 0.9271707534790039)\n",
      "- street fashion (Confidence: 0.9244189262390137)\n",
      "- casual dress (Confidence: 0.9234451055526733)\n",
      "- pattern (fashion design) (Confidence: 0.9201693534851074)\n",
      "- handbag (Confidence: 0.911948025226593)\n",
      "- fashion model (Confidence: 0.9077636003494263)\n",
      "- cocktail dress (Confidence: 0.8979761600494385)\n",
      "- woman (Confidence: 0.8847976922988892)\n",
      "- fashion accessory (Confidence: 0.8819758296012878)\n",
      "- lady (Confidence: 0.8737606406211853)\n",
      "- female person (Confidence: 0.8566533327102661)\n",
      "- blouse (Confidence: 0.8499457836151123)\n",
      "- outdoor (Confidence: 0.794875979423523)\n",
      "- ground (Confidence: 0.7735491394996643)\n",
      "- girl (Confidence: 0.5260487794876099)\n",
      "Dense Captions:\n",
      "- a woman wearing a blue dress (Confidence: 0.8691132664680481)\n",
      "- a woman wearing a blue dress (Confidence: 0.8528799414634705)\n",
      "- a close up of a bag (Confidence: 0.847114086151123)\n",
      "- a woman with long brown hair (Confidence: 0.7906026244163513)\n",
      "- a woman wearing a blue dress (Confidence: 0.7932541966438293)\n",
      "- a blurry picture of a window (Confidence: 0.717437207698822)\n",
      "- a close up of a foot (Confidence: 0.7826892137527466)\n",
      "- a woman wearing a blue dress and sandals holding a brown bag (Confidence: 0.7409933805465698)\n",
      "- a person standing in front of a bush (Confidence: 0.7797672748565674)\n",
      "- a close up of a bag (Confidence: 0.8011611104011536)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_15.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman wearing a dress', 'confidence': 0.767620861530304}, 'denseCaptionsResult': {'values': [{'text': 'a woman wearing a dress', 'confidence': 0.767620861530304, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a mannequin wearing a dress', 'confidence': 0.8507770299911499, 'boundingBox': {'x': 90, 'y': 0, 'w': 219, 'h': 785}}, {'text': 'a black object with a white object in the middle', 'confidence': 0.6208256483078003, 'boundingBox': {'x': 82, 'y': 537, 'w': 92, 'h': 247}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.995862603187561}, {'name': 'person', 'confidence': 0.9808655381202698}, {'name': 'fashion', 'confidence': 0.9716777801513672}, {'name': 'shoulder', 'confidence': 0.9553244709968567}, {'name': 'fashion design', 'confidence': 0.953970193862915}, {'name': 'waist', 'confidence': 0.9490103125572205}, {'name': 'casual dress', 'confidence': 0.942629337310791}, {'name': 'fashion model', 'confidence': 0.9373031854629517}, {'name': 'fashion accessory', 'confidence': 0.9167764186859131}, {'name': 'street fashion', 'confidence': 0.9086501002311707}, {'name': 'sleeve', 'confidence': 0.8979250192642212}, {'name': 'handbag', 'confidence': 0.8960253000259399}, {'name': 'standing', 'confidence': 0.8680335283279419}, {'name': 'high heels', 'confidence': 0.8586229085922241}, {'name': 'top', 'confidence': 0.8439357280731201}, {'name': 'woman', 'confidence': 0.7368322610855103}, {'name': 'wearing', 'confidence': 0.5736677050590515}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 95, 'y': 0, 'w': 225, 'h': 799}, 'confidence': 0.8528985977172852}, {'boundingBox': {'x': 115, 'y': 302, 'w': 197, 'h': 192}, 'confidence': 0.0013672951608896255}]}}\n",
      "Caption: a woman wearing a dress (Confidence: 0.767620861530304)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.995862603187561)\n",
      "- person (Confidence: 0.9808655381202698)\n",
      "- fashion (Confidence: 0.9716777801513672)\n",
      "- shoulder (Confidence: 0.9553244709968567)\n",
      "- fashion design (Confidence: 0.953970193862915)\n",
      "- waist (Confidence: 0.9490103125572205)\n",
      "- casual dress (Confidence: 0.942629337310791)\n",
      "- fashion model (Confidence: 0.9373031854629517)\n",
      "- fashion accessory (Confidence: 0.9167764186859131)\n",
      "- street fashion (Confidence: 0.9086501002311707)\n",
      "- sleeve (Confidence: 0.8979250192642212)\n",
      "- handbag (Confidence: 0.8960253000259399)\n",
      "- standing (Confidence: 0.8680335283279419)\n",
      "- high heels (Confidence: 0.8586229085922241)\n",
      "- top (Confidence: 0.8439357280731201)\n",
      "- woman (Confidence: 0.7368322610855103)\n",
      "- wearing (Confidence: 0.5736677050590515)\n",
      "Dense Captions:\n",
      "- a woman wearing a dress (Confidence: 0.767620861530304)\n",
      "- a mannequin wearing a dress (Confidence: 0.8507770299911499)\n",
      "- a black object with a white object in the middle (Confidence: 0.6208256483078003)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_16.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a beige dress', 'confidence': 0.7097204923629761}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a beige dress', 'confidence': 0.7097204923629761, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman in a skirt and jacket', 'confidence': 0.6960616111755371, 'boundingBox': {'x': 29, 'y': 29, 'w': 245, 'h': 757}}, {'text': 'a woman with long black hair', 'confidence': 0.8291908502578735, 'boundingBox': {'x': 104, 'y': 27, 'w': 136, 'h': 164}}, {'text': 'a woman wearing a beige suit', 'confidence': 0.6825509667396545, 'boundingBox': {'x': 37, 'y': 131, 'w': 232, 'h': 232}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9972602128982544}, {'name': 'fashion design', 'confidence': 0.963530421257019}, {'name': 'fashion', 'confidence': 0.9631099700927734}, {'name': 'fashion model', 'confidence': 0.9584842920303345}, {'name': 'person', 'confidence': 0.9518681764602661}, {'name': 'waist', 'confidence': 0.9120492935180664}, {'name': 'high heels', 'confidence': 0.9084748029708862}, {'name': 'casual dress', 'confidence': 0.9057003259658813}, {'name': 'day dress', 'confidence': 0.8948171138763428}, {'name': 'shoulder', 'confidence': 0.8917347192764282}, {'name': 'dress', 'confidence': 0.8897440433502197}, {'name': 'pattern (fashion design)', 'confidence': 0.8869384527206421}, {'name': 'photo shoot', 'confidence': 0.8593350648880005}, {'name': 'sheath dress', 'confidence': 0.8558259010314941}, {'name': 'fashion show', 'confidence': 0.8483113646507263}, {'name': 'model', 'confidence': 0.8475570678710938}, {'name': 'woman', 'confidence': 0.6406131982803345}, {'name': 'fabric', 'confidence': 0.5480199456214905}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 29, 'y': 26, 'w': 258, 'h': 765}, 'confidence': 0.950128436088562}, {'boundingBox': {'x': 43, 'y': 295, 'w': 221, 'h': 209}, 'confidence': 0.002667977474629879}]}}\n",
      "Caption: a woman in a beige dress (Confidence: 0.7097204923629761)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9972602128982544)\n",
      "- fashion design (Confidence: 0.963530421257019)\n",
      "- fashion (Confidence: 0.9631099700927734)\n",
      "- fashion model (Confidence: 0.9584842920303345)\n",
      "- person (Confidence: 0.9518681764602661)\n",
      "- waist (Confidence: 0.9120492935180664)\n",
      "- high heels (Confidence: 0.9084748029708862)\n",
      "- casual dress (Confidence: 0.9057003259658813)\n",
      "- day dress (Confidence: 0.8948171138763428)\n",
      "- shoulder (Confidence: 0.8917347192764282)\n",
      "- dress (Confidence: 0.8897440433502197)\n",
      "- pattern (fashion design) (Confidence: 0.8869384527206421)\n",
      "- photo shoot (Confidence: 0.8593350648880005)\n",
      "- sheath dress (Confidence: 0.8558259010314941)\n",
      "- fashion show (Confidence: 0.8483113646507263)\n",
      "- model (Confidence: 0.8475570678710938)\n",
      "- woman (Confidence: 0.6406131982803345)\n",
      "- fabric (Confidence: 0.5480199456214905)\n",
      "Dense Captions:\n",
      "- a woman in a beige dress (Confidence: 0.7097204923629761)\n",
      "- a woman in a skirt and jacket (Confidence: 0.6960616111755371)\n",
      "- a woman with long black hair (Confidence: 0.8291908502578735)\n",
      "- a woman wearing a beige suit (Confidence: 0.6825509667396545)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_17.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman wearing a red dress', 'confidence': 0.9161517024040222}, 'denseCaptionsResult': {'values': [{'text': 'a woman wearing a red dress', 'confidence': 0.9157201051712036, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a red dress', 'confidence': 0.9148845672607422, 'boundingBox': {'x': 0, 'y': 0, 'w': 387, 'h': 785}}, {'text': 'a close up of a bag', 'confidence': 0.8049901127815247, 'boundingBox': {'x': 9, 'y': 547, 'w': 101, 'h': 224}}, {'text': \"a close up of a woman's neck\", 'confidence': 0.8585164546966553, 'boundingBox': {'x': 76, 'y': 0, 'w': 201, 'h': 109}}, {'text': 'a necklace with a pink and white pendant', 'confidence': 0.6216142177581787, 'boundingBox': {'x': 139, 'y': 47, 'w': 68, 'h': 132}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9951098561286926}, {'name': 'person', 'confidence': 0.9904074668884277}, {'name': 'fashion', 'confidence': 0.9792625904083252}, {'name': 'day dress', 'confidence': 0.9741655588150024}, {'name': 'dress', 'confidence': 0.972741961479187}, {'name': 'fashion accessory', 'confidence': 0.9654363393783569}, {'name': 'fashion design', 'confidence': 0.9587016105651855}, {'name': 'cocktail dress', 'confidence': 0.9550453424453735}, {'name': 'fashion model', 'confidence': 0.9475044012069702}, {'name': 'shoulder', 'confidence': 0.9311147332191467}, {'name': 'waist', 'confidence': 0.9054537415504456}, {'name': 'casual dress', 'confidence': 0.8840087652206421}, {'name': 'sheath dress', 'confidence': 0.8786258697509766}, {'name': 'model', 'confidence': 0.8655201196670532}, {'name': 'lady', 'confidence': 0.8651233315467834}, {'name': 'pattern (fashion design)', 'confidence': 0.8629578351974487}, {'name': 'woman', 'confidence': 0.8353651762008667}, {'name': 'red', 'confidence': 0.7292051911354065}, {'name': 'wearing', 'confidence': 0.6393654346466064}, {'name': 'outdoor', 'confidence': 0.5365989804267883}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 13, 'y': 0, 'w': 386, 'h': 799}, 'confidence': 0.9458864331245422}, {'boundingBox': {'x': 61, 'y': 301, 'w': 314, 'h': 192}, 'confidence': 0.002283751033246517}]}}\n",
      "Caption: a woman wearing a red dress (Confidence: 0.9161517024040222)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9951098561286926)\n",
      "- person (Confidence: 0.9904074668884277)\n",
      "- fashion (Confidence: 0.9792625904083252)\n",
      "- day dress (Confidence: 0.9741655588150024)\n",
      "- dress (Confidence: 0.972741961479187)\n",
      "- fashion accessory (Confidence: 0.9654363393783569)\n",
      "- fashion design (Confidence: 0.9587016105651855)\n",
      "- cocktail dress (Confidence: 0.9550453424453735)\n",
      "- fashion model (Confidence: 0.9475044012069702)\n",
      "- shoulder (Confidence: 0.9311147332191467)\n",
      "- waist (Confidence: 0.9054537415504456)\n",
      "- casual dress (Confidence: 0.8840087652206421)\n",
      "- sheath dress (Confidence: 0.8786258697509766)\n",
      "- model (Confidence: 0.8655201196670532)\n",
      "- lady (Confidence: 0.8651233315467834)\n",
      "- pattern (fashion design) (Confidence: 0.8629578351974487)\n",
      "- woman (Confidence: 0.8353651762008667)\n",
      "- red (Confidence: 0.7292051911354065)\n",
      "- wearing (Confidence: 0.6393654346466064)\n",
      "- outdoor (Confidence: 0.5365989804267883)\n",
      "Dense Captions:\n",
      "- a woman wearing a red dress (Confidence: 0.9157201051712036)\n",
      "- a woman wearing a red dress (Confidence: 0.9148845672607422)\n",
      "- a close up of a bag (Confidence: 0.8049901127815247)\n",
      "- a close up of a woman's neck (Confidence: 0.8585164546966553)\n",
      "- a necklace with a pink and white pendant (Confidence: 0.6216142177581787)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_18.png\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a brown suit', 'confidence': 0.8280320167541504}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a brown suit', 'confidence': 0.8280320167541504, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a brown suit', 'confidence': 0.8094870448112488, 'boundingBox': {'x': 61, 'y': 6, 'w': 258, 'h': 781}}, {'text': 'a black bag from a swinger', 'confidence': 0.740566074848175, 'boundingBox': {'x': 46, 'y': 581, 'w': 93, 'h': 216}}, {'text': 'a woman with brown hair and a tan jacket', 'confidence': 0.6619463562965393, 'boundingBox': {'x': 127, 'y': 17, 'w': 137, 'h': 250}}, {'text': 'a woman wearing a brown suit', 'confidence': 0.79485684633255, 'boundingBox': {'x': 78, 'y': 155, 'w': 238, 'h': 411}}, {'text': \"a close up of a woman's legs\", 'confidence': 0.8313166499137878, 'boundingBox': {'x': 106, 'y': 414, 'w': 178, 'h': 377}}, {'text': 'a white tile floor with a black and white line', 'confidence': 0.5724473595619202, 'boundingBox': {'x': 269, 'y': 596, 'w': 124, 'h': 193}}, {'text': 'a white wall with a brown frame', 'confidence': 0.6440222859382629, 'boundingBox': {'x': 178, 'y': 207, 'w': 56, 'h': 206}}, {'text': \"a close-up of a person's arm\", 'confidence': 0.8577914834022522, 'boundingBox': {'x': 0, 'y': 0, 'w': 122, 'h': 616}}, {'text': 'a woman in a brown suit', 'confidence': 0.737571656703949, 'boundingBox': {'x': 103, 'y': 17, 'w': 208, 'h': 375}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9993869662284851}, {'name': 'person', 'confidence': 0.994262158870697}, {'name': 'fashion', 'confidence': 0.9654619693756104}, {'name': 'coat', 'confidence': 0.9603779911994934}, {'name': 'outerwear', 'confidence': 0.9585750102996826}, {'name': 'fashion design', 'confidence': 0.9457031488418579}, {'name': 'pocket', 'confidence': 0.9388413429260254}, {'name': 'handbag', 'confidence': 0.9346979856491089}, {'name': 'fashion model', 'confidence': 0.9256037473678589}, {'name': 'blazer', 'confidence': 0.9238333106040955}, {'name': 'trousers', 'confidence': 0.910300076007843}, {'name': 'fashion accessory', 'confidence': 0.9095699787139893}, {'name': 'casual dress', 'confidence': 0.8826900720596313}, {'name': 'collar', 'confidence': 0.8802382946014404}, {'name': 'leather', 'confidence': 0.8802059888839722}, {'name': 'shoulder', 'confidence': 0.8778834342956543}, {'name': 'sleeve', 'confidence': 0.8734757900238037}, {'name': 'waist', 'confidence': 0.868071436882019}, {'name': 'street fashion', 'confidence': 0.8593877553939819}, {'name': 'button', 'confidence': 0.854934811592102}, {'name': 'trench coat', 'confidence': 0.8541783094406128}, {'name': 'top', 'confidence': 0.8479210138320923}, {'name': 'luggage and bags', 'confidence': 0.8436155319213867}, {'name': 'fabric', 'confidence': 0.8369635939598083}, {'name': 'woman', 'confidence': 0.784443199634552}, {'name': 'indoor', 'confidence': 0.7717373967170715}, {'name': 'tan', 'confidence': 0.6556092500686646}, {'name': 'standing', 'confidence': 0.6457912921905518}, {'name': 'floor', 'confidence': 0.5893416404724121}, {'name': 'wearing', 'confidence': 0.5824952125549316}, {'name': 'suit', 'confidence': 0.4180590808391571}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 58, 'y': 576, 'w': 83, 'h': 219}, 'tags': [{'name': 'handbag', 'confidence': 0.586}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 83, 'y': 20, 'w': 240, 'h': 779}, 'confidence': 0.9569509625434875}, {'boundingBox': {'x': 92, 'y': 326, 'w': 218, 'h': 205}, 'confidence': 0.0030079027637839317}]}}\n",
      "Caption: a woman in a brown suit (Confidence: 0.8280320167541504)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9993869662284851)\n",
      "- person (Confidence: 0.994262158870697)\n",
      "- fashion (Confidence: 0.9654619693756104)\n",
      "- coat (Confidence: 0.9603779911994934)\n",
      "- outerwear (Confidence: 0.9585750102996826)\n",
      "- fashion design (Confidence: 0.9457031488418579)\n",
      "- pocket (Confidence: 0.9388413429260254)\n",
      "- handbag (Confidence: 0.9346979856491089)\n",
      "- fashion model (Confidence: 0.9256037473678589)\n",
      "- blazer (Confidence: 0.9238333106040955)\n",
      "- trousers (Confidence: 0.910300076007843)\n",
      "- fashion accessory (Confidence: 0.9095699787139893)\n",
      "- casual dress (Confidence: 0.8826900720596313)\n",
      "- collar (Confidence: 0.8802382946014404)\n",
      "- leather (Confidence: 0.8802059888839722)\n",
      "- shoulder (Confidence: 0.8778834342956543)\n",
      "- sleeve (Confidence: 0.8734757900238037)\n",
      "- waist (Confidence: 0.868071436882019)\n",
      "- street fashion (Confidence: 0.8593877553939819)\n",
      "- button (Confidence: 0.854934811592102)\n",
      "- trench coat (Confidence: 0.8541783094406128)\n",
      "- top (Confidence: 0.8479210138320923)\n",
      "- luggage and bags (Confidence: 0.8436155319213867)\n",
      "- fabric (Confidence: 0.8369635939598083)\n",
      "- woman (Confidence: 0.784443199634552)\n",
      "- indoor (Confidence: 0.7717373967170715)\n",
      "- tan (Confidence: 0.6556092500686646)\n",
      "- standing (Confidence: 0.6457912921905518)\n",
      "- floor (Confidence: 0.5893416404724121)\n",
      "- wearing (Confidence: 0.5824952125549316)\n",
      "- suit (Confidence: 0.4180590808391571)\n",
      "Dense Captions:\n",
      "- a woman in a brown suit (Confidence: 0.8280320167541504)\n",
      "- a woman wearing a brown suit (Confidence: 0.8094870448112488)\n",
      "- a black bag from a swinger (Confidence: 0.740566074848175)\n",
      "- a woman with brown hair and a tan jacket (Confidence: 0.6619463562965393)\n",
      "- a woman wearing a brown suit (Confidence: 0.79485684633255)\n",
      "- a close up of a woman's legs (Confidence: 0.8313166499137878)\n",
      "- a white tile floor with a black and white line (Confidence: 0.5724473595619202)\n",
      "- a white wall with a brown frame (Confidence: 0.6440222859382629)\n",
      "- a close-up of a person's arm (Confidence: 0.8577914834022522)\n",
      "- a woman in a brown suit (Confidence: 0.737571656703949)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_2.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a white dress', 'confidence': 0.8864009976387024}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a white dress', 'confidence': 0.8864009976387024, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman in a white dress', 'confidence': 0.888688862323761, 'boundingBox': {'x': 120, 'y': 27, 'w': 188, 'h': 761}}, {'text': 'a close up of a white object', 'confidence': 0.7208629250526428, 'boundingBox': {'x': 109, 'y': 451, 'w': 63, 'h': 217}}, {'text': 'a woman with blonde hair', 'confidence': 0.7765906453132629, 'boundingBox': {'x': 145, 'y': 26, 'w': 109, 'h': 168}}, {'text': 'a woman wearing a white skirt', 'confidence': 0.7915604114532471, 'boundingBox': {'x': 139, 'y': 283, 'w': 116, 'h': 279}}, {'text': \"a close up of a woman's legs\", 'confidence': 0.7792953848838806, 'boundingBox': {'x': 0, 'y': 648, 'w': 392, 'h': 143}}, {'text': 'a close up of a white surface', 'confidence': 0.6834165453910828, 'boundingBox': {'x': 200, 'y': 694, 'w': 196, 'h': 97}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9986934661865234}, {'name': 'fashion model', 'confidence': 0.9805846810340881}, {'name': 'fashion', 'confidence': 0.9764701128005981}, {'name': 'fashion design', 'confidence': 0.9697962999343872}, {'name': 'person', 'confidence': 0.9649460911750793}, {'name': 'shoulder', 'confidence': 0.9451214671134949}, {'name': 'high heels', 'confidence': 0.9379028081893921}, {'name': 'fashion show', 'confidence': 0.9341076612472534}, {'name': 'waist', 'confidence': 0.9200558662414551}, {'name': 'model', 'confidence': 0.9192517995834351}, {'name': 'footwear', 'confidence': 0.9183831810951233}, {'name': 'photo shoot', 'confidence': 0.9095937609672546}, {'name': 'day dress', 'confidence': 0.9085931181907654}, {'name': 'sandal', 'confidence': 0.8944541215896606}, {'name': 'runway', 'confidence': 0.8854551315307617}, {'name': 'casual dress', 'confidence': 0.880666971206665}, {'name': 'wall', 'confidence': 0.8680121898651123}, {'name': 'cocktail dress', 'confidence': 0.8651264905929565}, {'name': 'sheath dress', 'confidence': 0.8522499799728394}, {'name': 'haute couture', 'confidence': 0.8497970104217529}, {'name': 'dress', 'confidence': 0.6940029859542847}, {'name': 'indoor', 'confidence': 0.6744297742843628}, {'name': 'woman', 'confidence': 0.6555730700492859}, {'name': 'fabric', 'confidence': 0.5594304800033569}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 128, 'y': 26, 'w': 189, 'h': 772}, 'confidence': 0.9304049015045166}, {'boundingBox': {'x': 142, 'y': 298, 'w': 168, 'h': 203}, 'confidence': 0.0018974720733240247}]}}\n",
      "Caption: a woman in a white dress (Confidence: 0.8864009976387024)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9986934661865234)\n",
      "- fashion model (Confidence: 0.9805846810340881)\n",
      "- fashion (Confidence: 0.9764701128005981)\n",
      "- fashion design (Confidence: 0.9697962999343872)\n",
      "- person (Confidence: 0.9649460911750793)\n",
      "- shoulder (Confidence: 0.9451214671134949)\n",
      "- high heels (Confidence: 0.9379028081893921)\n",
      "- fashion show (Confidence: 0.9341076612472534)\n",
      "- waist (Confidence: 0.9200558662414551)\n",
      "- model (Confidence: 0.9192517995834351)\n",
      "- footwear (Confidence: 0.9183831810951233)\n",
      "- photo shoot (Confidence: 0.9095937609672546)\n",
      "- day dress (Confidence: 0.9085931181907654)\n",
      "- sandal (Confidence: 0.8944541215896606)\n",
      "- runway (Confidence: 0.8854551315307617)\n",
      "- casual dress (Confidence: 0.880666971206665)\n",
      "- wall (Confidence: 0.8680121898651123)\n",
      "- cocktail dress (Confidence: 0.8651264905929565)\n",
      "- sheath dress (Confidence: 0.8522499799728394)\n",
      "- haute couture (Confidence: 0.8497970104217529)\n",
      "- dress (Confidence: 0.6940029859542847)\n",
      "- indoor (Confidence: 0.6744297742843628)\n",
      "- woman (Confidence: 0.6555730700492859)\n",
      "- fabric (Confidence: 0.5594304800033569)\n",
      "Dense Captions:\n",
      "- a woman in a white dress (Confidence: 0.8864009976387024)\n",
      "- a woman in a white dress (Confidence: 0.888688862323761)\n",
      "- a close up of a white object (Confidence: 0.7208629250526428)\n",
      "- a woman with blonde hair (Confidence: 0.7765906453132629)\n",
      "- a woman wearing a white skirt (Confidence: 0.7915604114532471)\n",
      "- a close up of a woman's legs (Confidence: 0.7792953848838806)\n",
      "- a close up of a white surface (Confidence: 0.6834165453910828)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_3.png\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman wearing a blue dress', 'confidence': 0.8764075636863708}, 'denseCaptionsResult': {'values': [{'text': 'a woman wearing a blue dress', 'confidence': 0.8764075636863708, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a blue dress', 'confidence': 0.8327217102050781, 'boundingBox': {'x': 90, 'y': 0, 'w': 200, 'h': 786}}, {'text': 'a woman wearing a blue dress', 'confidence': 0.8606307506561279, 'boundingBox': {'x': 98, 'y': 132, 'w': 186, 'h': 400}}, {'text': 'a woman with brown hair', 'confidence': 0.7493699193000793, 'boundingBox': {'x': 146, 'y': 14, 'w': 120, 'h': 184}}, {'text': \"a close up of a person's arm\", 'confidence': 0.8078035712242126, 'boundingBox': {'x': 93, 'y': 151, 'w': 69, 'h': 320}}, {'text': 'a close up of a foot', 'confidence': 0.7328708171844482, 'boundingBox': {'x': 157, 'y': 676, 'w': 56, 'h': 120}}, {'text': 'a blurry image of a tree', 'confidence': 0.8456275463104248, 'boundingBox': {'x': 272, 'y': 0, 'w': 124, 'h': 157}}, {'text': 'a woman with long brown hair wearing a blue shirt', 'confidence': 0.7835777401924133, 'boundingBox': {'x': 109, 'y': 5, 'w': 186, 'h': 301}}, {'text': 'a woman wearing a blue dress', 'confidence': 0.7701810002326965, 'boundingBox': {'x': 113, 'y': 308, 'w': 166, 'h': 225}}, {'text': 'a woman wearing a blue dress and brown purse', 'confidence': 0.7651388049125671, 'boundingBox': {'x': 0, 'y': 269, 'w': 387, 'h': 519}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9972745180130005}, {'name': 'person', 'confidence': 0.9928367137908936}, {'name': 'day dress', 'confidence': 0.9490488767623901}, {'name': 'footwear', 'confidence': 0.946094274520874}, {'name': 'dress', 'confidence': 0.9421030282974243}, {'name': 'fashion', 'confidence': 0.9290869235992432}, {'name': 'casual dress', 'confidence': 0.9253028035163879}, {'name': 'street fashion', 'confidence': 0.9227240085601807}, {'name': 'shoulder', 'confidence': 0.922146201133728}, {'name': 'waist', 'confidence': 0.9070854187011719}, {'name': 'fashion design', 'confidence': 0.8897071480751038}, {'name': 'pattern (fashion design)', 'confidence': 0.876197338104248}, {'name': 'sandal', 'confidence': 0.8608630299568176}, {'name': 'fashion model', 'confidence': 0.8576446175575256}, {'name': 'cocktail dress', 'confidence': 0.8492273092269897}, {'name': 'woman', 'confidence': 0.8467689752578735}, {'name': 'outdoor', 'confidence': 0.8460165858268738}, {'name': 'blouse', 'confidence': 0.8414796590805054}, {'name': 'ground', 'confidence': 0.7051151990890503}, {'name': 'blue', 'confidence': 0.6363406181335449}, {'name': 'girl', 'confidence': 0.5726674199104309}, {'name': 'street', 'confidence': 0.48789331316947937}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 113, 'y': 17, 'w': 185, 'h': 510}, 'tags': [{'name': 'person', 'confidence': 0.708}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 112, 'y': 12, 'w': 182, 'h': 787}, 'confidence': 0.9324010014533997}, {'boundingBox': {'x': 118, 'y': 303, 'w': 159, 'h': 193}, 'confidence': 0.001568453386425972}]}}\n",
      "Caption: a woman wearing a blue dress (Confidence: 0.8764075636863708)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9972745180130005)\n",
      "- person (Confidence: 0.9928367137908936)\n",
      "- day dress (Confidence: 0.9490488767623901)\n",
      "- footwear (Confidence: 0.946094274520874)\n",
      "- dress (Confidence: 0.9421030282974243)\n",
      "- fashion (Confidence: 0.9290869235992432)\n",
      "- casual dress (Confidence: 0.9253028035163879)\n",
      "- street fashion (Confidence: 0.9227240085601807)\n",
      "- shoulder (Confidence: 0.922146201133728)\n",
      "- waist (Confidence: 0.9070854187011719)\n",
      "- fashion design (Confidence: 0.8897071480751038)\n",
      "- pattern (fashion design) (Confidence: 0.876197338104248)\n",
      "- sandal (Confidence: 0.8608630299568176)\n",
      "- fashion model (Confidence: 0.8576446175575256)\n",
      "- cocktail dress (Confidence: 0.8492273092269897)\n",
      "- woman (Confidence: 0.8467689752578735)\n",
      "- outdoor (Confidence: 0.8460165858268738)\n",
      "- blouse (Confidence: 0.8414796590805054)\n",
      "- ground (Confidence: 0.7051151990890503)\n",
      "- blue (Confidence: 0.6363406181335449)\n",
      "- girl (Confidence: 0.5726674199104309)\n",
      "- street (Confidence: 0.48789331316947937)\n",
      "Dense Captions:\n",
      "- a woman wearing a blue dress (Confidence: 0.8764075636863708)\n",
      "- a woman wearing a blue dress (Confidence: 0.8327217102050781)\n",
      "- a woman wearing a blue dress (Confidence: 0.8606307506561279)\n",
      "- a woman with brown hair (Confidence: 0.7493699193000793)\n",
      "- a close up of a person's arm (Confidence: 0.8078035712242126)\n",
      "- a close up of a foot (Confidence: 0.7328708171844482)\n",
      "- a blurry image of a tree (Confidence: 0.8456275463104248)\n",
      "- a woman with long brown hair wearing a blue shirt (Confidence: 0.7835777401924133)\n",
      "- a woman wearing a blue dress (Confidence: 0.7701810002326965)\n",
      "- a woman wearing a blue dress and brown purse (Confidence: 0.7651388049125671)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_4.png\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman wearing a green dress', 'confidence': 0.833340585231781}, 'denseCaptionsResult': {'values': [{'text': 'a woman wearing a green dress', 'confidence': 0.833340585231781, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': \"a close up of a woman's body\", 'confidence': 0.7985992431640625, 'boundingBox': {'x': 88, 'y': 0, 'w': 197, 'h': 786}}, {'text': 'a blurry image of a tree', 'confidence': 0.7742426991462708, 'boundingBox': {'x': 1, 'y': 8, 'w': 85, 'h': 309}}, {'text': 'a pair of tan heels', 'confidence': 0.6993755102157593, 'boundingBox': {'x': 152, 'y': 729, 'w': 135, 'h': 68}}, {'text': 'a woman with long hair', 'confidence': 0.732379674911499, 'boundingBox': {'x': 136, 'y': 16, 'w': 117, 'h': 194}}, {'text': \"a blurry picture of a person's hand\", 'confidence': 0.683834969997406, 'boundingBox': {'x': 328, 'y': 0, 'w': 67, 'h': 243}}, {'text': 'a close up of a dress', 'confidence': 0.796852707862854, 'boundingBox': {'x': 178, 'y': 298, 'w': 66, 'h': 52}}, {'text': 'a woman wearing a skirt and heels', 'confidence': 0.7283033132553101, 'boundingBox': {'x': 0, 'y': 432, 'w': 387, 'h': 357}}, {'text': \"a close up of a woman's dress\", 'confidence': 0.7718793749809265, 'boundingBox': {'x': 102, 'y': 295, 'w': 170, 'h': 301}}, {'text': 'a blurry image of a person', 'confidence': 0.7856195569038391, 'boundingBox': {'x': 0, 'y': 300, 'w': 111, 'h': 140}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9961115717887878}, {'name': 'person', 'confidence': 0.9809093475341797}, {'name': 'fashion', 'confidence': 0.9473564624786377}, {'name': 'day dress', 'confidence': 0.9429411888122559}, {'name': 'fashion design', 'confidence': 0.9371678829193115}, {'name': 'pattern (fashion design)', 'confidence': 0.9354206323623657}, {'name': 'dress', 'confidence': 0.9119091033935547}, {'name': 'casual dress', 'confidence': 0.897445023059845}, {'name': 'footwear', 'confidence': 0.8956830501556396}, {'name': 'sleeve', 'confidence': 0.8582527041435242}, {'name': 'fashion model', 'confidence': 0.8488249778747559}, {'name': 'woman', 'confidence': 0.8073955774307251}, {'name': 'ground', 'confidence': 0.7031102180480957}, {'name': 'standing', 'confidence': 0.6730610132217407}, {'name': 'outdoor', 'confidence': 0.6008553504943848}, {'name': 'wearing', 'confidence': 0.5603996515274048}, {'name': 'girl', 'confidence': 0.42381465435028076}, {'name': 'outfit', 'confidence': 0.4162232577800751}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 100, 'y': 13, 'w': 192, 'h': 786}, 'confidence': 0.9487801194190979}, {'boundingBox': {'x': 111, 'y': 305, 'w': 174, 'h': 193}, 'confidence': 0.0020908606238663197}]}}\n",
      "Caption: a woman wearing a green dress (Confidence: 0.833340585231781)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9961115717887878)\n",
      "- person (Confidence: 0.9809093475341797)\n",
      "- fashion (Confidence: 0.9473564624786377)\n",
      "- day dress (Confidence: 0.9429411888122559)\n",
      "- fashion design (Confidence: 0.9371678829193115)\n",
      "- pattern (fashion design) (Confidence: 0.9354206323623657)\n",
      "- dress (Confidence: 0.9119091033935547)\n",
      "- casual dress (Confidence: 0.897445023059845)\n",
      "- footwear (Confidence: 0.8956830501556396)\n",
      "- sleeve (Confidence: 0.8582527041435242)\n",
      "- fashion model (Confidence: 0.8488249778747559)\n",
      "- woman (Confidence: 0.8073955774307251)\n",
      "- ground (Confidence: 0.7031102180480957)\n",
      "- standing (Confidence: 0.6730610132217407)\n",
      "- outdoor (Confidence: 0.6008553504943848)\n",
      "- wearing (Confidence: 0.5603996515274048)\n",
      "- girl (Confidence: 0.42381465435028076)\n",
      "- outfit (Confidence: 0.4162232577800751)\n",
      "Dense Captions:\n",
      "- a woman wearing a green dress (Confidence: 0.833340585231781)\n",
      "- a close up of a woman's body (Confidence: 0.7985992431640625)\n",
      "- a blurry image of a tree (Confidence: 0.7742426991462708)\n",
      "- a pair of tan heels (Confidence: 0.6993755102157593)\n",
      "- a woman with long hair (Confidence: 0.732379674911499)\n",
      "- a blurry picture of a person's hand (Confidence: 0.683834969997406)\n",
      "- a close up of a dress (Confidence: 0.796852707862854)\n",
      "- a woman wearing a skirt and heels (Confidence: 0.7283033132553101)\n",
      "- a close up of a woman's dress (Confidence: 0.7718793749809265)\n",
      "- a blurry image of a person (Confidence: 0.7856195569038391)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_5.png\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman wearing a white shirt and blue jeans', 'confidence': 0.8249773979187012}, 'denseCaptionsResult': {'values': [{'text': 'a woman wearing a white shirt and blue jeans', 'confidence': 0.8249773979187012, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a white shirt and blue jeans', 'confidence': 0.820788562297821, 'boundingBox': {'x': 67, 'y': 7, 'w': 261, 'h': 782}}, {'text': \"a close up of a woman's jeans\", 'confidence': 0.7668552398681641, 'boundingBox': {'x': 96, 'y': 443, 'w': 187, 'h': 350}}, {'text': \"a close up of a person's arm\", 'confidence': 0.8225800395011902, 'boundingBox': {'x': 256, 'y': 209, 'w': 74, 'h': 420}}, {'text': 'a woman with long brown hair', 'confidence': 0.7895137667655945, 'boundingBox': {'x': 110, 'y': 14, 'w': 166, 'h': 296}}, {'text': 'a woman wearing a white shirt', 'confidence': 0.8073359131813049, 'boundingBox': {'x': 108, 'y': 199, 'w': 169, 'h': 251}}, {'text': 'a woman wearing a necklace', 'confidence': 0.7638812065124512, 'boundingBox': {'x': 169, 'y': 196, 'w': 76, 'h': 86}}, {'text': 'a close up of a hand in a pocket', 'confidence': 0.7653058171272278, 'boundingBox': {'x': 220, 'y': 452, 'w': 56, 'h': 62}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'person', 'confidence': 0.9964268207550049}, {'name': 'clothing', 'confidence': 0.9949787855148315}, {'name': 'casual dress', 'confidence': 0.9613709449768066}, {'name': 'jeans', 'confidence': 0.9603912830352783}, {'name': 'street fashion', 'confidence': 0.9595016241073608}, {'name': 'denim', 'confidence': 0.9570446610450745}, {'name': 'pocket', 'confidence': 0.9499059319496155}, {'name': 'waist', 'confidence': 0.9393982887268066}, {'name': 'shoulder', 'confidence': 0.9392387270927429}, {'name': 'human face', 'confidence': 0.9230179786682129}, {'name': 'top', 'confidence': 0.9062833189964294}, {'name': 'fashion', 'confidence': 0.9010878205299377}, {'name': 'sleeve', 'confidence': 0.8991961479187012}, {'name': 'fashion accessory', 'confidence': 0.8735402822494507}, {'name': 'trousers', 'confidence': 0.8659889101982117}, {'name': 'collar', 'confidence': 0.864447832107544}, {'name': 'jean', 'confidence': 0.86353999376297}, {'name': 'photo shoot', 'confidence': 0.8552418947219849}, {'name': 'fashion design', 'confidence': 0.8525421619415283}, {'name': 'belt', 'confidence': 0.8512721061706543}, {'name': 'fashion model', 'confidence': 0.8415735960006714}, {'name': 'handbag', 'confidence': 0.840111255645752}, {'name': 'woman', 'confidence': 0.8207708597183228}, {'name': 'outdoor', 'confidence': 0.6876866817474365}, {'name': 'girl', 'confidence': 0.6668562889099121}, {'name': 'blouse', 'confidence': 0.6018127202987671}, {'name': 'street', 'confidence': 0.47250115871429443}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 74, 'y': 15, 'w': 262, 'h': 783}, 'confidence': 0.9590182900428772}, {'boundingBox': {'x': 86, 'y': 325, 'w': 233, 'h': 203}, 'confidence': 0.002772001549601555}]}}\n",
      "Caption: a woman wearing a white shirt and blue jeans (Confidence: 0.8249773979187012)\n",
      "Tags:\n",
      "- person (Confidence: 0.9964268207550049)\n",
      "- clothing (Confidence: 0.9949787855148315)\n",
      "- casual dress (Confidence: 0.9613709449768066)\n",
      "- jeans (Confidence: 0.9603912830352783)\n",
      "- street fashion (Confidence: 0.9595016241073608)\n",
      "- denim (Confidence: 0.9570446610450745)\n",
      "- pocket (Confidence: 0.9499059319496155)\n",
      "- waist (Confidence: 0.9393982887268066)\n",
      "- shoulder (Confidence: 0.9392387270927429)\n",
      "- human face (Confidence: 0.9230179786682129)\n",
      "- top (Confidence: 0.9062833189964294)\n",
      "- fashion (Confidence: 0.9010878205299377)\n",
      "- sleeve (Confidence: 0.8991961479187012)\n",
      "- fashion accessory (Confidence: 0.8735402822494507)\n",
      "- trousers (Confidence: 0.8659889101982117)\n",
      "- collar (Confidence: 0.864447832107544)\n",
      "- jean (Confidence: 0.86353999376297)\n",
      "- photo shoot (Confidence: 0.8552418947219849)\n",
      "- fashion design (Confidence: 0.8525421619415283)\n",
      "- belt (Confidence: 0.8512721061706543)\n",
      "- fashion model (Confidence: 0.8415735960006714)\n",
      "- handbag (Confidence: 0.840111255645752)\n",
      "- woman (Confidence: 0.8207708597183228)\n",
      "- outdoor (Confidence: 0.6876866817474365)\n",
      "- girl (Confidence: 0.6668562889099121)\n",
      "- blouse (Confidence: 0.6018127202987671)\n",
      "- street (Confidence: 0.47250115871429443)\n",
      "Dense Captions:\n",
      "- a woman wearing a white shirt and blue jeans (Confidence: 0.8249773979187012)\n",
      "- a woman wearing a white shirt and blue jeans (Confidence: 0.820788562297821)\n",
      "- a close up of a woman's jeans (Confidence: 0.7668552398681641)\n",
      "- a close up of a person's arm (Confidence: 0.8225800395011902)\n",
      "- a woman with long brown hair (Confidence: 0.7895137667655945)\n",
      "- a woman wearing a white shirt (Confidence: 0.8073359131813049)\n",
      "- a woman wearing a necklace (Confidence: 0.7638812065124512)\n",
      "- a close up of a hand in a pocket (Confidence: 0.7653058171272278)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_6.png\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a brown dress', 'confidence': 0.8705564737319946}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a brown dress', 'confidence': 0.8705564737319946, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a brown dress', 'confidence': 0.8456539511680603, 'boundingBox': {'x': 73, 'y': 9, 'w': 223, 'h': 779}}, {'text': 'a blurry green plant', 'confidence': 0.6869519352912903, 'boundingBox': {'x': 0, 'y': 258, 'w': 68, 'h': 204}}, {'text': 'a close-up of a couch', 'confidence': 0.8573296666145325, 'boundingBox': {'x': 277, 'y': 364, 'w': 118, 'h': 348}}, {'text': 'a close up of a pillow', 'confidence': 0.8099033236503601, 'boundingBox': {'x': 315, 'y': 437, 'w': 81, 'h': 93}}, {'text': 'a table with a chair in the background', 'confidence': 0.6266690492630005, 'boundingBox': {'x': 1, 'y': 449, 'w': 74, 'h': 237}}, {'text': 'a woman with brown hair and brown eyes', 'confidence': 0.7054082155227661, 'boundingBox': {'x': 120, 'y': 15, 'w': 139, 'h': 255}}, {'text': \"a close-up of a person's legs\", 'confidence': 0.8492452502250671, 'boundingBox': {'x': 126, 'y': 670, 'w': 156, 'h': 126}}, {'text': 'a blurry picture of a stool', 'confidence': 0.7416636943817139, 'boundingBox': {'x': 0, 'y': 447, 'w': 73, 'h': 94}}, {'text': 'a table with a round top', 'confidence': 0.5791968703269958, 'boundingBox': {'x': 0, 'y': 516, 'w': 69, 'h': 110}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9959553480148315}, {'name': 'person', 'confidence': 0.989652156829834}, {'name': 'indoor', 'confidence': 0.9807457327842712}, {'name': 'furniture', 'confidence': 0.9758045077323914}, {'name': 'dress', 'confidence': 0.9582661390304565}, {'name': 'wall', 'confidence': 0.9411455392837524}, {'name': 'day dress', 'confidence': 0.9370676875114441}, {'name': 'pattern (fashion design)', 'confidence': 0.9235974550247192}, {'name': 'shoulder', 'confidence': 0.9136322736740112}, {'name': 'fashion', 'confidence': 0.8854346871376038}, {'name': 'chair', 'confidence': 0.8797444701194763}, {'name': 'fashion design', 'confidence': 0.874050498008728}, {'name': 'blouse', 'confidence': 0.8484407067298889}, {'name': 'woman', 'confidence': 0.7738350629806519}, {'name': 'floor', 'confidence': 0.7399076819419861}, {'name': 'standing', 'confidence': 0.6743842363357544}, {'name': 'girl', 'confidence': 0.5696935653686523}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 81, 'y': 17, 'w': 224, 'h': 782}, 'confidence': 0.9481858015060425}, {'boundingBox': {'x': 92, 'y': 303, 'w': 197, 'h': 195}, 'confidence': 0.0023208982311189175}]}}\n",
      "Caption: a woman in a brown dress (Confidence: 0.8705564737319946)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9959553480148315)\n",
      "- person (Confidence: 0.989652156829834)\n",
      "- indoor (Confidence: 0.9807457327842712)\n",
      "- furniture (Confidence: 0.9758045077323914)\n",
      "- dress (Confidence: 0.9582661390304565)\n",
      "- wall (Confidence: 0.9411455392837524)\n",
      "- day dress (Confidence: 0.9370676875114441)\n",
      "- pattern (fashion design) (Confidence: 0.9235974550247192)\n",
      "- shoulder (Confidence: 0.9136322736740112)\n",
      "- fashion (Confidence: 0.8854346871376038)\n",
      "- chair (Confidence: 0.8797444701194763)\n",
      "- fashion design (Confidence: 0.874050498008728)\n",
      "- blouse (Confidence: 0.8484407067298889)\n",
      "- woman (Confidence: 0.7738350629806519)\n",
      "- floor (Confidence: 0.7399076819419861)\n",
      "- standing (Confidence: 0.6743842363357544)\n",
      "- girl (Confidence: 0.5696935653686523)\n",
      "Dense Captions:\n",
      "- a woman in a brown dress (Confidence: 0.8705564737319946)\n",
      "- a woman wearing a brown dress (Confidence: 0.8456539511680603)\n",
      "- a blurry green plant (Confidence: 0.6869519352912903)\n",
      "- a close-up of a couch (Confidence: 0.8573296666145325)\n",
      "- a close up of a pillow (Confidence: 0.8099033236503601)\n",
      "- a table with a chair in the background (Confidence: 0.6266690492630005)\n",
      "- a woman with brown hair and brown eyes (Confidence: 0.7054082155227661)\n",
      "- a close-up of a person's legs (Confidence: 0.8492452502250671)\n",
      "- a blurry picture of a stool (Confidence: 0.7416636943817139)\n",
      "- a table with a round top (Confidence: 0.5791968703269958)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_7.png\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a suit', 'confidence': 0.8017448782920837}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a suit', 'confidence': 0.8017448782920837, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a close-up of a woman in a suit', 'confidence': 0.8580416440963745, 'boundingBox': {'x': 79, 'y': 0, 'w': 210, 'h': 787}}, {'text': 'a close up of a black bag', 'confidence': 0.7984638214111328, 'boundingBox': {'x': 82, 'y': 477, 'w': 55, 'h': 159}}, {'text': 'a close-up of a woman', 'confidence': 0.9191253185272217, 'boundingBox': {'x': 140, 'y': 13, 'w': 108, 'h': 193}}, {'text': 'a woman wearing a black suit', 'confidence': 0.7188959717750549, 'boundingBox': {'x': 101, 'y': 139, 'w': 180, 'h': 303}}, {'text': \"a close-up of a woman's legs\", 'confidence': 0.8695151805877686, 'boundingBox': {'x': 119, 'y': 412, 'w': 133, 'h': 352}}, {'text': 'a blurry image of a person standing in front of a window', 'confidence': 0.7219365239143372, 'boundingBox': {'x': 272, 'y': 0, 'w': 120, 'h': 607}}, {'text': 'a woman wearing black pants and a black purse', 'confidence': 0.6796312928199768, 'boundingBox': {'x': 51, 'y': 407, 'w': 267, 'h': 386}}, {'text': 'a woman in a black suit', 'confidence': 0.7834845781326294, 'boundingBox': {'x': 101, 'y': 8, 'w': 193, 'h': 298}}, {'text': 'a blurry image of a window', 'confidence': 0.6812468767166138, 'boundingBox': {'x': 0, 'y': 0, 'w': 88, 'h': 480}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9987820386886597}, {'name': 'person', 'confidence': 0.9952689409255981}, {'name': 'fashion', 'confidence': 0.9709872603416443}, {'name': 'coat', 'confidence': 0.9707978963851929}, {'name': 'outerwear', 'confidence': 0.9551929235458374}, {'name': 'blazer', 'confidence': 0.954968273639679}, {'name': 'fashion design', 'confidence': 0.9427081942558289}, {'name': 'fashion model', 'confidence': 0.9148706197738647}, {'name': 'pocket', 'confidence': 0.9147913455963135}, {'name': 'shoulder', 'confidence': 0.9107784032821655}, {'name': 'collar', 'confidence': 0.9033019542694092}, {'name': 'handbag', 'confidence': 0.8955056667327881}, {'name': 'trousers', 'confidence': 0.8939933776855469}, {'name': 'fashion accessory', 'confidence': 0.8933538198471069}, {'name': 'casual dress', 'confidence': 0.8748111724853516}, {'name': 'pantsuit', 'confidence': 0.8681498765945435}, {'name': 'street fashion', 'confidence': 0.8661400675773621}, {'name': 'footwear', 'confidence': 0.8632116317749023}, {'name': 'suit trousers', 'confidence': 0.8605404496192932}, {'name': 'sleeve', 'confidence': 0.8602983355522156}, {'name': 'indoor', 'confidence': 0.8566731214523315}, {'name': 'formal wear', 'confidence': 0.8509125709533691}, {'name': 'waist', 'confidence': 0.8491692543029785}, {'name': 'woman', 'confidence': 0.8443436622619629}, {'name': 'high heels', 'confidence': 0.8404204845428467}, {'name': 'suit', 'confidence': 0.7910135984420776}, {'name': 'floor', 'confidence': 0.7667304277420044}, {'name': 'wall', 'confidence': 0.7116981744766235}, {'name': 'standing', 'confidence': 0.692202091217041}, {'name': 'wearing', 'confidence': 0.638501763343811}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 87, 'y': 475, 'w': 53, 'h': 161}, 'tags': [{'name': 'Luggage and bags', 'confidence': 0.719}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 99, 'y': 12, 'w': 190, 'h': 787}, 'confidence': 0.9562032222747803}, {'boundingBox': {'x': 106, 'y': 302, 'w': 172, 'h': 195}, 'confidence': 0.0020256845746189356}]}}\n",
      "Caption: a woman in a suit (Confidence: 0.8017448782920837)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9987820386886597)\n",
      "- person (Confidence: 0.9952689409255981)\n",
      "- fashion (Confidence: 0.9709872603416443)\n",
      "- coat (Confidence: 0.9707978963851929)\n",
      "- outerwear (Confidence: 0.9551929235458374)\n",
      "- blazer (Confidence: 0.954968273639679)\n",
      "- fashion design (Confidence: 0.9427081942558289)\n",
      "- fashion model (Confidence: 0.9148706197738647)\n",
      "- pocket (Confidence: 0.9147913455963135)\n",
      "- shoulder (Confidence: 0.9107784032821655)\n",
      "- collar (Confidence: 0.9033019542694092)\n",
      "- handbag (Confidence: 0.8955056667327881)\n",
      "- trousers (Confidence: 0.8939933776855469)\n",
      "- fashion accessory (Confidence: 0.8933538198471069)\n",
      "- casual dress (Confidence: 0.8748111724853516)\n",
      "- pantsuit (Confidence: 0.8681498765945435)\n",
      "- street fashion (Confidence: 0.8661400675773621)\n",
      "- footwear (Confidence: 0.8632116317749023)\n",
      "- suit trousers (Confidence: 0.8605404496192932)\n",
      "- sleeve (Confidence: 0.8602983355522156)\n",
      "- indoor (Confidence: 0.8566731214523315)\n",
      "- formal wear (Confidence: 0.8509125709533691)\n",
      "- waist (Confidence: 0.8491692543029785)\n",
      "- woman (Confidence: 0.8443436622619629)\n",
      "- high heels (Confidence: 0.8404204845428467)\n",
      "- suit (Confidence: 0.7910135984420776)\n",
      "- floor (Confidence: 0.7667304277420044)\n",
      "- wall (Confidence: 0.7116981744766235)\n",
      "- standing (Confidence: 0.692202091217041)\n",
      "- wearing (Confidence: 0.638501763343811)\n",
      "Dense Captions:\n",
      "- a woman in a suit (Confidence: 0.8017448782920837)\n",
      "- a close-up of a woman in a suit (Confidence: 0.8580416440963745)\n",
      "- a close up of a black bag (Confidence: 0.7984638214111328)\n",
      "- a close-up of a woman (Confidence: 0.9191253185272217)\n",
      "- a woman wearing a black suit (Confidence: 0.7188959717750549)\n",
      "- a close-up of a woman's legs (Confidence: 0.8695151805877686)\n",
      "- a blurry image of a person standing in front of a window (Confidence: 0.7219365239143372)\n",
      "- a woman wearing black pants and a black purse (Confidence: 0.6796312928199768)\n",
      "- a woman in a black suit (Confidence: 0.7834845781326294)\n",
      "- a blurry image of a window (Confidence: 0.6812468767166138)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_8.png\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a pink suit', 'confidence': 0.7796657681465149}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a pink suit', 'confidence': 0.7796657681465149, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': \"a close up of a woman's dress\", 'confidence': 0.7212175726890564, 'boundingBox': {'x': 91, 'y': 0, 'w': 172, 'h': 788}}, {'text': 'a close up of a bag', 'confidence': 0.7956770062446594, 'boundingBox': {'x': 93, 'y': 507, 'w': 56, 'h': 197}}, {'text': \"a close-up of a woman's face\", 'confidence': 0.9374580979347229, 'boundingBox': {'x': 151, 'y': 0, 'w': 84, 'h': 85}}, {'text': 'a close up of a jacket', 'confidence': 0.7757164239883423, 'boundingBox': {'x': 112, 'y': 63, 'w': 149, 'h': 387}}, {'text': \"a close-up of a woman's legs\", 'confidence': 0.8460952043533325, 'boundingBox': {'x': 149, 'y': 642, 'w': 92, 'h': 155}}, {'text': \"a close-up of a woman's legs\", 'confidence': 0.8657164573669434, 'boundingBox': {'x': 0, 'y': 670, 'w': 393, 'h': 122}}, {'text': 'a white wall with a brown edge', 'confidence': 0.6364432573318481, 'boundingBox': {'x': 3, 'y': 340, 'w': 107, 'h': 342}}, {'text': \"a close-up of a woman's skin\", 'confidence': 0.8082153797149658, 'boundingBox': {'x': 122, 'y': 424, 'w': 120, 'h': 235}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9995061159133911}, {'name': 'fashion', 'confidence': 0.9719114303588867}, {'name': 'person', 'confidence': 0.9708497524261475}, {'name': 'fashion design', 'confidence': 0.9699382781982422}, {'name': 'fashion model', 'confidence': 0.9362046718597412}, {'name': 'pattern (fashion design)', 'confidence': 0.9285190105438232}, {'name': 'coat', 'confidence': 0.9184257984161377}, {'name': 'outerwear', 'confidence': 0.9108699560165405}, {'name': 'day dress', 'confidence': 0.9097860455513}, {'name': 'dress', 'confidence': 0.8859500885009766}, {'name': 'shoulder', 'confidence': 0.8847154974937439}, {'name': 'blazer', 'confidence': 0.8841245174407959}, {'name': 'fashion accessory', 'confidence': 0.8795701265335083}, {'name': 'casual dress', 'confidence': 0.871787428855896}, {'name': 'button', 'confidence': 0.8602299690246582}, {'name': 'collar', 'confidence': 0.8600295782089233}, {'name': 'sleeve', 'confidence': 0.8486815094947815}, {'name': 'waist', 'confidence': 0.8481172323226929}, {'name': 'sheath dress', 'confidence': 0.846332311630249}, {'name': 'duster', 'confidence': 0.8446614742279053}, {'name': 'blouse', 'confidence': 0.8403252959251404}, {'name': 'wall', 'confidence': 0.8180920481681824}, {'name': 'woman', 'confidence': 0.7238470315933228}, {'name': 'standing', 'confidence': 0.7205101847648621}, {'name': 'indoor', 'confidence': 0.681882381439209}, {'name': 'fabric', 'confidence': 0.6413652300834656}, {'name': 'floor', 'confidence': 0.5951852202415466}, {'name': 'wearing', 'confidence': 0.5914397239685059}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 101, 'y': 1, 'w': 177, 'h': 795}, 'confidence': 0.05512590333819389}]}}\n",
      "Caption: a woman in a pink suit (Confidence: 0.7796657681465149)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9995061159133911)\n",
      "- fashion (Confidence: 0.9719114303588867)\n",
      "- person (Confidence: 0.9708497524261475)\n",
      "- fashion design (Confidence: 0.9699382781982422)\n",
      "- fashion model (Confidence: 0.9362046718597412)\n",
      "- pattern (fashion design) (Confidence: 0.9285190105438232)\n",
      "- coat (Confidence: 0.9184257984161377)\n",
      "- outerwear (Confidence: 0.9108699560165405)\n",
      "- day dress (Confidence: 0.9097860455513)\n",
      "- dress (Confidence: 0.8859500885009766)\n",
      "- shoulder (Confidence: 0.8847154974937439)\n",
      "- blazer (Confidence: 0.8841245174407959)\n",
      "- fashion accessory (Confidence: 0.8795701265335083)\n",
      "- casual dress (Confidence: 0.871787428855896)\n",
      "- button (Confidence: 0.8602299690246582)\n",
      "- collar (Confidence: 0.8600295782089233)\n",
      "- sleeve (Confidence: 0.8486815094947815)\n",
      "- waist (Confidence: 0.8481172323226929)\n",
      "- sheath dress (Confidence: 0.846332311630249)\n",
      "- duster (Confidence: 0.8446614742279053)\n",
      "- blouse (Confidence: 0.8403252959251404)\n",
      "- wall (Confidence: 0.8180920481681824)\n",
      "- woman (Confidence: 0.7238470315933228)\n",
      "- standing (Confidence: 0.7205101847648621)\n",
      "- indoor (Confidence: 0.681882381439209)\n",
      "- fabric (Confidence: 0.6413652300834656)\n",
      "- floor (Confidence: 0.5951852202415466)\n",
      "- wearing (Confidence: 0.5914397239685059)\n",
      "Dense Captions:\n",
      "- a woman in a pink suit (Confidence: 0.7796657681465149)\n",
      "- a close up of a woman's dress (Confidence: 0.7212175726890564)\n",
      "- a close up of a bag (Confidence: 0.7956770062446594)\n",
      "- a close-up of a woman's face (Confidence: 0.9374580979347229)\n",
      "- a close up of a jacket (Confidence: 0.7757164239883423)\n",
      "- a close-up of a woman's legs (Confidence: 0.8460952043533325)\n",
      "- a close-up of a woman's legs (Confidence: 0.8657164573669434)\n",
      "- a white wall with a brown edge (Confidence: 0.6364432573318481)\n",
      "- a close-up of a woman's skin (Confidence: 0.8082153797149658)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\\F_9.png\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a red suit', 'confidence': 0.8577043414115906}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a red suit', 'confidence': 0.8577043414115906, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': \"a close up of a woman's body\", 'confidence': 0.7978391647338867, 'boundingBox': {'x': 110, 'y': 88, 'w': 182, 'h': 628}}, {'text': \"a close-up of a woman's feet\", 'confidence': 0.8186882734298706, 'boundingBox': {'x': 153, 'y': 687, 'w': 71, 'h': 109}}, {'text': 'a close-up of a woman', 'confidence': 0.920560896396637, 'boundingBox': {'x': 137, 'y': 17, 'w': 111, 'h': 179}}, {'text': 'a woman wearing a red suit', 'confidence': 0.7878881096839905, 'boundingBox': {'x': 108, 'y': 134, 'w': 168, 'h': 283}}, {'text': 'a close up of a window', 'confidence': 0.806471049785614, 'boundingBox': {'x': 267, 'y': 0, 'w': 120, 'h': 665}}, {'text': 'a blurry image of a door handle', 'confidence': 0.7564160227775574, 'boundingBox': {'x': 0, 'y': 0, 'w': 103, 'h': 689}}, {'text': 'a woman wearing red shoes', 'confidence': 0.729995846748352, 'boundingBox': {'x': 0, 'y': 443, 'w': 382, 'h': 348}}, {'text': 'a close up of a red jacket', 'confidence': 0.7566435933113098, 'boundingBox': {'x': 132, 'y': 298, 'w': 126, 'h': 110}}, {'text': \"a close up of a woman's skirt\", 'confidence': 0.8096569180488586, 'boundingBox': {'x': 128, 'y': 388, 'w': 132, 'h': 184}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9966444969177246}, {'name': 'person', 'confidence': 0.9906521439552307}, {'name': 'fashion', 'confidence': 0.98537677526474}, {'name': 'fashion design', 'confidence': 0.9766401052474976}, {'name': 'fashion model', 'confidence': 0.9642693400382996}, {'name': 'high heels', 'confidence': 0.949908971786499}, {'name': 'day dress', 'confidence': 0.940883457660675}, {'name': 'shoulder', 'confidence': 0.9373321533203125}, {'name': 'cocktail dress', 'confidence': 0.919298529624939}, {'name': 'waist', 'confidence': 0.9148350954055786}, {'name': 'woman', 'confidence': 0.8907544016838074}, {'name': 'fashion accessory', 'confidence': 0.8906106948852539}, {'name': 'red', 'confidence': 0.8856499791145325}, {'name': 'casual dress', 'confidence': 0.8827752470970154}, {'name': 'footwear', 'confidence': 0.8826475739479065}, {'name': 'sheath dress', 'confidence': 0.8773446083068848}, {'name': 'pattern (fashion design)', 'confidence': 0.8749680519104004}, {'name': 'sandal', 'confidence': 0.8709770441055298}, {'name': 'model', 'confidence': 0.8698287010192871}, {'name': 'lady', 'confidence': 0.8592513799667358}, {'name': 'fashion show', 'confidence': 0.845306396484375}, {'name': 'indoor', 'confidence': 0.8431221842765808}, {'name': 'fabric', 'confidence': 0.7826216220855713}, {'name': 'wall', 'confidence': 0.7464608550071716}, {'name': 'floor', 'confidence': 0.7345720529556274}, {'name': 'wearing', 'confidence': 0.7234100103378296}, {'name': 'standing', 'confidence': 0.6678606271743774}, {'name': 'dress', 'confidence': 0.6655608415603638}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 110, 'y': 15, 'w': 176, 'h': 784}, 'confidence': 0.9120821356773376}, {'boundingBox': {'x': 116, 'y': 303, 'w': 156, 'h': 193}, 'confidence': 0.0016447416273877025}]}}\n",
      "Caption: a woman in a red suit (Confidence: 0.8577043414115906)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9966444969177246)\n",
      "- person (Confidence: 0.9906521439552307)\n",
      "- fashion (Confidence: 0.98537677526474)\n",
      "- fashion design (Confidence: 0.9766401052474976)\n",
      "- fashion model (Confidence: 0.9642693400382996)\n",
      "- high heels (Confidence: 0.949908971786499)\n",
      "- day dress (Confidence: 0.940883457660675)\n",
      "- shoulder (Confidence: 0.9373321533203125)\n",
      "- cocktail dress (Confidence: 0.919298529624939)\n",
      "- waist (Confidence: 0.9148350954055786)\n",
      "- woman (Confidence: 0.8907544016838074)\n",
      "- fashion accessory (Confidence: 0.8906106948852539)\n",
      "- red (Confidence: 0.8856499791145325)\n",
      "- casual dress (Confidence: 0.8827752470970154)\n",
      "- footwear (Confidence: 0.8826475739479065)\n",
      "- sheath dress (Confidence: 0.8773446083068848)\n",
      "- pattern (fashion design) (Confidence: 0.8749680519104004)\n",
      "- sandal (Confidence: 0.8709770441055298)\n",
      "- model (Confidence: 0.8698287010192871)\n",
      "- lady (Confidence: 0.8592513799667358)\n",
      "- fashion show (Confidence: 0.845306396484375)\n",
      "- indoor (Confidence: 0.8431221842765808)\n",
      "- fabric (Confidence: 0.7826216220855713)\n",
      "- wall (Confidence: 0.7464608550071716)\n",
      "- floor (Confidence: 0.7345720529556274)\n",
      "- wearing (Confidence: 0.7234100103378296)\n",
      "- standing (Confidence: 0.6678606271743774)\n",
      "- dress (Confidence: 0.6655608415603638)\n",
      "Dense Captions:\n",
      "- a woman in a red suit (Confidence: 0.8577043414115906)\n",
      "- a close up of a woman's body (Confidence: 0.7978391647338867)\n",
      "- a close-up of a woman's feet (Confidence: 0.8186882734298706)\n",
      "- a close-up of a woman (Confidence: 0.920560896396637)\n",
      "- a woman wearing a red suit (Confidence: 0.7878881096839905)\n",
      "- a close up of a window (Confidence: 0.806471049785614)\n",
      "- a blurry image of a door handle (Confidence: 0.7564160227775574)\n",
      "- a woman wearing red shoes (Confidence: 0.729995846748352)\n",
      "- a close up of a red jacket (Confidence: 0.7566435933113098)\n",
      "- a close up of a woman's skirt (Confidence: 0.8096569180488586)\n"
     ]
    }
   ],
   "source": [
    "# Tag extraction from a directory of images\n",
    "analyze_images_in_directory(r\"D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\resized_images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965fa1f7",
   "metadata": {},
   "source": [
    "### Mens Tag Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e3bfc26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path, PureWindowsPath\n",
    "\n",
    "# file_path = Path(r\"D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\Men_Tags\")  # Change this to your actual file path\n",
    "file_path = Path(r\"D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\women\\Run1\\Women_Tags\")  # Change this to your actual file path\n",
    "\n",
    "data = []\n",
    "current = {}\n",
    "\n",
    "def extract_confidence(text):\n",
    "    match = re.search(r\"\\(Confidence:\\s*([0-9.]+)\\)\", text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        stripped = line.strip()\n",
    "\n",
    "        if stripped.startswith(\"Analyzing image:\"):\n",
    "            if current:\n",
    "                data.append(current)\n",
    "            current = {\n",
    "                \"ImagePath\": stripped.replace(\"Analyzing image:\", \"\").strip(),\n",
    "                \"Caption\": \"\",\n",
    "                \"CaptionConfidence\": None,\n",
    "                \"Tags\": [],\n",
    "                \"TagConfidences\": []\n",
    "            }\n",
    "\n",
    "        elif stripped.startswith(\"Caption:\"):\n",
    "            caption_line = stripped.replace(\"Caption:\", \"\").strip()\n",
    "            current[\"CaptionConfidence\"] = extract_confidence(caption_line)\n",
    "            current[\"Caption\"] = caption_line[:caption_line.rfind(\"(\")].strip() if \"(\" in caption_line else caption_line\n",
    "\n",
    "        elif stripped.startswith(\"-\"):\n",
    "            tag_line = stripped[1:].strip()\n",
    "            confidence = extract_confidence(tag_line)\n",
    "            tag = tag_line[:tag_line.rfind(\"(\")].strip() if \"(\" in tag_line else tag_line\n",
    "            current[\"Tags\"].append(tag)\n",
    "            current[\"TagConfidences\"].append(round(confidence, 3) if confidence else None)\n",
    "\n",
    "# Add last parsed item\n",
    "if current:\n",
    "    data.append(current)\n",
    "\n",
    "# Add TagConfidenceMap and ImageName\n",
    "for entry in data:\n",
    "    entry[\"TagConfidenceMap\"] = {\n",
    "        tag: conf for tag, conf in zip(entry[\"Tags\"], entry[\"TagConfidences\"])\n",
    "    }\n",
    "    entry[\"ImageName\"] = PureWindowsPath(entry[\"ImagePath\"]).name\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optional: Save to file\n",
    "# df.to_csv(\"parsed_image_tags.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5b7bb821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ImageName                                       Caption  CaptionConfidence  \\\n",
      "0    F_1.jpg                       a woman in a blue dress           0.880444   \n",
      "1   F_10.jpg                        a woman in a red dress           0.870892   \n",
      "2   F_11.jpg     a woman standing in front of a white wall           0.871490   \n",
      "3   F_12.jpg        a woman wearing sunglasses and a skirt           0.768047   \n",
      "4   F_13.png                 a woman wearing a brown dress           0.799620   \n",
      "5   F_14.png                  a woman wearing a blue dress           0.869113   \n",
      "6   F_15.jpg                  a woman wearing a grey dress           0.767621   \n",
      "7   F_16.jpg                      a woman in a beige dress           0.709720   \n",
      "8   F_17.jpg                   a woman wearing a red dress           0.916152   \n",
      "9   F_18.png                       a woman in a brown suit           0.828032   \n",
      "10   F_2.jpg                      a woman in a white dress           0.886401   \n",
      "11   F_3.png                  a woman wearing a blue dress           0.876408   \n",
      "12   F_4.png                 a woman wearing a green dress           0.833341   \n",
      "13   F_5.png  a woman wearing a white shirt and blue jeans           0.824977   \n",
      "14   F_6.png                      a woman in a brown dress           0.870556   \n",
      "15   F_7.png                             a woman in a suit           0.801745   \n",
      "16   F_8.png                        a woman in a pink suit           0.779666   \n",
      "17   F_9.png                         a woman in a red suit           0.857704   \n",
      "\n",
      "                                                 Tags  \\\n",
      "0   [high heels, cocktail dress, blue dress, blue ...   \n",
      "1   [gala, business casual, red dress, bag, pearl ...   \n",
      "2   [sandal, blouse, casual dress, blue shirt, whi...   \n",
      "3   [casual dress, outdoor, a close up of a white ...   \n",
      "4   [day dress, polka dot, casual dress, cocktail ...   \n",
      "5   [day dress, street fashion, casual dress, patt...   \n",
      "6    [formal dress, sleeve, handbag, top, grey color]   \n",
      "7   [high heels, formal dress, day dress, pattern ...   \n",
      "8   [cocktail dress, business casual, sheath dress...   \n",
      "9   [handbag, blazer, trousers, fashion accessory,...   \n",
      "10  [high heels, formal dress, white dress, blonde...   \n",
      "11  [day dress, casual dress, pattern (fashion des...   \n",
      "12  [casual dress, footwear, green dress, tan heel...   \n",
      "13  [casual dress, jeans, street fashion, denim, t...   \n",
      "14  [blouse, brown dress, brown hair and brown eye...   \n",
      "15  [coat, blazer, handbag, trousers, pantsuit, fo...   \n",
      "16  [coat, outerwear, blazer, formal wear, pink su...   \n",
      "17  [high heels, cocktail dress, red, formal wear,...   \n",
      "\n",
      "                                     TagConfidenceMap  \n",
      "0   {'high heels': 0.907, 'cocktail dress': 0.915,...  \n",
      "1   {'gala': 0.915, 'business casual': None, 'red ...  \n",
      "2   {'sandal': 0.921, 'blouse': 0.876, 'casual dre...  \n",
      "3   {'casual dress': 0.841, 'outdoor': 0.93, 'a cl...  \n",
      "4   {'day dress': 0.964, 'polka dot': 0.892, 'casu...  \n",
      "5   {'day dress': 0.966, 'street fashion': 0.924, ...  \n",
      "6   {'formal dress': 0.943, 'sleeve': 0.898, 'hand...  \n",
      "7   {'high heels': 0.908, 'formal dress': 0.906, '...  \n",
      "8   {'cocktail dress': 0.955, 'business casual': 0...  \n",
      "9   {'handbag': 0.935, 'blazer': 0.924, 'trousers'...  \n",
      "10  {'high heels': 0.938, 'formal dress': 0.881, '...  \n",
      "11  {'day dress': 0.949, 'casual dress': 0.925, 'p...  \n",
      "12  {'casual dress': 0.897, 'footwear': 0.896, 'gr...  \n",
      "13  {'casual dress': 0.961, 'jeans': 0.96, 'street...  \n",
      "14  {'blouse': 0.848, 'brown dress': 0.871, 'brown...  \n",
      "15  {'coat': 0.971, 'blazer': 0.955, 'handbag': 0....  \n",
      "16  {'coat': 0.918, 'outerwear': 0.911, 'blazer': ...  \n",
      "17  {'high heels': 0.95, 'cocktail dress': 0.919, ...  \n"
     ]
    }
   ],
   "source": [
    "print(df[[\"ImageName\", \"Caption\", \"CaptionConfidence\", \"Tags\", \"TagConfidenceMap\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "873707c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images with keyword 'formal':\n",
      "   ImageName                                               Tags\n",
      "6   F_15.jpg   [formal dress, sleeve, handbag, top, grey color]\n",
      "7   F_16.jpg  [high heels, formal dress, day dress, pattern ...\n",
      "9   F_18.png  [handbag, blazer, trousers, fashion accessory,...\n",
      "10   F_2.jpg  [high heels, formal dress, white dress, blonde...\n",
      "15   F_7.png  [coat, blazer, handbag, trousers, pantsuit, fo...\n",
      "16   F_8.png  [coat, outerwear, blazer, formal wear, pink su...\n",
      "17   F_9.png  [high heels, cocktail dress, red, formal wear,...\n"
     ]
    }
   ],
   "source": [
    "# gather all image names whose tags contains keyword formal\n",
    "keyword = \"formal\"\n",
    "filtered_df = df[df['Tags'].apply(lambda tags: any(keyword in tag.lower() for tag in tags))]\n",
    "print(f\"Images with keyword '{keyword}':\")\n",
    "print(filtered_df[['ImageName', 'Tags']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "078b1ca4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User1 Tags:\n",
      "  [['handbag', 'blazer', 'trousers', 'fashion accessory', 'formal dress', 'trench coat', 'top', 'tan color', 'suit', 'black bag from a swinger', 'brown hair and a tan jacket', 'tan jacket', 'brown suit']]\n"
     ]
    }
   ],
   "source": [
    "# Get the tages of user1 \n",
    "user1_tags = df[df['ImageName'].str.contains('F_18', case=False, na=False)]['Tags'].tolist()\n",
    "print(f\"User1 Tags:\\n  {user1_tags}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d7fff3",
   "metadata": {},
   "source": [
    "### Recommender engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "74686c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended Items: For a friends' meeting, here are the top 3 suitable dress options from your wardrobe:\n",
      "\n",
      "1. **Item: F_14.png** – A blue casual dress with a pattern. It's stylish for street fashion and can be dressed up or down with accessories like a handbag or sandals.\n",
      "\n",
      "2. **Item: F_4.png** – A green casual dress paired with tan heels. This option is relaxed yet chic, perfect for spending time with friends.\n",
      "\n",
      "3. **Item: F_13.png** – A brown polka dot day dress. It's comfortable and fun, making it ideal for a friendly get-together.\n",
      "\n",
      "These outfits provide a balance of comfort and style for your occasion.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "# Azure OpenAI API setup\n",
    "# openai.api_key = azure_openai_key\n",
    "# openai.api_base = azure_openai_uri\n",
    "\n",
    "# # Sample Pandas DataFrame (this would be your wardrobe DataFrame)\n",
    "# data = {\n",
    "#     \"ImageName\": [\"M 10.jpg\", \"M 11.jpg\", \"M 12.jpg\"],\n",
    "#     \"Caption\": [\"a grey suit with a tie\", \"a red dress\", \"a blue blazer\"],\n",
    "#     \"Tags\": [\n",
    "#         [\"clothing\", \"coat\", \"blazer\", \"collar\", \"outerwear\", \"button\", \"person\", \"formal wear\", \"suit\", \"a grey suit with a tie\"],\n",
    "#         [\"clothing\", \"dress\", \"red dress\", \"formal wear\", \"elegant\", \"evening wear\", \"party dress\"],\n",
    "#         [\"clothing\", \"blazer\", \"jacket\", \"suit\", \"blue blazer\", \"formal wear\", \"professional\", \"business attire\"]\n",
    "#     ]\n",
    "# }\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# Function to interact with Azure OpenAI's ChatGPT Turbo and get suggestions\n",
    "def get_dress_recommendation(user_query: str, df: pd.DataFrame) -> str:\n",
    "    # Prepare the prompt for the model\n",
    "    # Based on the following wardrobe items, recommend the top 3 most suitable dress items for the query\n",
    "    prompt = f\"The user asked: '{user_query}'\\n\\n\"\n",
    "\n",
    "    # Adding the items in the DataFrame to the prompt for ChatGPT to consider\n",
    "    prompt += \"Here are the wardrobe items:\\n\"\n",
    "    for index, row in df.iterrows():\n",
    "        prompt += f\"Item: {row['ImageName']} \\t Caption:{row['Caption']} \\t Tags: {', '.join(row['Tags'])}\\n\"\n",
    "    \n",
    "    # Making the request to Azure OpenAI's ChatGPT Turbo\n",
    "    # response = openai.ChatCompletion.create(\n",
    "    #     engine=\"gpt-3.5-turbo\",  # or the engine name you are using in Azure\n",
    "    #     prompt=prompt,\n",
    "    #     max_tokens=150,\n",
    "    #     temperature=0.7\n",
    "    # )\n",
    "    client = OpenAI(\n",
    "    # This is the default and can be omitted\n",
    "    api_key=openai_key,\n",
    "    #api_base=azure_openai_uri, # Getting error with this line\n",
    "    #api_version=\"2023-05-15\",  # Use the appropriate API version\n",
    "    )\n",
    "\n",
    "    response = client.responses.create(\n",
    "        model=\"gpt-4o\",\n",
    "        instructions=\"You are a fashion assistant. Based on the user's query and wardrobe items, recommend the top 3 most suitable dress items.\",\n",
    "        input=prompt,\n",
    "        )\n",
    "\n",
    "    #print(response.output_text)\n",
    "    # Get the response text (recommendations)\n",
    "    #recommendations = response.choices[0].text.strip()\n",
    "    \n",
    "    return response.output_text\n",
    "\n",
    "# Example User Query\n",
    "#user_query = \"Recommend a dress for tomorrow's interview\"\n",
    "#user_query = \"Recommend a dress for gala dinner\"\n",
    "user_query = \"Recommend a dress for friends meeting\"\n",
    "\n",
    "#user_query = \"whats the weather like today?\"\n",
    "#user_query = \"share me about latest Iran and Israel war news\"\n",
    "\n",
    "\n",
    "# Get the recommendation from ChatGPT\n",
    "recommendation = get_dress_recommendation(user_query, df)\n",
    "\n",
    "print(\"Recommended Items:\", recommendation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471321fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
