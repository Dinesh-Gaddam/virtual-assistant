{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7195b1a9",
   "metadata": {},
   "source": [
    "# Code To test recomender engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dea5315",
   "metadata": {},
   "source": [
    "## Feature Extraction Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631bb520",
   "metadata": {},
   "source": [
    "### Load Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554b18a9",
   "metadata": {},
   "source": [
    "#### Load configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "764b21b1",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4bda834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vision URI: https://virtualstylistvisionservice.cognitiveservices.azure.com/\n",
      "Vision Key: 62aMM1K1uFmbyJWMCmmnnKzAAmz9A5hcDksR04CNuZVV7r7aguFLJQQJ99BFACYeBjFXJ3w3AAAFACOGkbDN\n"
     ]
    }
   ],
   "source": [
    "#### Load Configurations ####\n",
    "import json\n",
    "\n",
    "# Load configuration from JSON file\n",
    "def load_config(config_file='imageconfig.json'):\n",
    "    with open(config_file, 'r') as file:\n",
    "        config = json.load(file)\n",
    "    return config\n",
    "\n",
    "# Example of loading and using the configuration\n",
    "config = load_config()\n",
    "\n",
    "vision_uri = config['AzureVisionService']['Uri']\n",
    "vision_key = config['AzureVisionService']['Key']\n",
    "\n",
    "\n",
    "print(f\"Vision URI: {vision_uri}\")\n",
    "print(f\"Vision Key: {vision_key}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "11d6d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.vision.imageanalysis import ImageAnalysisClient\n",
    "from azure.ai.vision.imageanalysis.models import VisualFeatures\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from PIL import Image\n",
    "import io\n",
    "\n",
    "# Function to read image from path and analyze using Azure ImageAnalysisClient\n",
    "def analyze_image(image_path):\n",
    "    # Read image as bytes\n",
    "    with open(image_path, \"rb\") as image_file:\n",
    "        image_data = image_file.read()\n",
    "\n",
    "    # Create ImageAnalysisClient\n",
    "    client = ImageAnalysisClient(\n",
    "        endpoint=vision_uri,\n",
    "        credential=AzureKeyCredential(vision_key)\n",
    "    )\n",
    "\n",
    "    # Analyze image (example: describe)\n",
    "       # visual_features=[\"Description\"]\n",
    "    result = client.analyze(\n",
    "        image_data=image_data,\n",
    "    \n",
    "       visual_features =[\n",
    "        VisualFeatures.TAGS,\n",
    "        VisualFeatures.OBJECTS,\n",
    "        VisualFeatures.CAPTION,\n",
    "        VisualFeatures.DENSE_CAPTIONS,\n",
    "        VisualFeatures.READ,\n",
    "       # VisualFeatures.SMART_CROPS,\n",
    "        VisualFeatures.PEOPLE,\n",
    "    ]\n",
    "    )\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3cae3d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\dinesh_image_trails\\men_grey_suit.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a suit with a tie', 'confidence': 0.7532808184623718}, 'denseCaptionsResult': {'values': [{'text': 'a suit with a tie', 'confidence': 0.7532808184623718, 'boundingBox': {'x': 0, 'y': 0, 'w': 600, 'h': 700}}, {'text': 'a suit with a tie', 'confidence': 0.7210917472839355, 'boundingBox': {'x': 69, 'y': 15, 'w': 440, 'h': 669}}, {'text': 'a close up of a tie', 'confidence': 0.860759437084198, 'boundingBox': {'x': 267, 'y': 106, 'w': 62, 'h': 213}}]}, 'metadata': {'width': 600, 'height': 700}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9997585415840149}, {'name': 'coat', 'confidence': 0.9702774286270142}, {'name': 'collar', 'confidence': 0.9564415216445923}, {'name': 'outerwear', 'confidence': 0.9557503461837769}, {'name': 'blazer', 'confidence': 0.9508758187294006}, {'name': 'button', 'confidence': 0.9468828439712524}, {'name': 'person', 'confidence': 0.9405521154403687}, {'name': 'formal wear', 'confidence': 0.8835344314575195}, {'name': 'pocket', 'confidence': 0.8797683715820312}, {'name': 'shirt', 'confidence': 0.843208372592926}, {'name': 'wearing', 'confidence': 0.6651028990745544}, {'name': 'suit', 'confidence': 0.6221878528594971}, {'name': 'fabric', 'confidence': 0.5588575005531311}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 263, 'y': 102, 'w': 72, 'h': 225}, 'tags': [{'name': 'tie', 'confidence': 0.638}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 78, 'y': 10, 'w': 445, 'h': 689}, 'confidence': 0.9236836433410645}, {'boundingBox': {'x': 190, 'y': 263, 'w': 247, 'h': 175}, 'confidence': 0.001563498517498374}]}}\n",
      "Caption: a suit with a tie (Confidence: 0.7532808184623718)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9997585415840149)\n",
      "- coat (Confidence: 0.9702774286270142)\n",
      "- collar (Confidence: 0.9564415216445923)\n",
      "- outerwear (Confidence: 0.9557503461837769)\n",
      "- blazer (Confidence: 0.9508758187294006)\n",
      "- button (Confidence: 0.9468828439712524)\n",
      "- person (Confidence: 0.9405521154403687)\n",
      "- formal wear (Confidence: 0.8835344314575195)\n",
      "- pocket (Confidence: 0.8797683715820312)\n",
      "- shirt (Confidence: 0.843208372592926)\n",
      "- wearing (Confidence: 0.6651028990745544)\n",
      "- suit (Confidence: 0.6221878528594971)\n",
      "- fabric (Confidence: 0.5588575005531311)\n",
      "Dense Captions:\n",
      "- a suit with a tie (Confidence: 0.7532808184623718)\n",
      "- a suit with a tie (Confidence: 0.7210917472839355)\n",
      "- a close up of a tie (Confidence: 0.860759437084198)\n",
      "People:\n",
      "- BoundingBox: {'x': 78, 'y': 10, 'w': 445, 'h': 689}, Confidence: 0.9236836433410645\n",
      "- BoundingBox: {'x': 190, 'y': 263, 'w': 247, 'h': 175}, Confidence: 0.001563498517498374\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "# All tag extraction good except the category tag as per indian context\n",
    "#image_path = r\"D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 1.jpg\"\n",
    "image_path = r\"D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\dinesh_image_trails\\men_grey_suit.jpg\"\n",
    "\n",
    "print(f\"Analyzing image: {image_path}\")\n",
    "result = analyze_image(image_path=image_path)\n",
    "print(result)\n",
    "# Print caption\n",
    "if 'captionResult' in result:\n",
    "    caption = result['captionResult']\n",
    "    print(f\"Caption: {caption['text']} (Confidence: {caption['confidence']})\")\n",
    "\n",
    "# Print tags\n",
    "if 'tagsResult' in result and 'values' in result['tagsResult']:\n",
    "    print(\"Tags:\")\n",
    "    for tag in result['tagsResult']['values']:\n",
    "        print(f\"- {tag['name']} (Confidence: {tag['confidence']})\")\n",
    "\n",
    "# Print dense captions\n",
    "if 'denseCaptionsResult' in result and 'values' in result['denseCaptionsResult']:\n",
    "    print(\"Dense Captions:\")\n",
    "    for dense_caption in result['denseCaptionsResult']['values']:\n",
    "        print(f\"- {dense_caption['text']} (Confidence: {dense_caption['confidence']})\")\n",
    "\n",
    "# Print people\n",
    "if 'peopleResult' in result and 'values' in result['peopleResult']:\n",
    "    print(\"People:\")\n",
    "    for person in result['peopleResult']['values']:\n",
    "        bbox = person['boundingBox']\n",
    "        print(f\"- BoundingBox: {bbox}, Confidence: {person['confidence']}\")\n",
    "# Generate a caption for the image\n",
    "# if result.caption:\n",
    "#     print(f\"Caption: {result.caption.text} (Confidence: {result.caption.confidence})\")\n",
    "# # Print tags\n",
    "# if result.tags:\n",
    "#     print(\"Tags:\")\n",
    "#     for tag in result.tags:\n",
    "#         print(f\"- {tag.name} (Confidence: {tag.confidence})\")\n",
    "# # Print objects detected in the image\n",
    "# if result.objects:\n",
    "#     print(\"Objects:\")\n",
    "#     for obj in result.objects:\n",
    "#         print(f\"- {obj.object_property} (Confidence: {obj.confidence})\")\n",
    "# # Print dense captions\n",
    "# if result.dense_captions:\n",
    "#     print(\"Dense Captions:\")\n",
    "#     for dense_caption in result.dense_captions:\n",
    "#         print(f\"- {dense_caption.text} (Confidence: {dense_caption.confidence})\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "479b2400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read images from the specified directory\n",
    "def read_images_from_directory(directory):\n",
    "    from os import listdir\n",
    "    from os.path import isfile, join\n",
    "\n",
    "    image_files = [f for f in listdir(directory) if isfile(join(directory, f))]\n",
    "    return [join(directory, f) for f in image_files if f.lower().endswith(('.png', '.jpg', '.jpeg'))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a817cbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through images in a directory and analyze each one and extract tags\n",
    "def analyze_images_in_directory(directory):\n",
    "    image_paths = read_images_from_directory(directory)\n",
    "    for image_path in image_paths:\n",
    "        print(f\"Analyzing image: {image_path}\")\n",
    "        result = analyze_image(image_path=image_path)\n",
    "        print(result)\n",
    "        # Print caption\n",
    "        if 'captionResult' in result:\n",
    "            caption = result['captionResult']\n",
    "            print(f\"Caption: {caption['text']} (Confidence: {caption['confidence']})\")\n",
    "\n",
    "        # Print tags\n",
    "        if 'tagsResult' in result and 'values' in result['tagsResult']:\n",
    "            print(\"Tags:\")\n",
    "            for tag in result['tagsResult']['values']:\n",
    "                print(f\"- {tag['name']} (Confidence: {tag['confidence']})\")\n",
    "\n",
    "        if 'denseCaptionsResult' in result and 'values' in result['denseCaptionsResult']:\n",
    "            print(\"Dense Captions:\")\n",
    "            for dense_caption in result['denseCaptionsResult']['values']:\n",
    "                print(f\"- {dense_caption['text']} (Confidence: {dense_caption['confidence']})\")               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e5465d72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 1.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man leaning against a wall', 'confidence': 0.8148792386054993}, 'denseCaptionsResult': {'values': [{'text': 'a man leaning against a wall', 'confidence': 0.8148792386054993, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a close-up of a man wearing a white shirt', 'confidence': 0.8405238389968872, 'boundingBox': {'x': 34, 'y': 116, 'w': 252, 'h': 651}}, {'text': 'a person wearing a white shirt and black pants', 'confidence': 0.7267715930938721, 'boundingBox': {'x': 52, 'y': 617, 'w': 208, 'h': 180}}, {'text': 'a close up of a button', 'confidence': 0.8260501623153687, 'boundingBox': {'x': 94, 'y': 309, 'w': 92, 'h': 228}}, {'text': 'a man with curly hair', 'confidence': 0.7204979658126831, 'boundingBox': {'x': 83, 'y': 107, 'w': 125, 'h': 200}}, {'text': 'a man in a white shirt', 'confidence': 0.7812774181365967, 'boundingBox': {'x': 42, 'y': 104, 'w': 234, 'h': 351}}, {'text': \"a close up of a man's chin\", 'confidence': 0.8233923316001892, 'boundingBox': {'x': 86, 'y': 270, 'w': 102, 'h': 82}}, {'text': \"a close-up of a person's back\", 'confidence': 0.7886658310890198, 'boundingBox': {'x': 197, 'y': 57, 'w': 195, 'h': 552}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9972237348556519}, {'name': 'person', 'confidence': 0.9914588928222656}, {'name': 'collar', 'confidence': 0.9730098247528076}, {'name': 'dress shirt', 'confidence': 0.9620614051818848}, {'name': 'sleeve', 'confidence': 0.96200031042099}, {'name': 'top', 'confidence': 0.9515008330345154}, {'name': 'human face', 'confidence': 0.9417112469673157}, {'name': 'standing', 'confidence': 0.9375580549240112}, {'name': 'wall', 'confidence': 0.932641863822937}, {'name': 'button', 'confidence': 0.9166678190231323}, {'name': 'shoulder', 'confidence': 0.9123274087905884}, {'name': 'pocket', 'confidence': 0.9122952222824097}, {'name': 'casual dress', 'confidence': 0.8992321491241455}, {'name': 'trousers', 'confidence': 0.8959338665008545}, {'name': 'man', 'confidence': 0.8826649188995361}, {'name': 'neck', 'confidence': 0.8707128763198853}, {'name': 'male person', 'confidence': 0.8586791157722473}, {'name': 'outerwear', 'confidence': 0.8582819700241089}, {'name': 'indoor', 'confidence': 0.8502939939498901}, {'name': 'wearing', 'confidence': 0.7274115681648254}, {'name': 'shirt', 'confidence': 0.6718450784683228}, {'name': 'fabric', 'confidence': 0.5534815788269043}, {'name': 'fashion', 'confidence': 0.4169178009033203}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 34, 'y': 107, 'w': 257, 'h': 692}, 'confidence': 0.9401260018348694}, {'boundingBox': {'x': 247, 'y': 524, 'w': 151, 'h': 274}, 'confidence': 0.004138357937335968}, {'boundingBox': {'x': 47, 'y': 371, 'w': 225, 'h': 201}, 'confidence': 0.003791073802858591}]}}\n",
      "Caption: a man leaning against a wall (Confidence: 0.8148792386054993)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9972237348556519)\n",
      "- person (Confidence: 0.9914588928222656)\n",
      "- collar (Confidence: 0.9730098247528076)\n",
      "- dress shirt (Confidence: 0.9620614051818848)\n",
      "- sleeve (Confidence: 0.96200031042099)\n",
      "- top (Confidence: 0.9515008330345154)\n",
      "- human face (Confidence: 0.9417112469673157)\n",
      "- standing (Confidence: 0.9375580549240112)\n",
      "- wall (Confidence: 0.932641863822937)\n",
      "- button (Confidence: 0.9166678190231323)\n",
      "- shoulder (Confidence: 0.9123274087905884)\n",
      "- pocket (Confidence: 0.9122952222824097)\n",
      "- casual dress (Confidence: 0.8992321491241455)\n",
      "- trousers (Confidence: 0.8959338665008545)\n",
      "- man (Confidence: 0.8826649188995361)\n",
      "- neck (Confidence: 0.8707128763198853)\n",
      "- male person (Confidence: 0.8586791157722473)\n",
      "- outerwear (Confidence: 0.8582819700241089)\n",
      "- indoor (Confidence: 0.8502939939498901)\n",
      "- wearing (Confidence: 0.7274115681648254)\n",
      "- shirt (Confidence: 0.6718450784683228)\n",
      "- fabric (Confidence: 0.5534815788269043)\n",
      "- fashion (Confidence: 0.4169178009033203)\n",
      "Dense Captions:\n",
      "- a man leaning against a wall (Confidence: 0.8148792386054993)\n",
      "- a close-up of a man wearing a white shirt (Confidence: 0.8405238389968872)\n",
      "- a person wearing a white shirt and black pants (Confidence: 0.7267715930938721)\n",
      "- a close up of a button (Confidence: 0.8260501623153687)\n",
      "- a man with curly hair (Confidence: 0.7204979658126831)\n",
      "- a man in a white shirt (Confidence: 0.7812774181365967)\n",
      "- a close up of a man's chin (Confidence: 0.8233923316001892)\n",
      "- a close-up of a person's back (Confidence: 0.7886658310890198)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 10.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man holding a bag', 'confidence': 0.8175417184829712}, 'denseCaptionsResult': {'values': [{'text': 'a man holding a bag', 'confidence': 0.8175417184829712, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man in a white shirt and black pants holding a briefcase', 'confidence': 0.8099160194396973, 'boundingBox': {'x': 144, 'y': 16, 'w': 185, 'h': 736}}, {'text': 'a pair of black shoes', 'confidence': 0.7699841856956482, 'boundingBox': {'x': 160, 'y': 683, 'w': 134, 'h': 81}}, {'text': 'a man in a white shirt', 'confidence': 0.8475863933563232, 'boundingBox': {'x': 153, 'y': 123, 'w': 180, 'h': 199}}, {'text': \"a person's legs in black pants\", 'confidence': 0.7826703786849976, 'boundingBox': {'x': 168, 'y': 300, 'w': 120, 'h': 403}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9953299164772034}, {'name': 'person', 'confidence': 0.993445098400116}, {'name': 'trousers', 'confidence': 0.9737942218780518}, {'name': 'standing', 'confidence': 0.9693556427955627}, {'name': 'shirt', 'confidence': 0.9427850246429443}, {'name': 'pocket', 'confidence': 0.9381670951843262}, {'name': 'shoulder', 'confidence': 0.9375288486480713}, {'name': 'sleeve', 'confidence': 0.932364821434021}, {'name': 'joint', 'confidence': 0.920040488243103}, {'name': 'top', 'confidence': 0.9112091660499573}, {'name': 'casual dress', 'confidence': 0.90833580493927}, {'name': 'footwear', 'confidence': 0.906639814376831}, {'name': 'waist', 'confidence': 0.9036440849304199}, {'name': 'elbow', 'confidence': 0.886120080947876}, {'name': 'collar', 'confidence': 0.8857921361923218}, {'name': 'dress shirt', 'confidence': 0.8642368316650391}, {'name': 'man', 'confidence': 0.8568055033683777}, {'name': 'wall', 'confidence': 0.72083580493927}, {'name': 'trouser', 'confidence': 0.7076188325881958}, {'name': 'fabric', 'confidence': 0.6164069175720215}, {'name': 'indoor', 'confidence': 0.5344501733779907}, {'name': 'fashion', 'confidence': 0.5185784101486206}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 154, 'y': 30, 'w': 191, 'h': 482}, 'tags': [{'name': 'person', 'confidence': 0.624}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 149, 'y': 23, 'w': 191, 'h': 743}, 'confidence': 0.9331282377243042}, {'boundingBox': {'x': 156, 'y': 290, 'w': 167, 'h': 214}, 'confidence': 0.002489805221557617}]}}\n",
      "Caption: a man holding a bag (Confidence: 0.8175417184829712)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9953299164772034)\n",
      "- person (Confidence: 0.993445098400116)\n",
      "- trousers (Confidence: 0.9737942218780518)\n",
      "- standing (Confidence: 0.9693556427955627)\n",
      "- shirt (Confidence: 0.9427850246429443)\n",
      "- pocket (Confidence: 0.9381670951843262)\n",
      "- shoulder (Confidence: 0.9375288486480713)\n",
      "- sleeve (Confidence: 0.932364821434021)\n",
      "- joint (Confidence: 0.920040488243103)\n",
      "- top (Confidence: 0.9112091660499573)\n",
      "- casual dress (Confidence: 0.90833580493927)\n",
      "- footwear (Confidence: 0.906639814376831)\n",
      "- waist (Confidence: 0.9036440849304199)\n",
      "- elbow (Confidence: 0.886120080947876)\n",
      "- collar (Confidence: 0.8857921361923218)\n",
      "- dress shirt (Confidence: 0.8642368316650391)\n",
      "- man (Confidence: 0.8568055033683777)\n",
      "- wall (Confidence: 0.72083580493927)\n",
      "- trouser (Confidence: 0.7076188325881958)\n",
      "- fabric (Confidence: 0.6164069175720215)\n",
      "- indoor (Confidence: 0.5344501733779907)\n",
      "- fashion (Confidence: 0.5185784101486206)\n",
      "Dense Captions:\n",
      "- a man holding a bag (Confidence: 0.8175417184829712)\n",
      "- a man in a white shirt and black pants holding a briefcase (Confidence: 0.8099160194396973)\n",
      "- a pair of black shoes (Confidence: 0.7699841856956482)\n",
      "- a man in a white shirt (Confidence: 0.8475863933563232)\n",
      "- a person's legs in black pants (Confidence: 0.7826703786849976)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 11.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man wearing a dark blue shirt', 'confidence': 0.7493599653244019}, 'denseCaptionsResult': {'values': [{'text': 'a man wearing a dark blue shirt', 'confidence': 0.7493599653244019, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man wearing a dark blue shirt', 'confidence': 0.7627872824668884, 'boundingBox': {'x': 41, 'y': 0, 'w': 286, 'h': 775}}, {'text': \"a close-up of a man's pants\", 'confidence': 0.8383514881134033, 'boundingBox': {'x': 81, 'y': 513, 'w': 204, 'h': 267}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9998487234115601}, {'name': 'person', 'confidence': 0.9888368248939514}, {'name': 'collar', 'confidence': 0.9673699736595154}, {'name': 'top', 'confidence': 0.963508129119873}, {'name': 'sleeve', 'confidence': 0.9612844586372375}, {'name': 'pocket', 'confidence': 0.9587752819061279}, {'name': 'fabric', 'confidence': 0.9457298517227173}, {'name': 'casual dress', 'confidence': 0.9343644380569458}, {'name': 'shoulder', 'confidence': 0.9166061878204346}, {'name': 'button', 'confidence': 0.9093574285507202}, {'name': 'dress shirt', 'confidence': 0.9022173881530762}, {'name': 'standing', 'confidence': 0.8984347581863403}, {'name': 'trousers', 'confidence': 0.8963309526443481}, {'name': 'outerwear', 'confidence': 0.8864849805831909}, {'name': 'man', 'confidence': 0.8751082420349121}, {'name': 'neck', 'confidence': 0.8621441125869751}, {'name': 'waist', 'confidence': 0.8516270518302917}, {'name': 'wall', 'confidence': 0.7486701011657715}, {'name': 'indoor', 'confidence': 0.667488694190979}, {'name': 'wearing', 'confidence': 0.5598333477973938}, {'name': 'shirt', 'confidence': 0.5257371068000793}, {'name': 'fashion', 'confidence': 0.4096224904060364}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 47, 'y': 3, 'w': 293, 'h': 781}, 'confidence': 0.9528812766075134}, {'boundingBox': {'x': 67, 'y': 292, 'w': 250, 'h': 206}, 'confidence': 0.0027367891743779182}]}}\n",
      "Caption: a man wearing a dark blue shirt (Confidence: 0.7493599653244019)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9998487234115601)\n",
      "- person (Confidence: 0.9888368248939514)\n",
      "- collar (Confidence: 0.9673699736595154)\n",
      "- top (Confidence: 0.963508129119873)\n",
      "- sleeve (Confidence: 0.9612844586372375)\n",
      "- pocket (Confidence: 0.9587752819061279)\n",
      "- fabric (Confidence: 0.9457298517227173)\n",
      "- casual dress (Confidence: 0.9343644380569458)\n",
      "- shoulder (Confidence: 0.9166061878204346)\n",
      "- button (Confidence: 0.9093574285507202)\n",
      "- dress shirt (Confidence: 0.9022173881530762)\n",
      "- standing (Confidence: 0.8984347581863403)\n",
      "- trousers (Confidence: 0.8963309526443481)\n",
      "- outerwear (Confidence: 0.8864849805831909)\n",
      "- man (Confidence: 0.8751082420349121)\n",
      "- neck (Confidence: 0.8621441125869751)\n",
      "- waist (Confidence: 0.8516270518302917)\n",
      "- wall (Confidence: 0.7486701011657715)\n",
      "- indoor (Confidence: 0.667488694190979)\n",
      "- wearing (Confidence: 0.5598333477973938)\n",
      "- shirt (Confidence: 0.5257371068000793)\n",
      "- fashion (Confidence: 0.4096224904060364)\n",
      "Dense Captions:\n",
      "- a man wearing a dark blue shirt (Confidence: 0.7493599653244019)\n",
      "- a man wearing a dark blue shirt (Confidence: 0.7627872824668884)\n",
      "- a close-up of a man's pants (Confidence: 0.8383514881134033)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 12.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in a white shirt', 'confidence': 0.7291719913482666}, 'denseCaptionsResult': {'values': [{'text': 'a man in a white shirt', 'confidence': 0.7291719913482666, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man wearing a white shirt', 'confidence': 0.7359451055526733, 'boundingBox': {'x': 59, 'y': 36, 'w': 250, 'h': 751}}, {'text': 'a man with curly hair looking up', 'confidence': 0.7422065734863281, 'boundingBox': {'x': 136, 'y': 44, 'w': 127, 'h': 172}}, {'text': \"a close up of a man's pants\", 'confidence': 0.8002896308898926, 'boundingBox': {'x': 95, 'y': 506, 'w': 192, 'h': 286}}, {'text': 'a close up of a box', 'confidence': 0.7477977275848389, 'boundingBox': {'x': 114, 'y': 506, 'w': 154, 'h': 58}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9996604919433594}, {'name': 'person', 'confidence': 0.987277090549469}, {'name': 'shirt', 'confidence': 0.9842140078544617}, {'name': 'collar', 'confidence': 0.9723389744758606}, {'name': 'fabric', 'confidence': 0.9527124166488647}, {'name': 'sleeve', 'confidence': 0.9502647519111633}, {'name': 'pocket', 'confidence': 0.9389969110488892}, {'name': 'button', 'confidence': 0.9379044771194458}, {'name': 'dress shirt', 'confidence': 0.9310948848724365}, {'name': 'casual dress', 'confidence': 0.9246662855148315}, {'name': 'top', 'confidence': 0.9230414032936096}, {'name': 'trousers', 'confidence': 0.9227699637413025}, {'name': 'shoulder', 'confidence': 0.8774887323379517}, {'name': 'man', 'confidence': 0.8602543473243713}, {'name': 'outerwear', 'confidence': 0.8517366647720337}, {'name': 'khaki', 'confidence': 0.8410894870758057}, {'name': 'fashion', 'confidence': 0.8018125295639038}, {'name': 'standing', 'confidence': 0.6191888451576233}, {'name': 'pants', 'confidence': 0.5875991582870483}, {'name': 'wearing', 'confidence': 0.5477477312088013}, {'name': 'indoor', 'confidence': 0.5437765717506409}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 64, 'y': 45, 'w': 257, 'h': 754}, 'confidence': 0.9358143210411072}, {'boundingBox': {'x': 79, 'y': 328, 'w': 225, 'h': 216}, 'confidence': 0.0027702420484274626}]}}\n",
      "Caption: a man in a white shirt (Confidence: 0.7291719913482666)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9996604919433594)\n",
      "- person (Confidence: 0.987277090549469)\n",
      "- shirt (Confidence: 0.9842140078544617)\n",
      "- collar (Confidence: 0.9723389744758606)\n",
      "- fabric (Confidence: 0.9527124166488647)\n",
      "- sleeve (Confidence: 0.9502647519111633)\n",
      "- pocket (Confidence: 0.9389969110488892)\n",
      "- button (Confidence: 0.9379044771194458)\n",
      "- dress shirt (Confidence: 0.9310948848724365)\n",
      "- casual dress (Confidence: 0.9246662855148315)\n",
      "- top (Confidence: 0.9230414032936096)\n",
      "- trousers (Confidence: 0.9227699637413025)\n",
      "- shoulder (Confidence: 0.8774887323379517)\n",
      "- man (Confidence: 0.8602543473243713)\n",
      "- outerwear (Confidence: 0.8517366647720337)\n",
      "- khaki (Confidence: 0.8410894870758057)\n",
      "- fashion (Confidence: 0.8018125295639038)\n",
      "- standing (Confidence: 0.6191888451576233)\n",
      "- pants (Confidence: 0.5875991582870483)\n",
      "- wearing (Confidence: 0.5477477312088013)\n",
      "- indoor (Confidence: 0.5437765717506409)\n",
      "Dense Captions:\n",
      "- a man in a white shirt (Confidence: 0.7291719913482666)\n",
      "- a man wearing a white shirt (Confidence: 0.7359451055526733)\n",
      "- a man with curly hair looking up (Confidence: 0.7422065734863281)\n",
      "- a close up of a man's pants (Confidence: 0.8002896308898926)\n",
      "- a close up of a box (Confidence: 0.7477977275848389)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 13.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man wearing a grey shirt', 'confidence': 0.7673459649085999}, 'denseCaptionsResult': {'values': [{'text': 'a man wearing a grey shirt', 'confidence': 0.7673459649085999, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man wearing a black shirt', 'confidence': 0.711184024810791, 'boundingBox': {'x': 45, 'y': 3, 'w': 276, 'h': 775}}, {'text': 'a person in a black shirt', 'confidence': 0.6043955087661743, 'boundingBox': {'x': 48, 'y': 565, 'w': 230, 'h': 222}}, {'text': 'a man with dark hair', 'confidence': 0.7229440212249756, 'boundingBox': {'x': 108, 'y': 7, 'w': 122, 'h': 193}}, {'text': \"a close up of a man's face\", 'confidence': 0.8682374954223633, 'boundingBox': {'x': 117, 'y': 11, 'w': 106, 'h': 87}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9993646144866943}, {'name': 'person', 'confidence': 0.9658459424972534}, {'name': 'collar', 'confidence': 0.9620420932769775}, {'name': 'sleeve', 'confidence': 0.9613888263702393}, {'name': 'top', 'confidence': 0.9569550156593323}, {'name': 'shirt', 'confidence': 0.9491689801216125}, {'name': 'casual dress', 'confidence': 0.9238112568855286}, {'name': 'button', 'confidence': 0.9169801473617554}, {'name': 'pocket', 'confidence': 0.9139989018440247}, {'name': 'man', 'confidence': 0.8993128538131714}, {'name': 'dress shirt', 'confidence': 0.8882888555526733}, {'name': 'neck', 'confidence': 0.8790273666381836}, {'name': 'fabric', 'confidence': 0.8758178949356079}, {'name': 'shoulder', 'confidence': 0.8727924823760986}, {'name': 'standing', 'confidence': 0.8536173105239868}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 52, 'y': 11, 'w': 281, 'h': 780}, 'confidence': 0.9514544606208801}, {'boundingBox': {'x': 72, 'y': 298, 'w': 241, 'h': 203}, 'confidence': 0.0028923270292580128}]}}\n",
      "Caption: a man wearing a grey shirt (Confidence: 0.7673459649085999)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9993646144866943)\n",
      "- person (Confidence: 0.9658459424972534)\n",
      "- collar (Confidence: 0.9620420932769775)\n",
      "- sleeve (Confidence: 0.9613888263702393)\n",
      "- top (Confidence: 0.9569550156593323)\n",
      "- shirt (Confidence: 0.9491689801216125)\n",
      "- casual dress (Confidence: 0.9238112568855286)\n",
      "- button (Confidence: 0.9169801473617554)\n",
      "- pocket (Confidence: 0.9139989018440247)\n",
      "- man (Confidence: 0.8993128538131714)\n",
      "- dress shirt (Confidence: 0.8882888555526733)\n",
      "- neck (Confidence: 0.8790273666381836)\n",
      "- fabric (Confidence: 0.8758178949356079)\n",
      "- shoulder (Confidence: 0.8727924823760986)\n",
      "- standing (Confidence: 0.8536173105239868)\n",
      "Dense Captions:\n",
      "- a man wearing a grey shirt (Confidence: 0.7673459649085999)\n",
      "- a man wearing a black shirt (Confidence: 0.711184024810791)\n",
      "- a person in a black shirt (Confidence: 0.6043955087661743)\n",
      "- a man with dark hair (Confidence: 0.7229440212249756)\n",
      "- a close up of a man's face (Confidence: 0.8682374954223633)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 2.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in a white shirt', 'confidence': 0.8360416889190674}, 'denseCaptionsResult': {'values': [{'text': 'a man in a white shirt', 'confidence': 0.8360416889190674, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man in a white shirt', 'confidence': 0.7833253741264343, 'boundingBox': {'x': 87, 'y': 14, 'w': 277, 'h': 761}}, {'text': \"a close-up of a man's hands in his pockets\", 'confidence': 0.8407101035118103, 'boundingBox': {'x': 128, 'y': 537, 'w': 155, 'h': 245}}, {'text': 'a man with curly hair', 'confidence': 0.7659653425216675, 'boundingBox': {'x': 94, 'y': 19, 'w': 131, 'h': 190}}, {'text': 'a close up of a white shirt', 'confidence': 0.8325155377388, 'boundingBox': {'x': 148, 'y': 238, 'w': 87, 'h': 252}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9982713460922241}, {'name': 'person', 'confidence': 0.9915043115615845}, {'name': 'sleeve', 'confidence': 0.9755033254623413}, {'name': 'collar', 'confidence': 0.973625898361206}, {'name': 'top', 'confidence': 0.9486327767372131}, {'name': 'dress shirt', 'confidence': 0.9483451247215271}, {'name': 'casual dress', 'confidence': 0.9321959018707275}, {'name': 'shoulder', 'confidence': 0.9292988777160645}, {'name': 'pocket', 'confidence': 0.92301344871521}, {'name': 'button', 'confidence': 0.9054025411605835}, {'name': 'trousers', 'confidence': 0.8989773392677307}, {'name': 'waist', 'confidence': 0.8724199533462524}, {'name': 'neck', 'confidence': 0.8538874983787537}, {'name': 'blouse', 'confidence': 0.848712682723999}, {'name': 'wall', 'confidence': 0.8431991338729858}, {'name': 'fabric', 'confidence': 0.7710107564926147}, {'name': 'standing', 'confidence': 0.6576583385467529}, {'name': 'shirt', 'confidence': 0.6560342311859131}, {'name': 'indoor', 'confidence': 0.531450092792511}, {'name': 'fashion', 'confidence': 0.44102954864501953}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': [{'lines': [{'text': '>', 'boundingPolygon': [{'x': 382, 'y': 386}, {'x': 392, 'y': 384}, {'x': 394, 'y': 405}, {'x': 383, 'y': 407}], 'words': [{'text': '>', 'boundingPolygon': [{'x': 382, 'y': 386}, {'x': 390, 'y': 384}, {'x': 394, 'y': 404}, {'x': 383, 'y': 406}], 'confidence': 0.82}]}]}]}, 'peopleResult': {'values': [{'boundingBox': {'x': 95, 'y': 17, 'w': 284, 'h': 769}, 'confidence': 0.9611490964889526}, {'boundingBox': {'x': 114, 'y': 294, 'w': 244, 'h': 211}, 'confidence': 0.003012124914675951}]}}\n",
      "Caption: a man in a white shirt (Confidence: 0.8360416889190674)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9982713460922241)\n",
      "- person (Confidence: 0.9915043115615845)\n",
      "- sleeve (Confidence: 0.9755033254623413)\n",
      "- collar (Confidence: 0.973625898361206)\n",
      "- top (Confidence: 0.9486327767372131)\n",
      "- dress shirt (Confidence: 0.9483451247215271)\n",
      "- casual dress (Confidence: 0.9321959018707275)\n",
      "- shoulder (Confidence: 0.9292988777160645)\n",
      "- pocket (Confidence: 0.92301344871521)\n",
      "- button (Confidence: 0.9054025411605835)\n",
      "- trousers (Confidence: 0.8989773392677307)\n",
      "- waist (Confidence: 0.8724199533462524)\n",
      "- neck (Confidence: 0.8538874983787537)\n",
      "- blouse (Confidence: 0.848712682723999)\n",
      "- wall (Confidence: 0.8431991338729858)\n",
      "- fabric (Confidence: 0.7710107564926147)\n",
      "- standing (Confidence: 0.6576583385467529)\n",
      "- shirt (Confidence: 0.6560342311859131)\n",
      "- indoor (Confidence: 0.531450092792511)\n",
      "- fashion (Confidence: 0.44102954864501953)\n",
      "Dense Captions:\n",
      "- a man in a white shirt (Confidence: 0.8360416889190674)\n",
      "- a man in a white shirt (Confidence: 0.7833253741264343)\n",
      "- a close-up of a man's hands in his pockets (Confidence: 0.8407101035118103)\n",
      "- a man with curly hair (Confidence: 0.7659653425216675)\n",
      "- a close up of a white shirt (Confidence: 0.8325155377388)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 3.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in a green shirt', 'confidence': 0.7356724143028259}, 'denseCaptionsResult': {'values': [{'text': 'a man in a green shirt', 'confidence': 0.7356724143028259, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man wearing khaki pants', 'confidence': 0.7978602051734924, 'boundingBox': {'x': 138, 'y': 21, 'w': 143, 'h': 762}}, {'text': 'a close up of a white shoe', 'confidence': 0.7806796431541443, 'boundingBox': {'x': 142, 'y': 721, 'w': 141, 'h': 66}}, {'text': 'a man wearing a green shirt', 'confidence': 0.7747616767883301, 'boundingBox': {'x': 130, 'y': 108, 'w': 148, 'h': 297}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9992477893829346}, {'name': 'person', 'confidence': 0.9809218645095825}, {'name': 'trousers', 'confidence': 0.9217740893363953}, {'name': 'waist', 'confidence': 0.9148560762405396}, {'name': 'shoulder', 'confidence': 0.8978973627090454}, {'name': 'top', 'confidence': 0.8913607597351074}, {'name': 'pocket', 'confidence': 0.8835198879241943}, {'name': 'joint', 'confidence': 0.8776378035545349}, {'name': 'wall', 'confidence': 0.8712863922119141}, {'name': 'casual dress', 'confidence': 0.8647650480270386}, {'name': 'sleeve', 'confidence': 0.8594068884849548}, {'name': 'trouser', 'confidence': 0.8013904690742493}, {'name': 'indoor', 'confidence': 0.7438921332359314}, {'name': 'standing', 'confidence': 0.7070565223693848}, {'name': 'pants', 'confidence': 0.6290770173072815}, {'name': 'fabric', 'confidence': 0.5820688605308533}, {'name': 'fashion', 'confidence': 0.5807902812957764}, {'name': 'man', 'confidence': 0.49981749057769775}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 125, 'y': 28, 'w': 162, 'h': 484}, 'tags': [{'name': 'person', 'confidence': 0.768}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 124, 'y': 9, 'w': 172, 'h': 779}, 'confidence': 0.03450183570384979}]}}\n",
      "Caption: a man in a green shirt (Confidence: 0.7356724143028259)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9992477893829346)\n",
      "- person (Confidence: 0.9809218645095825)\n",
      "- trousers (Confidence: 0.9217740893363953)\n",
      "- waist (Confidence: 0.9148560762405396)\n",
      "- shoulder (Confidence: 0.8978973627090454)\n",
      "- top (Confidence: 0.8913607597351074)\n",
      "- pocket (Confidence: 0.8835198879241943)\n",
      "- joint (Confidence: 0.8776378035545349)\n",
      "- wall (Confidence: 0.8712863922119141)\n",
      "- casual dress (Confidence: 0.8647650480270386)\n",
      "- sleeve (Confidence: 0.8594068884849548)\n",
      "- trouser (Confidence: 0.8013904690742493)\n",
      "- indoor (Confidence: 0.7438921332359314)\n",
      "- standing (Confidence: 0.7070565223693848)\n",
      "- pants (Confidence: 0.6290770173072815)\n",
      "- fabric (Confidence: 0.5820688605308533)\n",
      "- fashion (Confidence: 0.5807902812957764)\n",
      "- man (Confidence: 0.49981749057769775)\n",
      "Dense Captions:\n",
      "- a man in a green shirt (Confidence: 0.7356724143028259)\n",
      "- a man wearing khaki pants (Confidence: 0.7978602051734924)\n",
      "- a close up of a white shoe (Confidence: 0.7806796431541443)\n",
      "- a man wearing a green shirt (Confidence: 0.7747616767883301)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 4.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man standing with his hands in his pockets', 'confidence': 0.8231262564659119}, 'denseCaptionsResult': {'values': [{'text': 'a man standing with his hands in his pockets', 'confidence': 0.8231262564659119, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man in a pink shirt and white pants', 'confidence': 0.7540850639343262, 'boundingBox': {'x': 110, 'y': 14, 'w': 164, 'h': 760}}, {'text': 'a man in a pink shirt', 'confidence': 0.8181167840957642, 'boundingBox': {'x': 111, 'y': 123, 'w': 165, 'h': 208}}, {'text': 'a close up of a pair of boots', 'confidence': 0.8018085956573486, 'boundingBox': {'x': 176, 'y': 728, 'w': 95, 'h': 54}}, {'text': \"a close up of a person's legs\", 'confidence': 0.7844464778900146, 'boundingBox': {'x': 133, 'y': 314, 'w': 136, 'h': 449}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9994328618049622}, {'name': 'person', 'confidence': 0.9904530048370361}, {'name': 'trousers', 'confidence': 0.9751688241958618}, {'name': 'shirt', 'confidence': 0.9458872079849243}, {'name': 'trouser', 'confidence': 0.9301143884658813}, {'name': 'pocket', 'confidence': 0.9209855794906616}, {'name': 'casual dress', 'confidence': 0.9181965589523315}, {'name': 'sleeve', 'confidence': 0.9057955741882324}, {'name': 'fabric', 'confidence': 0.9035808444023132}, {'name': 'top', 'confidence': 0.8730406165122986}, {'name': 'collar', 'confidence': 0.8704959154129028}, {'name': 'shoulder', 'confidence': 0.8410294651985168}, {'name': 'pants', 'confidence': 0.6110646724700928}, {'name': 'standing', 'confidence': 0.6070533394813538}, {'name': 'fashion', 'confidence': 0.5477150678634644}, {'name': 'indoor', 'confidence': 0.5269810557365417}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 110, 'y': 37, 'w': 172, 'h': 473}, 'tags': [{'name': 'person', 'confidence': 0.671}]}, {'boundingBox': {'x': 130, 'y': 295, 'w': 156, 'h': 476}, 'tags': [{'name': 'Jeans', 'confidence': 0.584}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 111, 'y': 20, 'w': 175, 'h': 766}, 'confidence': 0.8369060754776001}, {'boundingBox': {'x': 169, 'y': 0, 'w': 55, 'h': 15}, 'confidence': 0.036125894635915756}, {'boundingBox': {'x': 210, 'y': 0, 'w': 63, 'h': 15}, 'confidence': 0.023617412894964218}, {'boundingBox': {'x': 117, 'y': 294, 'w': 153, 'h': 207}, 'confidence': 0.00196420238353312}]}}\n",
      "Caption: a man standing with his hands in his pockets (Confidence: 0.8231262564659119)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9994328618049622)\n",
      "- person (Confidence: 0.9904530048370361)\n",
      "- trousers (Confidence: 0.9751688241958618)\n",
      "- shirt (Confidence: 0.9458872079849243)\n",
      "- trouser (Confidence: 0.9301143884658813)\n",
      "- pocket (Confidence: 0.9209855794906616)\n",
      "- casual dress (Confidence: 0.9181965589523315)\n",
      "- sleeve (Confidence: 0.9057955741882324)\n",
      "- fabric (Confidence: 0.9035808444023132)\n",
      "- top (Confidence: 0.8730406165122986)\n",
      "- collar (Confidence: 0.8704959154129028)\n",
      "- shoulder (Confidence: 0.8410294651985168)\n",
      "- pants (Confidence: 0.6110646724700928)\n",
      "- standing (Confidence: 0.6070533394813538)\n",
      "- fashion (Confidence: 0.5477150678634644)\n",
      "- indoor (Confidence: 0.5269810557365417)\n",
      "Dense Captions:\n",
      "- a man standing with his hands in his pockets (Confidence: 0.8231262564659119)\n",
      "- a man in a pink shirt and white pants (Confidence: 0.7540850639343262)\n",
      "- a man in a pink shirt (Confidence: 0.8181167840957642)\n",
      "- a close up of a pair of boots (Confidence: 0.8018085956573486)\n",
      "- a close up of a person's legs (Confidence: 0.7844464778900146)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 5.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in a white shirt', 'confidence': 0.79832923412323}, 'denseCaptionsResult': {'values': [{'text': 'a man in a white shirt', 'confidence': 0.7981970310211182, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man wearing a white shirt', 'confidence': 0.7824667692184448, 'boundingBox': {'x': 95, 'y': 14, 'w': 277, 'h': 771}}, {'text': 'a person wearing black pants', 'confidence': 0.7746246457099915, 'boundingBox': {'x': 123, 'y': 500, 'w': 194, 'h': 288}}, {'text': 'a man with curly hair', 'confidence': 0.7465900182723999, 'boundingBox': {'x': 170, 'y': 22, 'w': 113, 'h': 168}}, {'text': \"a close up of a man's face\", 'confidence': 0.8185763359069824, 'boundingBox': {'x': 179, 'y': 20, 'w': 97, 'h': 92}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9988824725151062}, {'name': 'person', 'confidence': 0.9910719394683838}, {'name': 'dress shirt', 'confidence': 0.9599442481994629}, {'name': 'collar', 'confidence': 0.9541717767715454}, {'name': 'sleeve', 'confidence': 0.9477725028991699}, {'name': 'top', 'confidence': 0.9443472623825073}, {'name': 'standing', 'confidence': 0.9328851699829102}, {'name': 'trousers', 'confidence': 0.9319429397583008}, {'name': 'pocket', 'confidence': 0.9246839284896851}, {'name': 'casual dress', 'confidence': 0.9164291620254517}, {'name': 'button', 'confidence': 0.900519847869873}, {'name': 'shoulder', 'confidence': 0.884363055229187}, {'name': 'neck', 'confidence': 0.8572096824645996}, {'name': 'fabric', 'confidence': 0.8372199535369873}, {'name': 'wall', 'confidence': 0.8361340761184692}, {'name': 'indoor', 'confidence': 0.6911754608154297}, {'name': 'shirt', 'confidence': 0.5440642237663269}, {'name': 'man', 'confidence': 0.44562777876853943}, {'name': 'fashion', 'confidence': 0.4214524030685425}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 101, 'y': 154, 'w': 264, 'h': 408}, 'tags': [{'name': 'shirts', 'confidence': 0.692}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 100, 'y': 20, 'w': 283, 'h': 774}, 'confidence': 0.9540712237358093}, {'boundingBox': {'x': 119, 'y': 298, 'w': 239, 'h': 206}, 'confidence': 0.002497105859220028}, {'boundingBox': {'x': 101, 'y': 40, 'w': 90, 'h': 199}, 'confidence': 0.0011336725438013673}]}}\n",
      "Caption: a man in a white shirt (Confidence: 0.79832923412323)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9988824725151062)\n",
      "- person (Confidence: 0.9910719394683838)\n",
      "- dress shirt (Confidence: 0.9599442481994629)\n",
      "- collar (Confidence: 0.9541717767715454)\n",
      "- sleeve (Confidence: 0.9477725028991699)\n",
      "- top (Confidence: 0.9443472623825073)\n",
      "- standing (Confidence: 0.9328851699829102)\n",
      "- trousers (Confidence: 0.9319429397583008)\n",
      "- pocket (Confidence: 0.9246839284896851)\n",
      "- casual dress (Confidence: 0.9164291620254517)\n",
      "- button (Confidence: 0.900519847869873)\n",
      "- shoulder (Confidence: 0.884363055229187)\n",
      "- neck (Confidence: 0.8572096824645996)\n",
      "- fabric (Confidence: 0.8372199535369873)\n",
      "- wall (Confidence: 0.8361340761184692)\n",
      "- indoor (Confidence: 0.6911754608154297)\n",
      "- shirt (Confidence: 0.5440642237663269)\n",
      "- man (Confidence: 0.44562777876853943)\n",
      "- fashion (Confidence: 0.4214524030685425)\n",
      "Dense Captions:\n",
      "- a man in a white shirt (Confidence: 0.7981970310211182)\n",
      "- a man wearing a white shirt (Confidence: 0.7824667692184448)\n",
      "- a person wearing black pants (Confidence: 0.7746246457099915)\n",
      "- a man with curly hair (Confidence: 0.7465900182723999)\n",
      "- a close up of a man's face (Confidence: 0.8185763359069824)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 6.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in a long sleeved shirt', 'confidence': 0.7495476603507996}, 'denseCaptionsResult': {'values': [{'text': 'a man in a long sleeved shirt', 'confidence': 0.7495476603507996, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': \"a close-up of a man's shirt\", 'confidence': 0.856276273727417, 'boundingBox': {'x': 97, 'y': 16, 'w': 225, 'h': 772}}, {'text': 'a man with curly hair', 'confidence': 0.7681804895401001, 'boundingBox': {'x': 143, 'y': 22, 'w': 117, 'h': 172}}, {'text': \"a close-up of a person's pants\", 'confidence': 0.796844482421875, 'boundingBox': {'x': 114, 'y': 527, 'w': 175, 'h': 266}}, {'text': \"a person's hair on their head\", 'confidence': 0.676020085811615, 'boundingBox': {'x': 148, 'y': 19, 'w': 101, 'h': 88}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9998400211334229}, {'name': 'person', 'confidence': 0.9859426021575928}, {'name': 'sleeve', 'confidence': 0.9623622894287109}, {'name': 'top', 'confidence': 0.9604546427726746}, {'name': 'fabric', 'confidence': 0.956572413444519}, {'name': 'casual dress', 'confidence': 0.9426096677780151}, {'name': 'pocket', 'confidence': 0.9411505460739136}, {'name': 'collar', 'confidence': 0.9410613775253296}, {'name': 'standing', 'confidence': 0.9208518266677856}, {'name': 'button', 'confidence': 0.9177579879760742}, {'name': 'shoulder', 'confidence': 0.90352463722229}, {'name': 'dress shirt', 'confidence': 0.8809553384780884}, {'name': 'trousers', 'confidence': 0.8694489002227783}, {'name': 'neck', 'confidence': 0.8580911159515381}, {'name': 'waist', 'confidence': 0.856735110282898}, {'name': 'outerwear', 'confidence': 0.8489096164703369}, {'name': 'wall', 'confidence': 0.795156717300415}, {'name': 'indoor', 'confidence': 0.6748990416526794}, {'name': 'shirt', 'confidence': 0.5308834910392761}, {'name': 'fashion', 'confidence': 0.47968435287475586}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 98, 'y': 19, 'w': 234, 'h': 779}, 'confidence': 0.9350330829620361}, {'boundingBox': {'x': 119, 'y': 302, 'w': 203, 'h': 199}, 'confidence': 0.0018260214710608125}]}}\n",
      "Caption: a man in a long sleeved shirt (Confidence: 0.7495476603507996)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9998400211334229)\n",
      "- person (Confidence: 0.9859426021575928)\n",
      "- sleeve (Confidence: 0.9623622894287109)\n",
      "- top (Confidence: 0.9604546427726746)\n",
      "- fabric (Confidence: 0.956572413444519)\n",
      "- casual dress (Confidence: 0.9426096677780151)\n",
      "- pocket (Confidence: 0.9411505460739136)\n",
      "- collar (Confidence: 0.9410613775253296)\n",
      "- standing (Confidence: 0.9208518266677856)\n",
      "- button (Confidence: 0.9177579879760742)\n",
      "- shoulder (Confidence: 0.90352463722229)\n",
      "- dress shirt (Confidence: 0.8809553384780884)\n",
      "- trousers (Confidence: 0.8694489002227783)\n",
      "- neck (Confidence: 0.8580911159515381)\n",
      "- waist (Confidence: 0.856735110282898)\n",
      "- outerwear (Confidence: 0.8489096164703369)\n",
      "- wall (Confidence: 0.795156717300415)\n",
      "- indoor (Confidence: 0.6748990416526794)\n",
      "- shirt (Confidence: 0.5308834910392761)\n",
      "- fashion (Confidence: 0.47968435287475586)\n",
      "Dense Captions:\n",
      "- a man in a long sleeved shirt (Confidence: 0.7495476603507996)\n",
      "- a close-up of a man's shirt (Confidence: 0.856276273727417)\n",
      "- a man with curly hair (Confidence: 0.7681804895401001)\n",
      "- a close-up of a person's pants (Confidence: 0.796844482421875)\n",
      "- a person's hair on their head (Confidence: 0.676020085811615)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 7.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man wearing a light blue shirt', 'confidence': 0.802401065826416}, 'denseCaptionsResult': {'values': [{'text': 'a man wearing a light blue shirt', 'confidence': 0.802401065826416, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man wearing a light blue shirt', 'confidence': 0.7941995859146118, 'boundingBox': {'x': 69, 'y': 0, 'w': 262, 'h': 787}}, {'text': \"a close-up of a man's pants\", 'confidence': 0.8594912886619568, 'boundingBox': {'x': 113, 'y': 536, 'w': 192, 'h': 255}}, {'text': 'a close up of a hand', 'confidence': 0.8527768850326538, 'boundingBox': {'x': 89, 'y': 691, 'w': 62, 'h': 104}}, {'text': \"a close-up of a man's face\", 'confidence': 0.9204775094985962, 'boundingBox': {'x': 174, 'y': 6, 'w': 117, 'h': 205}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9994406700134277}, {'name': 'person', 'confidence': 0.9868506193161011}, {'name': 'collar', 'confidence': 0.9741789102554321}, {'name': 'dress shirt', 'confidence': 0.9602698087692261}, {'name': 'sleeve', 'confidence': 0.9439027309417725}, {'name': 'pocket', 'confidence': 0.9375259876251221}, {'name': 'man', 'confidence': 0.9327850937843323}, {'name': 'fabric', 'confidence': 0.932498037815094}, {'name': 'top', 'confidence': 0.9243634343147278}, {'name': 'button', 'confidence': 0.9215173721313477}, {'name': 'casual dress', 'confidence': 0.8953659534454346}, {'name': 'blue', 'confidence': 0.7925927639007568}, {'name': 'shirt', 'confidence': 0.6038019061088562}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 78, 'y': 181, 'w': 261, 'h': 441}, 'tags': [{'name': 'shirts', 'confidence': 0.556}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 75, 'y': 9, 'w': 266, 'h': 789}, 'confidence': 0.9457802772521973}, {'boundingBox': {'x': 96, 'y': 299, 'w': 234, 'h': 202}, 'confidence': 0.0025512874126434326}]}}\n",
      "Caption: a man wearing a light blue shirt (Confidence: 0.802401065826416)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9994406700134277)\n",
      "- person (Confidence: 0.9868506193161011)\n",
      "- collar (Confidence: 0.9741789102554321)\n",
      "- dress shirt (Confidence: 0.9602698087692261)\n",
      "- sleeve (Confidence: 0.9439027309417725)\n",
      "- pocket (Confidence: 0.9375259876251221)\n",
      "- man (Confidence: 0.9327850937843323)\n",
      "- fabric (Confidence: 0.932498037815094)\n",
      "- top (Confidence: 0.9243634343147278)\n",
      "- button (Confidence: 0.9215173721313477)\n",
      "- casual dress (Confidence: 0.8953659534454346)\n",
      "- blue (Confidence: 0.7925927639007568)\n",
      "- shirt (Confidence: 0.6038019061088562)\n",
      "Dense Captions:\n",
      "- a man wearing a light blue shirt (Confidence: 0.802401065826416)\n",
      "- a man wearing a light blue shirt (Confidence: 0.7941995859146118)\n",
      "- a close-up of a man's pants (Confidence: 0.8594912886619568)\n",
      "- a close up of a hand (Confidence: 0.8527768850326538)\n",
      "- a close-up of a man's face (Confidence: 0.9204775094985962)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 8.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man standing with his hands in his pockets', 'confidence': 0.817496120929718}, 'denseCaptionsResult': {'values': [{'text': 'a man standing with his hands in his pockets', 'confidence': 0.817496120929718, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man wearing a blue shirt and grey pants', 'confidence': 0.7188915014266968, 'boundingBox': {'x': 110, 'y': 28, 'w': 186, 'h': 754}}, {'text': 'a man wearing a black shirt', 'confidence': 0.7725108861923218, 'boundingBox': {'x': 106, 'y': 126, 'w': 186, 'h': 248}}, {'text': 'a pair of black shoes', 'confidence': 0.8193057179450989, 'boundingBox': {'x': 162, 'y': 716, 'w': 136, 'h': 76}}, {'text': 'a person wearing white pants', 'confidence': 0.7462526559829712, 'boundingBox': {'x': 131, 'y': 341, 'w': 141, 'h': 398}}, {'text': 'a man with curly hair', 'confidence': 0.7736547589302063, 'boundingBox': {'x': 150, 'y': 16, 'w': 89, 'h': 112}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9997522830963135}, {'name': 'person', 'confidence': 0.9899864196777344}, {'name': 'trousers', 'confidence': 0.9701294898986816}, {'name': 'pocket', 'confidence': 0.9679052829742432}, {'name': 'sleeve', 'confidence': 0.9517226219177246}, {'name': 'collar', 'confidence': 0.9437153935432434}, {'name': 'shirt', 'confidence': 0.9422609806060791}, {'name': 'casual dress', 'confidence': 0.9421325325965881}, {'name': 'top', 'confidence': 0.9393692016601562}, {'name': 'button', 'confidence': 0.9316558837890625}, {'name': 'fabric', 'confidence': 0.9273817539215088}, {'name': 'outerwear', 'confidence': 0.877811074256897}, {'name': 'waist', 'confidence': 0.876234233379364}, {'name': 'dress shirt', 'confidence': 0.8740911483764648}, {'name': 'shoulder', 'confidence': 0.8435614109039307}, {'name': 'trouser', 'confidence': 0.7215795516967773}, {'name': 'standing', 'confidence': 0.6536542177200317}, {'name': 'indoor', 'confidence': 0.5305435061454773}, {'name': 'fashion', 'confidence': 0.5156587362289429}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 113, 'y': 38, 'w': 187, 'h': 469}, 'tags': [{'name': 'person', 'confidence': 0.693}]}, {'boundingBox': {'x': 130, 'y': 341, 'w': 156, 'h': 446}, 'tags': [{'name': 'Jeans', 'confidence': 0.565}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 108, 'y': 15, 'w': 194, 'h': 781}, 'confidence': 0.9508066177368164}, {'boundingBox': {'x': 115, 'y': 301, 'w': 167, 'h': 196}, 'confidence': 0.0023148292675614357}]}}\n",
      "Caption: a man standing with his hands in his pockets (Confidence: 0.817496120929718)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9997522830963135)\n",
      "- person (Confidence: 0.9899864196777344)\n",
      "- trousers (Confidence: 0.9701294898986816)\n",
      "- pocket (Confidence: 0.9679052829742432)\n",
      "- sleeve (Confidence: 0.9517226219177246)\n",
      "- collar (Confidence: 0.9437153935432434)\n",
      "- shirt (Confidence: 0.9422609806060791)\n",
      "- casual dress (Confidence: 0.9421325325965881)\n",
      "- top (Confidence: 0.9393692016601562)\n",
      "- button (Confidence: 0.9316558837890625)\n",
      "- fabric (Confidence: 0.9273817539215088)\n",
      "- outerwear (Confidence: 0.877811074256897)\n",
      "- waist (Confidence: 0.876234233379364)\n",
      "- dress shirt (Confidence: 0.8740911483764648)\n",
      "- shoulder (Confidence: 0.8435614109039307)\n",
      "- trouser (Confidence: 0.7215795516967773)\n",
      "- standing (Confidence: 0.6536542177200317)\n",
      "- indoor (Confidence: 0.5305435061454773)\n",
      "- fashion (Confidence: 0.5156587362289429)\n",
      "Dense Captions:\n",
      "- a man standing with his hands in his pockets (Confidence: 0.817496120929718)\n",
      "- a man wearing a blue shirt and grey pants (Confidence: 0.7188915014266968)\n",
      "- a man wearing a black shirt (Confidence: 0.7725108861923218)\n",
      "- a pair of black shoes (Confidence: 0.8193057179450989)\n",
      "- a person wearing white pants (Confidence: 0.7462526559829712)\n",
      "- a man with curly hair (Confidence: 0.7736547589302063)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\\M 9.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man standing in a white shirt and grey pants', 'confidence': 0.7450444102287292}, 'denseCaptionsResult': {'values': [{'text': 'a man standing in a white shirt and grey pants', 'confidence': 0.7450444102287292, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man wearing grey pants', 'confidence': 0.7203697562217712, 'boundingBox': {'x': 135, 'y': 52, 'w': 155, 'h': 728}}, {'text': 'a man in a white shirt', 'confidence': 0.7906837463378906, 'boundingBox': {'x': 162, 'y': 158, 'w': 131, 'h': 183}}, {'text': 'a person wearing grey pants', 'confidence': 0.7331613898277283, 'boundingBox': {'x': 152, 'y': 323, 'w': 117, 'h': 427}}, {'text': 'a white wall with a white background', 'confidence': 0.584192156791687, 'boundingBox': {'x': 54, 'y': 151, 'w': 112, 'h': 586}}, {'text': \"a person's feet wearing black shoes\", 'confidence': 0.7701877355575562, 'boundingBox': {'x': 141, 'y': 730, 'w': 115, 'h': 57}}, {'text': \"a close up of a person's feet\", 'confidence': 0.7984752058982849, 'boundingBox': {'x': 0, 'y': 730, 'w': 393, 'h': 63}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9961327910423279}, {'name': 'person', 'confidence': 0.9896672964096069}, {'name': 'trousers', 'confidence': 0.9675225019454956}, {'name': 'shirt', 'confidence': 0.9660091996192932}, {'name': 'wall', 'confidence': 0.9453268051147461}, {'name': 'shoulder', 'confidence': 0.9373266696929932}, {'name': 'pocket', 'confidence': 0.9336303472518921}, {'name': 'casual dress', 'confidence': 0.9225932359695435}, {'name': 'top', 'confidence': 0.9158780574798584}, {'name': 'sleeve', 'confidence': 0.8974661231040955}, {'name': 'dress shirt', 'confidence': 0.895902693271637}, {'name': 'waist', 'confidence': 0.8908859491348267}, {'name': 'collar', 'confidence': 0.8888688683509827}, {'name': 'joint', 'confidence': 0.8453596234321594}, {'name': 'footwear', 'confidence': 0.8450616598129272}, {'name': 'button', 'confidence': 0.843661367893219}, {'name': 'denim', 'confidence': 0.8414623737335205}, {'name': 'fashion', 'confidence': 0.7681881189346313}, {'name': 'trouser', 'confidence': 0.7671860456466675}, {'name': 'indoor', 'confidence': 0.7397366166114807}, {'name': 'fabric', 'confidence': 0.6527320742607117}, {'name': 'standing', 'confidence': 0.6320809721946716}, {'name': 'jean', 'confidence': 0.4593870937824249}, {'name': 'man', 'confidence': 0.4025653898715973}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 159, 'y': 65, 'w': 141, 'h': 493}, 'tags': [{'name': 'person', 'confidence': 0.822}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 139, 'y': 55, 'w': 163, 'h': 738}, 'confidence': 0.9112542271614075}, {'boundingBox': {'x': 154, 'y': 332, 'w': 145, 'h': 212}, 'confidence': 0.0011546731693670154}]}}\n",
      "Caption: a man standing in a white shirt and grey pants (Confidence: 0.7450444102287292)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9961327910423279)\n",
      "- person (Confidence: 0.9896672964096069)\n",
      "- trousers (Confidence: 0.9675225019454956)\n",
      "- shirt (Confidence: 0.9660091996192932)\n",
      "- wall (Confidence: 0.9453268051147461)\n",
      "- shoulder (Confidence: 0.9373266696929932)\n",
      "- pocket (Confidence: 0.9336303472518921)\n",
      "- casual dress (Confidence: 0.9225932359695435)\n",
      "- top (Confidence: 0.9158780574798584)\n",
      "- sleeve (Confidence: 0.8974661231040955)\n",
      "- dress shirt (Confidence: 0.895902693271637)\n",
      "- waist (Confidence: 0.8908859491348267)\n",
      "- collar (Confidence: 0.8888688683509827)\n",
      "- joint (Confidence: 0.8453596234321594)\n",
      "- footwear (Confidence: 0.8450616598129272)\n",
      "- button (Confidence: 0.843661367893219)\n",
      "- denim (Confidence: 0.8414623737335205)\n",
      "- fashion (Confidence: 0.7681881189346313)\n",
      "- trouser (Confidence: 0.7671860456466675)\n",
      "- indoor (Confidence: 0.7397366166114807)\n",
      "- fabric (Confidence: 0.6527320742607117)\n",
      "- standing (Confidence: 0.6320809721946716)\n",
      "- jean (Confidence: 0.4593870937824249)\n",
      "- man (Confidence: 0.4025653898715973)\n",
      "Dense Captions:\n",
      "- a man standing in a white shirt and grey pants (Confidence: 0.7450444102287292)\n",
      "- a man wearing grey pants (Confidence: 0.7203697562217712)\n",
      "- a man in a white shirt (Confidence: 0.7906837463378906)\n",
      "- a person wearing grey pants (Confidence: 0.7331613898277283)\n",
      "- a white wall with a white background (Confidence: 0.584192156791687)\n",
      "- a person's feet wearing black shoes (Confidence: 0.7701877355575562)\n",
      "- a close up of a person's feet (Confidence: 0.7984752058982849)\n"
     ]
    }
   ],
   "source": [
    "analyze_images_in_directory(r\"D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\men\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "de4511bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\women\\F 1.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a black dress and tan trench coat', 'confidence': 0.7984259128570557}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a black dress and tan trench coat', 'confidence': 0.7984259128570557, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a black dress and a trench coat', 'confidence': 0.7748208045959473, 'boundingBox': {'x': 107, 'y': 21, 'w': 182, 'h': 756}}, {'text': 'a close up of a black object', 'confidence': 0.6621670126914978, 'boundingBox': {'x': 236, 'y': 432, 'w': 59, 'h': 192}}, {'text': 'a woman with long hair', 'confidence': 0.7169262170791626, 'boundingBox': {'x': 156, 'y': 33, 'w': 120, 'h': 158}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9987072944641113}, {'name': 'fashion', 'confidence': 0.9867891073226929}, {'name': 'person', 'confidence': 0.976088285446167}, {'name': 'fashion design', 'confidence': 0.9718385934829712}, {'name': 'fashion model', 'confidence': 0.9636294841766357}, {'name': 'footwear', 'confidence': 0.9250125885009766}, {'name': 'fashion show', 'confidence': 0.9154960513114929}, {'name': 'coat', 'confidence': 0.9043012857437134}, {'name': 'outerwear', 'confidence': 0.8962482213973999}, {'name': 'high heels', 'confidence': 0.8928102850914001}, {'name': 'overcoat', 'confidence': 0.8869194388389587}, {'name': 'trench coat', 'confidence': 0.8789054155349731}, {'name': 'shoulder', 'confidence': 0.8684109449386597}, {'name': 'street fashion', 'confidence': 0.858099102973938}, {'name': 'duster', 'confidence': 0.8536352515220642}, {'name': 'casual dress', 'confidence': 0.8510898351669312}, {'name': 'fashion accessory', 'confidence': 0.8424944877624512}, {'name': 'indoor', 'confidence': 0.7973933219909668}, {'name': 'woman', 'confidence': 0.7413569688796997}, {'name': 'fabric', 'confidence': 0.6033467650413513}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 115, 'y': 35, 'w': 179, 'h': 758}, 'confidence': 0.8619827628135681}, {'boundingBox': {'x': 42, 'y': 209, 'w': 39, 'h': 79}, 'confidence': 0.12631462514400482}, {'boundingBox': {'x': 121, 'y': 300, 'w': 153, 'h': 202}, 'confidence': 0.0020625386387109756}, {'boundingBox': {'x': 36, 'y': 207, 'w': 52, 'h': 198}, 'confidence': 0.0018035309622064233}, {'boundingBox': {'x': 329, 'y': 255, 'w': 43, 'h': 123}, 'confidence': 0.001364989671856165}]}}\n",
      "Caption: a woman in a black dress and tan trench coat (Confidence: 0.7984259128570557)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9987072944641113)\n",
      "- fashion (Confidence: 0.9867891073226929)\n",
      "- person (Confidence: 0.976088285446167)\n",
      "- fashion design (Confidence: 0.9718385934829712)\n",
      "- fashion model (Confidence: 0.9636294841766357)\n",
      "- footwear (Confidence: 0.9250125885009766)\n",
      "- fashion show (Confidence: 0.9154960513114929)\n",
      "- coat (Confidence: 0.9043012857437134)\n",
      "- outerwear (Confidence: 0.8962482213973999)\n",
      "- high heels (Confidence: 0.8928102850914001)\n",
      "- overcoat (Confidence: 0.8869194388389587)\n",
      "- trench coat (Confidence: 0.8789054155349731)\n",
      "- shoulder (Confidence: 0.8684109449386597)\n",
      "- street fashion (Confidence: 0.858099102973938)\n",
      "- duster (Confidence: 0.8536352515220642)\n",
      "- casual dress (Confidence: 0.8510898351669312)\n",
      "- fashion accessory (Confidence: 0.8424944877624512)\n",
      "- indoor (Confidence: 0.7973933219909668)\n",
      "- woman (Confidence: 0.7413569688796997)\n",
      "- fabric (Confidence: 0.6033467650413513)\n",
      "Dense Captions:\n",
      "- a woman in a black dress and tan trench coat (Confidence: 0.7984259128570557)\n",
      "- a woman wearing a black dress and a trench coat (Confidence: 0.7748208045959473)\n",
      "- a close up of a black object (Confidence: 0.6621670126914978)\n",
      "- a woman with long hair (Confidence: 0.7169262170791626)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\women\\F 10.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a pink suit', 'confidence': 0.851831316947937}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a pink suit', 'confidence': 0.8514387011528015, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman in a pink suit', 'confidence': 0.814492404460907, 'boundingBox': {'x': 122, 'y': 164, 'w': 151, 'h': 519}}, {'text': 'a woman wearing a pink skirt', 'confidence': 0.7633144855499268, 'boundingBox': {'x': 131, 'y': 374, 'w': 123, 'h': 256}}, {'text': 'a woman in a pink jacket', 'confidence': 0.7713363766670227, 'boundingBox': {'x': 124, 'y': 161, 'w': 113, 'h': 199}}, {'text': 'a woman in a pink suit', 'confidence': 0.8407079577445984, 'boundingBox': {'x': 17, 'y': 110, 'w': 337, 'h': 621}}, {'text': 'a woman wearing a pink suit', 'confidence': 0.8273497819900513, 'boundingBox': {'x': 115, 'y': 256, 'w': 154, 'h': 211}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9921492338180542}, {'name': 'person', 'confidence': 0.9780027866363525}, {'name': 'human face', 'confidence': 0.9359349608421326}, {'name': 'fashion', 'confidence': 0.9355292320251465}, {'name': 'fashion design', 'confidence': 0.9149783849716187}, {'name': 'day dress', 'confidence': 0.8970587849617004}, {'name': 'shoulder', 'confidence': 0.88905268907547}, {'name': 'fashion model', 'confidence': 0.8766179084777832}, {'name': 'waist', 'confidence': 0.8752461075782776}, {'name': 'casual dress', 'confidence': 0.8431316614151001}, {'name': 'woman', 'confidence': 0.7116224765777588}, {'name': 'dress', 'confidence': 0.6140433549880981}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 120, 'y': 165, 'w': 151, 'h': 488}, 'tags': [{'name': 'person', 'confidence': 0.814}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 116, 'y': 164, 'w': 160, 'h': 522}, 'confidence': 0.9341841340065002}]}}\n",
      "Caption: a woman in a pink suit (Confidence: 0.851831316947937)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9921492338180542)\n",
      "- person (Confidence: 0.9780027866363525)\n",
      "- human face (Confidence: 0.9359349608421326)\n",
      "- fashion (Confidence: 0.9355292320251465)\n",
      "- fashion design (Confidence: 0.9149783849716187)\n",
      "- day dress (Confidence: 0.8970587849617004)\n",
      "- shoulder (Confidence: 0.88905268907547)\n",
      "- fashion model (Confidence: 0.8766179084777832)\n",
      "- waist (Confidence: 0.8752461075782776)\n",
      "- casual dress (Confidence: 0.8431316614151001)\n",
      "- woman (Confidence: 0.7116224765777588)\n",
      "- dress (Confidence: 0.6140433549880981)\n",
      "Dense Captions:\n",
      "- a woman in a pink suit (Confidence: 0.8514387011528015)\n",
      "- a woman in a pink suit (Confidence: 0.814492404460907)\n",
      "- a woman wearing a pink skirt (Confidence: 0.7633144855499268)\n",
      "- a woman in a pink jacket (Confidence: 0.7713363766670227)\n",
      "- a woman in a pink suit (Confidence: 0.8407079577445984)\n",
      "- a woman wearing a pink suit (Confidence: 0.8273497819900513)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\women\\F 11.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman wearing a white shirt and black skirt', 'confidence': 0.7984779477119446}, 'denseCaptionsResult': {'values': [{'text': 'a woman wearing a white shirt and black skirt', 'confidence': 0.7984779477119446, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a close up of a bag', 'confidence': 0.7624508142471313, 'boundingBox': {'x': 113, 'y': 482, 'w': 82, 'h': 197}}, {'text': 'a woman wearing a black skirt', 'confidence': 0.7739684581756592, 'boundingBox': {'x': 134, 'y': 45, 'w': 169, 'h': 740}}, {'text': \"a close-up of a person's foot\", 'confidence': 0.8522458076477051, 'boundingBox': {'x': 271, 'y': 143, 'w': 120, 'h': 329}}, {'text': 'a woman wearing a black skirt', 'confidence': 0.7670865654945374, 'boundingBox': {'x': 161, 'y': 323, 'w': 118, 'h': 286}}, {'text': 'a woman with long brown hair', 'confidence': 0.7718188762664795, 'boundingBox': {'x': 165, 'y': 47, 'w': 113, 'h': 171}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'person', 'confidence': 0.9853836297988892}, {'name': 'fashion accessory', 'confidence': 0.9772806167602539}, {'name': 'clothing', 'confidence': 0.9772164821624756}, {'name': 'fashion', 'confidence': 0.9725580215454102}, {'name': 'handbag', 'confidence': 0.9716434478759766}, {'name': 'waist', 'confidence': 0.9615475535392761}, {'name': 'shoulder', 'confidence': 0.9510176181793213}, {'name': 'fashion design', 'confidence': 0.9458508491516113}, {'name': 'high heels', 'confidence': 0.9356070160865784}, {'name': 'skirt', 'confidence': 0.9331506490707397}, {'name': 'fashion model', 'confidence': 0.9280413389205933}, {'name': 'casual dress', 'confidence': 0.9267762303352356}, {'name': 'street fashion', 'confidence': 0.9233736991882324}, {'name': 'miniskirt', 'confidence': 0.9045434594154358}, {'name': 'day dress', 'confidence': 0.9040029048919678}, {'name': 'blouse', 'confidence': 0.8949810862541199}, {'name': 'luggage and bags', 'confidence': 0.8758443593978882}, {'name': 'pocket', 'confidence': 0.8714227676391602}, {'name': 'sandal', 'confidence': 0.8662333488464355}, {'name': 'cocktail dress', 'confidence': 0.8597584962844849}, {'name': 'woman', 'confidence': 0.8515580892562866}, {'name': 'pattern (fashion design)', 'confidence': 0.8475919365882874}, {'name': 'sleeve', 'confidence': 0.843571126461029}, {'name': 'standing', 'confidence': 0.8412718772888184}, {'name': 'outdoor', 'confidence': 0.6565588116645813}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 115, 'y': 462, 'w': 90, 'h': 223}, 'tags': [{'name': 'handbag', 'confidence': 0.649}]}, {'boundingBox': {'x': 139, 'y': 74, 'w': 173, 'h': 478}, 'tags': [{'name': 'person', 'confidence': 0.628}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 142, 'y': 52, 'w': 170, 'h': 747}, 'confidence': 0.9214467406272888}, {'boundingBox': {'x': 151, 'y': 330, 'w': 156, 'h': 211}, 'confidence': 0.001870850333943963}]}}\n",
      "Caption: a woman wearing a white shirt and black skirt (Confidence: 0.7984779477119446)\n",
      "Tags:\n",
      "- person (Confidence: 0.9853836297988892)\n",
      "- fashion accessory (Confidence: 0.9772806167602539)\n",
      "- clothing (Confidence: 0.9772164821624756)\n",
      "- fashion (Confidence: 0.9725580215454102)\n",
      "- handbag (Confidence: 0.9716434478759766)\n",
      "- waist (Confidence: 0.9615475535392761)\n",
      "- shoulder (Confidence: 0.9510176181793213)\n",
      "- fashion design (Confidence: 0.9458508491516113)\n",
      "- high heels (Confidence: 0.9356070160865784)\n",
      "- skirt (Confidence: 0.9331506490707397)\n",
      "- fashion model (Confidence: 0.9280413389205933)\n",
      "- casual dress (Confidence: 0.9267762303352356)\n",
      "- street fashion (Confidence: 0.9233736991882324)\n",
      "- miniskirt (Confidence: 0.9045434594154358)\n",
      "- day dress (Confidence: 0.9040029048919678)\n",
      "- blouse (Confidence: 0.8949810862541199)\n",
      "- luggage and bags (Confidence: 0.8758443593978882)\n",
      "- pocket (Confidence: 0.8714227676391602)\n",
      "- sandal (Confidence: 0.8662333488464355)\n",
      "- cocktail dress (Confidence: 0.8597584962844849)\n",
      "- woman (Confidence: 0.8515580892562866)\n",
      "- pattern (fashion design) (Confidence: 0.8475919365882874)\n",
      "- sleeve (Confidence: 0.843571126461029)\n",
      "- standing (Confidence: 0.8412718772888184)\n",
      "- outdoor (Confidence: 0.6565588116645813)\n",
      "Dense Captions:\n",
      "- a woman wearing a white shirt and black skirt (Confidence: 0.7984779477119446)\n",
      "- a close up of a bag (Confidence: 0.7624508142471313)\n",
      "- a woman wearing a black skirt (Confidence: 0.7739684581756592)\n",
      "- a close-up of a person's foot (Confidence: 0.8522458076477051)\n",
      "- a woman wearing a black skirt (Confidence: 0.7670865654945374)\n",
      "- a woman with long brown hair (Confidence: 0.7718188762664795)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\women\\F 2.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a suit and skirt', 'confidence': 0.7338113784790039}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a suit and skirt', 'confidence': 0.7338471412658691, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a black dress and tan jacket', 'confidence': 0.7346651554107666, 'boundingBox': {'x': 99, 'y': 6, 'w': 183, 'h': 780}}, {'text': 'a blurry picture of a plant', 'confidence': 0.7630556225776672, 'boundingBox': {'x': 0, 'y': 235, 'w': 61, 'h': 319}}, {'text': 'a blurry picture of a tree', 'confidence': 0.7631300091743469, 'boundingBox': {'x': 337, 'y': 308, 'w': 59, 'h': 149}}, {'text': 'a woman with blonde hair', 'confidence': 0.7819589376449585, 'boundingBox': {'x': 142, 'y': 9, 'w': 113, 'h': 149}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9978297352790833}, {'name': 'person', 'confidence': 0.9925375580787659}, {'name': 'fashion', 'confidence': 0.9771836400032043}, {'name': 'fashion design', 'confidence': 0.9608058929443359}, {'name': 'collar', 'confidence': 0.9422716498374939}, {'name': 'outerwear', 'confidence': 0.9400631785392761}, {'name': 'shoulder', 'confidence': 0.9391581416130066}, {'name': 'fashion model', 'confidence': 0.9285507202148438}, {'name': 'waist', 'confidence': 0.9276937246322632}, {'name': 'blazer', 'confidence': 0.9248325228691101}, {'name': 'coat', 'confidence': 0.9245637655258179}, {'name': 'high heels', 'confidence': 0.9001359939575195}, {'name': 'footwear', 'confidence': 0.8974721431732178}, {'name': 'casual dress', 'confidence': 0.8898029923439026}, {'name': 'sleeve', 'confidence': 0.8862099647521973}, {'name': 'day dress', 'confidence': 0.8820467591285706}, {'name': 'pocket', 'confidence': 0.8780580759048462}, {'name': 'indoor', 'confidence': 0.8713602423667908}, {'name': 'handbag', 'confidence': 0.8676807284355164}, {'name': 'trench coat', 'confidence': 0.8560391664505005}, {'name': 'fashion accessory', 'confidence': 0.852587103843689}, {'name': 'blouse', 'confidence': 0.8464829921722412}, {'name': 'skirt', 'confidence': 0.8448488712310791}, {'name': 'top', 'confidence': 0.8443198204040527}, {'name': 'cocktail dress', 'confidence': 0.8423999547958374}, {'name': 'woman', 'confidence': 0.7987270355224609}, {'name': 'wall', 'confidence': 0.7475354671478271}, {'name': 'floor', 'confidence': 0.7182116508483887}, {'name': 'wearing', 'confidence': 0.6923815608024597}, {'name': 'standing', 'confidence': 0.6578595042228699}, {'name': 'ground', 'confidence': 0.5571207404136658}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 104, 'y': 6, 'w': 186, 'h': 793}, 'confidence': 0.931286096572876}, {'boundingBox': {'x': 112, 'y': 301, 'w': 164, 'h': 195}, 'confidence': 0.001621755538508296}]}}\n",
      "Caption: a woman in a suit and skirt (Confidence: 0.7338113784790039)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9978297352790833)\n",
      "- person (Confidence: 0.9925375580787659)\n",
      "- fashion (Confidence: 0.9771836400032043)\n",
      "- fashion design (Confidence: 0.9608058929443359)\n",
      "- collar (Confidence: 0.9422716498374939)\n",
      "- outerwear (Confidence: 0.9400631785392761)\n",
      "- shoulder (Confidence: 0.9391581416130066)\n",
      "- fashion model (Confidence: 0.9285507202148438)\n",
      "- waist (Confidence: 0.9276937246322632)\n",
      "- blazer (Confidence: 0.9248325228691101)\n",
      "- coat (Confidence: 0.9245637655258179)\n",
      "- high heels (Confidence: 0.9001359939575195)\n",
      "- footwear (Confidence: 0.8974721431732178)\n",
      "- casual dress (Confidence: 0.8898029923439026)\n",
      "- sleeve (Confidence: 0.8862099647521973)\n",
      "- day dress (Confidence: 0.8820467591285706)\n",
      "- pocket (Confidence: 0.8780580759048462)\n",
      "- indoor (Confidence: 0.8713602423667908)\n",
      "- handbag (Confidence: 0.8676807284355164)\n",
      "- trench coat (Confidence: 0.8560391664505005)\n",
      "- fashion accessory (Confidence: 0.852587103843689)\n",
      "- blouse (Confidence: 0.8464829921722412)\n",
      "- skirt (Confidence: 0.8448488712310791)\n",
      "- top (Confidence: 0.8443198204040527)\n",
      "- cocktail dress (Confidence: 0.8423999547958374)\n",
      "- woman (Confidence: 0.7987270355224609)\n",
      "- wall (Confidence: 0.7475354671478271)\n",
      "- floor (Confidence: 0.7182116508483887)\n",
      "- wearing (Confidence: 0.6923815608024597)\n",
      "- standing (Confidence: 0.6578595042228699)\n",
      "- ground (Confidence: 0.5571207404136658)\n",
      "Dense Captions:\n",
      "- a woman in a suit and skirt (Confidence: 0.7338471412658691)\n",
      "- a woman wearing a black dress and tan jacket (Confidence: 0.7346651554107666)\n",
      "- a blurry picture of a plant (Confidence: 0.7630556225776672)\n",
      "- a blurry picture of a tree (Confidence: 0.7631300091743469)\n",
      "- a woman with blonde hair (Confidence: 0.7819589376449585)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\women\\F 3.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a suit and skirt', 'confidence': 0.708289384841919}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a suit and skirt', 'confidence': 0.708289384841919, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a skirt and a vest', 'confidence': 0.7180163264274597, 'boundingBox': {'x': 142, 'y': 99, 'w': 120, 'h': 606}}, {'text': 'a woman with long hair', 'confidence': 0.7175929546356201, 'boundingBox': {'x': 144, 'y': 84, 'w': 85, 'h': 172}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9957177639007568}, {'name': 'person', 'confidence': 0.9800673127174377}, {'name': 'fashion', 'confidence': 0.9492290616035461}, {'name': 'fashion design', 'confidence': 0.9434827566146851}, {'name': 'coat', 'confidence': 0.9353421330451965}, {'name': 'blazer', 'confidence': 0.9100309610366821}, {'name': 'fashion model', 'confidence': 0.9095709323883057}, {'name': 'outerwear', 'confidence': 0.8849916458129883}, {'name': 'collar', 'confidence': 0.8771324157714844}, {'name': 'day dress', 'confidence': 0.871573805809021}, {'name': 'casual dress', 'confidence': 0.8686463832855225}, {'name': 'dress', 'confidence': 0.8683798313140869}, {'name': 'pattern (fashion design)', 'confidence': 0.865405797958374}, {'name': 'button', 'confidence': 0.8533780574798584}, {'name': 'woman', 'confidence': 0.6967390179634094}, {'name': 'standing', 'confidence': 0.6546507477760315}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 156, 'y': 358, 'w': 95, 'h': 161}, 'tags': [{'name': 'Miniskirt', 'confidence': 0.505}]}, {'boundingBox': {'x': 137, 'y': 96, 'w': 135, 'h': 448}, 'tags': [{'name': 'person', 'confidence': 0.569}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 139, 'y': 86, 'w': 130, 'h': 633}, 'confidence': 0.9284332394599915}]}}\n",
      "Caption: a woman in a suit and skirt (Confidence: 0.708289384841919)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9957177639007568)\n",
      "- person (Confidence: 0.9800673127174377)\n",
      "- fashion (Confidence: 0.9492290616035461)\n",
      "- fashion design (Confidence: 0.9434827566146851)\n",
      "- coat (Confidence: 0.9353421330451965)\n",
      "- blazer (Confidence: 0.9100309610366821)\n",
      "- fashion model (Confidence: 0.9095709323883057)\n",
      "- outerwear (Confidence: 0.8849916458129883)\n",
      "- collar (Confidence: 0.8771324157714844)\n",
      "- day dress (Confidence: 0.871573805809021)\n",
      "- casual dress (Confidence: 0.8686463832855225)\n",
      "- dress (Confidence: 0.8683798313140869)\n",
      "- pattern (fashion design) (Confidence: 0.865405797958374)\n",
      "- button (Confidence: 0.8533780574798584)\n",
      "- woman (Confidence: 0.6967390179634094)\n",
      "- standing (Confidence: 0.6546507477760315)\n",
      "Dense Captions:\n",
      "- a woman in a suit and skirt (Confidence: 0.708289384841919)\n",
      "- a woman wearing a skirt and a vest (Confidence: 0.7180163264274597)\n",
      "- a woman with long hair (Confidence: 0.7175929546356201)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\women\\F 4.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a skirt and blouse', 'confidence': 0.7034857869148254}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a skirt and blouse', 'confidence': 0.7034857869148254, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a black skirt', 'confidence': 0.8199406266212463, 'boundingBox': {'x': 105, 'y': 68, 'w': 187, 'h': 719}}, {'text': \"a close up of a woman's skirt\", 'confidence': 0.8041300773620605, 'boundingBox': {'x': 132, 'y': 337, 'w': 132, 'h': 304}}, {'text': 'a woman smiling with her hair blowing in the wind', 'confidence': 0.772121012210846, 'boundingBox': {'x': 111, 'y': 66, 'w': 140, 'h': 139}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9936479330062866}, {'name': 'person', 'confidence': 0.9903221130371094}, {'name': 'fashion', 'confidence': 0.9715772271156311}, {'name': 'waist', 'confidence': 0.9683555364608765}, {'name': 'shoulder', 'confidence': 0.9670990705490112}, {'name': 'street fashion', 'confidence': 0.9651206731796265}, {'name': 'fashion model', 'confidence': 0.9378894567489624}, {'name': 'casual dress', 'confidence': 0.9372140169143677}, {'name': 'outdoor', 'confidence': 0.9252172708511353}, {'name': 'lady', 'confidence': 0.9249401092529297}, {'name': 'high heels', 'confidence': 0.9158104658126831}, {'name': 'fashion design', 'confidence': 0.9150017499923706}, {'name': 'blouse', 'confidence': 0.9002287983894348}, {'name': 'building', 'confidence': 0.898080587387085}, {'name': 'female person', 'confidence': 0.8961195945739746}, {'name': 'woman', 'confidence': 0.895744800567627}, {'name': 'fashion accessory', 'confidence': 0.885344922542572}, {'name': 'skirt', 'confidence': 0.8756769299507141}, {'name': 'day dress', 'confidence': 0.8627912998199463}, {'name': 'sleeve', 'confidence': 0.8556791543960571}, {'name': 'photo shoot', 'confidence': 0.8525943756103516}, {'name': 'model', 'confidence': 0.8428838849067688}, {'name': 'ground', 'confidence': 0.6431778073310852}, {'name': 'street', 'confidence': 0.5492902398109436}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 124, 'y': 353, 'w': 162, 'h': 279}, 'tags': [{'name': 'Miniskirt', 'confidence': 0.73}]}, {'boundingBox': {'x': 109, 'y': 83, 'w': 196, 'h': 463}, 'tags': [{'name': 'person', 'confidence': 0.528}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 111, 'y': 73, 'w': 188, 'h': 726}, 'confidence': 0.9468265771865845}, {'boundingBox': {'x': 0, 'y': 336, 'w': 22, 'h': 162}, 'confidence': 0.007766443304717541}, {'boundingBox': {'x': 117, 'y': 331, 'w': 165, 'h': 214}, 'confidence': 0.0030558102298527956}]}}\n",
      "Caption: a woman in a skirt and blouse (Confidence: 0.7034857869148254)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9936479330062866)\n",
      "- person (Confidence: 0.9903221130371094)\n",
      "- fashion (Confidence: 0.9715772271156311)\n",
      "- waist (Confidence: 0.9683555364608765)\n",
      "- shoulder (Confidence: 0.9670990705490112)\n",
      "- street fashion (Confidence: 0.9651206731796265)\n",
      "- fashion model (Confidence: 0.9378894567489624)\n",
      "- casual dress (Confidence: 0.9372140169143677)\n",
      "- outdoor (Confidence: 0.9252172708511353)\n",
      "- lady (Confidence: 0.9249401092529297)\n",
      "- high heels (Confidence: 0.9158104658126831)\n",
      "- fashion design (Confidence: 0.9150017499923706)\n",
      "- blouse (Confidence: 0.9002287983894348)\n",
      "- building (Confidence: 0.898080587387085)\n",
      "- female person (Confidence: 0.8961195945739746)\n",
      "- woman (Confidence: 0.895744800567627)\n",
      "- fashion accessory (Confidence: 0.885344922542572)\n",
      "- skirt (Confidence: 0.8756769299507141)\n",
      "- day dress (Confidence: 0.8627912998199463)\n",
      "- sleeve (Confidence: 0.8556791543960571)\n",
      "- photo shoot (Confidence: 0.8525943756103516)\n",
      "- model (Confidence: 0.8428838849067688)\n",
      "- ground (Confidence: 0.6431778073310852)\n",
      "- street (Confidence: 0.5492902398109436)\n",
      "Dense Captions:\n",
      "- a woman in a skirt and blouse (Confidence: 0.7034857869148254)\n",
      "- a woman wearing a black skirt (Confidence: 0.8199406266212463)\n",
      "- a close up of a woman's skirt (Confidence: 0.8041300773620605)\n",
      "- a woman smiling with her hair blowing in the wind (Confidence: 0.772121012210846)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\women\\F 5.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a pink suit', 'confidence': 0.7960219383239746}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a pink suit', 'confidence': 0.7960219383239746, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a pink skirt', 'confidence': 0.7349811792373657, 'boundingBox': {'x': 115, 'y': 19, 'w': 170, 'h': 766}}, {'text': 'a woman with long hair', 'confidence': 0.6577296257019043, 'boundingBox': {'x': 174, 'y': 20, 'w': 76, 'h': 111}}, {'text': \"a close up of a person's feet\", 'confidence': 0.7982906699180603, 'boundingBox': {'x': 169, 'y': 738, 'w': 91, 'h': 58}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.99587082862854}, {'name': 'person', 'confidence': 0.9944287538528442}, {'name': 'fashion design', 'confidence': 0.9289777278900146}, {'name': 'fashion', 'confidence': 0.9261701107025146}, {'name': 'shoulder', 'confidence': 0.9104946851730347}, {'name': 'casual dress', 'confidence': 0.8990424871444702}, {'name': 'day dress', 'confidence': 0.8989772796630859}, {'name': 'waist', 'confidence': 0.8984031081199646}, {'name': 'collar', 'confidence': 0.8864355683326721}, {'name': 'standing', 'confidence': 0.8857401609420776}, {'name': 'pattern (fashion design)', 'confidence': 0.8769408464431763}, {'name': 'blazer', 'confidence': 0.8715298175811768}, {'name': 'blouse', 'confidence': 0.8648285865783691}, {'name': 'sleeve', 'confidence': 0.8639148473739624}, {'name': 'fashion model', 'confidence': 0.8608281016349792}, {'name': 'skirt', 'confidence': 0.8485586643218994}, {'name': 'sheath dress', 'confidence': 0.8476399183273315}, {'name': 'cocktail dress', 'confidence': 0.8400048017501831}, {'name': 'woman', 'confidence': 0.7902947068214417}, {'name': 'wearing', 'confidence': 0.7073326706886292}, {'name': 'dress', 'confidence': 0.6546826958656311}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 116, 'y': 21, 'w': 180, 'h': 775}, 'confidence': 0.9280736446380615}, {'boundingBox': {'x': 122, 'y': 300, 'w': 156, 'h': 198}, 'confidence': 0.0019110467983409762}]}}\n",
      "Caption: a woman in a pink suit (Confidence: 0.7960219383239746)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.99587082862854)\n",
      "- person (Confidence: 0.9944287538528442)\n",
      "- fashion design (Confidence: 0.9289777278900146)\n",
      "- fashion (Confidence: 0.9261701107025146)\n",
      "- shoulder (Confidence: 0.9104946851730347)\n",
      "- casual dress (Confidence: 0.8990424871444702)\n",
      "- day dress (Confidence: 0.8989772796630859)\n",
      "- waist (Confidence: 0.8984031081199646)\n",
      "- collar (Confidence: 0.8864355683326721)\n",
      "- standing (Confidence: 0.8857401609420776)\n",
      "- pattern (fashion design) (Confidence: 0.8769408464431763)\n",
      "- blazer (Confidence: 0.8715298175811768)\n",
      "- blouse (Confidence: 0.8648285865783691)\n",
      "- sleeve (Confidence: 0.8639148473739624)\n",
      "- fashion model (Confidence: 0.8608281016349792)\n",
      "- skirt (Confidence: 0.8485586643218994)\n",
      "- sheath dress (Confidence: 0.8476399183273315)\n",
      "- cocktail dress (Confidence: 0.8400048017501831)\n",
      "- woman (Confidence: 0.7902947068214417)\n",
      "- wearing (Confidence: 0.7073326706886292)\n",
      "- dress (Confidence: 0.6546826958656311)\n",
      "Dense Captions:\n",
      "- a woman in a pink suit (Confidence: 0.7960219383239746)\n",
      "- a woman wearing a pink skirt (Confidence: 0.7349811792373657)\n",
      "- a woman with long hair (Confidence: 0.6577296257019043)\n",
      "- a close up of a person's feet (Confidence: 0.7982906699180603)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\women\\F 6.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a black dress and grey blazer', 'confidence': 0.7886706590652466}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a black dress and grey blazer', 'confidence': 0.7886706590652466, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman in a black dress and grey blazer', 'confidence': 0.8239507079124451, 'boundingBox': {'x': 97, 'y': 109, 'w': 212, 'h': 668}}, {'text': 'a person wearing a suit', 'confidence': 0.691598653793335, 'boundingBox': {'x': 94, 'y': 459, 'w': 53, 'h': 250}}, {'text': 'a person looking at a mirror', 'confidence': 0.6972936391830444, 'boundingBox': {'x': 273, 'y': 194, 'w': 121, 'h': 172}}, {'text': 'a woman in a black dress', 'confidence': 0.83590167760849, 'boundingBox': {'x': 105, 'y': 212, 'w': 198, 'h': 277}}, {'text': 'a close-up of a woman smiling', 'confidence': 0.8699545860290527, 'boundingBox': {'x': 149, 'y': 116, 'w': 91, 'h': 147}}, {'text': 'a pair of legs wearing black shoes', 'confidence': 0.7136432528495789, 'boundingBox': {'x': 163, 'y': 667, 'w': 120, 'h': 125}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'person', 'confidence': 0.9901076555252075}, {'name': 'clothing', 'confidence': 0.9810616970062256}, {'name': 'footwear', 'confidence': 0.9682590961456299}, {'name': 'indoor', 'confidence': 0.9631772041320801}, {'name': 'wall', 'confidence': 0.9586803913116455}, {'name': 'fashion design', 'confidence': 0.9347202181816101}, {'name': 'fashion', 'confidence': 0.9333932399749756}, {'name': 'coat', 'confidence': 0.9268296957015991}, {'name': 'high heels', 'confidence': 0.9254823923110962}, {'name': 'mirror', 'confidence': 0.9086154699325562}, {'name': 'woman', 'confidence': 0.9076292514801025}, {'name': 'fashion accessory', 'confidence': 0.8935544490814209}, {'name': 'fashion model', 'confidence': 0.8911568522453308}, {'name': 'handbag', 'confidence': 0.8843884468078613}, {'name': 'sandal', 'confidence': 0.8451749086380005}, {'name': 'floor', 'confidence': 0.790454089641571}, {'name': 'standing', 'confidence': 0.6919139623641968}, {'name': 'ground', 'confidence': 0.6208716034889221}, {'name': 'wearing', 'confidence': 0.5971503853797913}, {'name': 'hosiery', 'confidence': 0.5210577845573425}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 104, 'y': 116, 'w': 207, 'h': 677}, 'confidence': 0.9533897638320923}, {'boundingBox': {'x': 384, 'y': 333, 'w': 15, 'h': 26}, 'confidence': 0.011775105260312557}, {'boundingBox': {'x': 111, 'y': 344, 'w': 185, 'h': 203}, 'confidence': 0.0036136056296527386}]}}\n",
      "Caption: a woman in a black dress and grey blazer (Confidence: 0.7886706590652466)\n",
      "Tags:\n",
      "- person (Confidence: 0.9901076555252075)\n",
      "- clothing (Confidence: 0.9810616970062256)\n",
      "- footwear (Confidence: 0.9682590961456299)\n",
      "- indoor (Confidence: 0.9631772041320801)\n",
      "- wall (Confidence: 0.9586803913116455)\n",
      "- fashion design (Confidence: 0.9347202181816101)\n",
      "- fashion (Confidence: 0.9333932399749756)\n",
      "- coat (Confidence: 0.9268296957015991)\n",
      "- high heels (Confidence: 0.9254823923110962)\n",
      "- mirror (Confidence: 0.9086154699325562)\n",
      "- woman (Confidence: 0.9076292514801025)\n",
      "- fashion accessory (Confidence: 0.8935544490814209)\n",
      "- fashion model (Confidence: 0.8911568522453308)\n",
      "- handbag (Confidence: 0.8843884468078613)\n",
      "- sandal (Confidence: 0.8451749086380005)\n",
      "- floor (Confidence: 0.790454089641571)\n",
      "- standing (Confidence: 0.6919139623641968)\n",
      "- ground (Confidence: 0.6208716034889221)\n",
      "- wearing (Confidence: 0.5971503853797913)\n",
      "- hosiery (Confidence: 0.5210577845573425)\n",
      "Dense Captions:\n",
      "- a woman in a black dress and grey blazer (Confidence: 0.7886706590652466)\n",
      "- a woman in a black dress and grey blazer (Confidence: 0.8239507079124451)\n",
      "- a person wearing a suit (Confidence: 0.691598653793335)\n",
      "- a person looking at a mirror (Confidence: 0.6972936391830444)\n",
      "- a woman in a black dress (Confidence: 0.83590167760849)\n",
      "- a close-up of a woman smiling (Confidence: 0.8699545860290527)\n",
      "- a pair of legs wearing black shoes (Confidence: 0.7136432528495789)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\women\\F 7.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a blue blazer and black dress', 'confidence': 0.791405200958252}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a blue blazer and black dress', 'confidence': 0.7918416261672974, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a blue dress', 'confidence': 0.7947093844413757, 'boundingBox': {'x': 125, 'y': 120, 'w': 141, 'h': 652}}, {'text': 'a close-up of a rug', 'confidence': 0.8663970232009888, 'boundingBox': {'x': 266, 'y': 603, 'w': 128, 'h': 190}}, {'text': 'a person wearing a blue jacket', 'confidence': 0.7696890830993652, 'boundingBox': {'x': 132, 'y': 212, 'w': 130, 'h': 331}}, {'text': 'a woman smiling for the camera', 'confidence': 0.726590096950531, 'boundingBox': {'x': 155, 'y': 121, 'w': 86, 'h': 142}}, {'text': 'a white wall with a shadow', 'confidence': 0.6459268927574158, 'boundingBox': {'x': 5, 'y': 0, 'w': 91, 'h': 646}}, {'text': \"a woman's legs in high heels\", 'confidence': 0.7339574098587036, 'boundingBox': {'x': 0, 'y': 539, 'w': 387, 'h': 249}}, {'text': 'a close up of a pair of heels', 'confidence': 0.7291746139526367, 'boundingBox': {'x': 175, 'y': 650, 'w': 81, 'h': 143}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'person', 'confidence': 0.9942096471786499}, {'name': 'wall', 'confidence': 0.9889839887619019}, {'name': 'indoor', 'confidence': 0.9884048104286194}, {'name': 'clothing', 'confidence': 0.9775516986846924}, {'name': 'shoulder', 'confidence': 0.9248461723327637}, {'name': 'fashion', 'confidence': 0.9161190986633301}, {'name': 'pattern (fashion design)', 'confidence': 0.9069229364395142}, {'name': 'floor', 'confidence': 0.9020605087280273}, {'name': 'footwear', 'confidence': 0.896198570728302}, {'name': 'fashion design', 'confidence': 0.883110761642456}, {'name': 'smile', 'confidence': 0.8712321519851685}, {'name': 'day dress', 'confidence': 0.8616887331008911}, {'name': 'waist', 'confidence': 0.8613712787628174}, {'name': 'outerwear', 'confidence': 0.860763430595398}, {'name': 'human face', 'confidence': 0.859147310256958}, {'name': 'sleeve', 'confidence': 0.8435180187225342}, {'name': 'woman', 'confidence': 0.8220993280410767}, {'name': 'standing', 'confidence': 0.7237094640731812}, {'name': 'dress', 'confidence': 0.6245272755622864}, {'name': 'outfit', 'confidence': 0.4060181975364685}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 122, 'y': 129, 'w': 148, 'h': 460}, 'tags': [{'name': 'person', 'confidence': 0.703}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 127, 'y': 123, 'w': 146, 'h': 661}, 'confidence': 0.9382160305976868}, {'boundingBox': {'x': 132, 'y': 343, 'w': 133, 'h': 202}, 'confidence': 0.0012802467681467533}]}}\n",
      "Caption: a woman in a blue blazer and black dress (Confidence: 0.791405200958252)\n",
      "Tags:\n",
      "- person (Confidence: 0.9942096471786499)\n",
      "- wall (Confidence: 0.9889839887619019)\n",
      "- indoor (Confidence: 0.9884048104286194)\n",
      "- clothing (Confidence: 0.9775516986846924)\n",
      "- shoulder (Confidence: 0.9248461723327637)\n",
      "- fashion (Confidence: 0.9161190986633301)\n",
      "- pattern (fashion design) (Confidence: 0.9069229364395142)\n",
      "- floor (Confidence: 0.9020605087280273)\n",
      "- footwear (Confidence: 0.896198570728302)\n",
      "- fashion design (Confidence: 0.883110761642456)\n",
      "- smile (Confidence: 0.8712321519851685)\n",
      "- day dress (Confidence: 0.8616887331008911)\n",
      "- waist (Confidence: 0.8613712787628174)\n",
      "- outerwear (Confidence: 0.860763430595398)\n",
      "- human face (Confidence: 0.859147310256958)\n",
      "- sleeve (Confidence: 0.8435180187225342)\n",
      "- woman (Confidence: 0.8220993280410767)\n",
      "- standing (Confidence: 0.7237094640731812)\n",
      "- dress (Confidence: 0.6245272755622864)\n",
      "- outfit (Confidence: 0.4060181975364685)\n",
      "Dense Captions:\n",
      "- a woman in a blue blazer and black dress (Confidence: 0.7918416261672974)\n",
      "- a woman wearing a blue dress (Confidence: 0.7947093844413757)\n",
      "- a close-up of a rug (Confidence: 0.8663970232009888)\n",
      "- a person wearing a blue jacket (Confidence: 0.7696890830993652)\n",
      "- a woman smiling for the camera (Confidence: 0.726590096950531)\n",
      "- a white wall with a shadow (Confidence: 0.6459268927574158)\n",
      "- a woman's legs in high heels (Confidence: 0.7339574098587036)\n",
      "- a close up of a pair of heels (Confidence: 0.7291746139526367)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\women\\F 8.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman standing in front of a dresser', 'confidence': 0.8605771660804749}, 'denseCaptionsResult': {'values': [{'text': 'a woman standing in front of a dresser', 'confidence': 0.8605771660804749, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman in a black dress', 'confidence': 0.8366380929946899, 'boundingBox': {'x': 94, 'y': 90, 'w': 252, 'h': 699}}, {'text': 'a hand on a chair', 'confidence': 0.7409756779670715, 'boundingBox': {'x': 302, 'y': 408, 'w': 95, 'h': 181}}, {'text': 'a black frame with a drawing on it', 'confidence': 0.6826942563056946, 'boundingBox': {'x': 310, 'y': 181, 'w': 57, 'h': 108}}, {'text': \"a blurry image of a person's face\", 'confidence': 0.6825976371765137, 'boundingBox': {'x': 0, 'y': 0, 'w': 62, 'h': 322}}, {'text': 'a close up of a dresser', 'confidence': 0.7868629097938538, 'boundingBox': {'x': 0, 'y': 417, 'w': 210, 'h': 365}}, {'text': 'a lamp on a wall', 'confidence': 0.7273011803627014, 'boundingBox': {'x': 64, 'y': 239, 'w': 59, 'h': 189}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'person', 'confidence': 0.9971078634262085}, {'name': 'clothing', 'confidence': 0.9918461441993713}, {'name': 'wall', 'confidence': 0.9854812622070312}, {'name': 'indoor', 'confidence': 0.9830890893936157}, {'name': 'furniture', 'confidence': 0.9590304493904114}, {'name': 'drawer', 'confidence': 0.958135724067688}, {'name': 'chest of drawers', 'confidence': 0.9452892541885376}, {'name': 'woman', 'confidence': 0.9336910843849182}, {'name': 'shoulder', 'confidence': 0.9010082483291626}, {'name': 'cabinetry', 'confidence': 0.8911614418029785}, {'name': 'footwear', 'confidence': 0.886915922164917}, {'name': 'fashion accessory', 'confidence': 0.8855165243148804}, {'name': 'chest', 'confidence': 0.8820140361785889}, {'name': 'high heels', 'confidence': 0.8749518394470215}, {'name': 'fashion', 'confidence': 0.8745789527893066}, {'name': 'waist', 'confidence': 0.8521347045898438}, {'name': 'fashion design', 'confidence': 0.8516714572906494}, {'name': 'standing', 'confidence': 0.8103039860725403}, {'name': 'floor', 'confidence': 0.7409943342208862}, {'name': 'dress', 'confidence': 0.6496313810348511}, {'name': 'wearing', 'confidence': 0.5366871356964111}, {'name': 'wooden', 'confidence': 0.5351431965827942}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 124, 'y': 111, 'w': 215, 'h': 498}, 'tags': [{'name': 'person', 'confidence': 0.808}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 97, 'y': 94, 'w': 263, 'h': 705}, 'confidence': 0.9478018283843994}, {'boundingBox': {'x': 124, 'y': 368, 'w': 213, 'h': 199}, 'confidence': 0.003296842100098729}, {'boundingBox': {'x': 320, 'y': 205, 'w': 41, 'h': 73}, 'confidence': 0.0012549527455121279}]}}\n",
      "Caption: a woman standing in front of a dresser (Confidence: 0.8605771660804749)\n",
      "Tags:\n",
      "- person (Confidence: 0.9971078634262085)\n",
      "- clothing (Confidence: 0.9918461441993713)\n",
      "- wall (Confidence: 0.9854812622070312)\n",
      "- indoor (Confidence: 0.9830890893936157)\n",
      "- furniture (Confidence: 0.9590304493904114)\n",
      "- drawer (Confidence: 0.958135724067688)\n",
      "- chest of drawers (Confidence: 0.9452892541885376)\n",
      "- woman (Confidence: 0.9336910843849182)\n",
      "- shoulder (Confidence: 0.9010082483291626)\n",
      "- cabinetry (Confidence: 0.8911614418029785)\n",
      "- footwear (Confidence: 0.886915922164917)\n",
      "- fashion accessory (Confidence: 0.8855165243148804)\n",
      "- chest (Confidence: 0.8820140361785889)\n",
      "- high heels (Confidence: 0.8749518394470215)\n",
      "- fashion (Confidence: 0.8745789527893066)\n",
      "- waist (Confidence: 0.8521347045898438)\n",
      "- fashion design (Confidence: 0.8516714572906494)\n",
      "- standing (Confidence: 0.8103039860725403)\n",
      "- floor (Confidence: 0.7409943342208862)\n",
      "- dress (Confidence: 0.6496313810348511)\n",
      "- wearing (Confidence: 0.5366871356964111)\n",
      "- wooden (Confidence: 0.5351431965827942)\n",
      "Dense Captions:\n",
      "- a woman standing in front of a dresser (Confidence: 0.8605771660804749)\n",
      "- a woman in a black dress (Confidence: 0.8366380929946899)\n",
      "- a hand on a chair (Confidence: 0.7409756779670715)\n",
      "- a black frame with a drawing on it (Confidence: 0.6826942563056946)\n",
      "- a blurry image of a person's face (Confidence: 0.6825976371765137)\n",
      "- a close up of a dresser (Confidence: 0.7868629097938538)\n",
      "- a lamp on a wall (Confidence: 0.7273011803627014)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\women\\F 9.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a woman in a brown jacket and black skirt', 'confidence': 0.7816101312637329}, 'denseCaptionsResult': {'values': [{'text': 'a woman in a brown jacket and black skirt', 'confidence': 0.7816101312637329, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a woman wearing a black skirt', 'confidence': 0.8111545443534851, 'boundingBox': {'x': 103, 'y': 88, 'w': 182, 'h': 672}}, {'text': \"a blurry image of a person's arm\", 'confidence': 0.764485239982605, 'boundingBox': {'x': 79, 'y': 432, 'w': 55, 'h': 222}}, {'text': 'a woman wearing a black skirt', 'confidence': 0.7295424938201904, 'boundingBox': {'x': 129, 'y': 321, 'w': 118, 'h': 211}}, {'text': 'a woman wearing sunglasses smiling', 'confidence': 0.7680703401565552, 'boundingBox': {'x': 165, 'y': 92, 'w': 121, 'h': 184}}, {'text': 'a blurry image of a field of green plants', 'confidence': 0.6710941791534424, 'boundingBox': {'x': 269, 'y': 270, 'w': 127, 'h': 160}}, {'text': \"a black purse on a person's hand\", 'confidence': 0.7432492971420288, 'boundingBox': {'x': 51, 'y': 380, 'w': 103, 'h': 338}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'outdoor', 'confidence': 0.9901933670043945}, {'name': 'person', 'confidence': 0.9858798980712891}, {'name': 'fashion', 'confidence': 0.9683690071105957}, {'name': 'fashion accessory', 'confidence': 0.9656189680099487}, {'name': 'footwear', 'confidence': 0.9519913196563721}, {'name': 'clothing', 'confidence': 0.949988603591919}, {'name': 'street fashion', 'confidence': 0.9480686187744141}, {'name': 'high heels', 'confidence': 0.9367468953132629}, {'name': 'fashion model', 'confidence': 0.9364528656005859}, {'name': 'waist', 'confidence': 0.9321730136871338}, {'name': 'shoulder', 'confidence': 0.9227896928787231}, {'name': 'sandal', 'confidence': 0.9002746343612671}, {'name': 'casual dress', 'confidence': 0.8995646238327026}, {'name': 'day dress', 'confidence': 0.8924291133880615}, {'name': 'fashion design', 'confidence': 0.8795159459114075}, {'name': 'handbag', 'confidence': 0.8647733330726624}, {'name': 'woman', 'confidence': 0.857376217842102}, {'name': 'cocktail dress', 'confidence': 0.8526679277420044}, {'name': 'miniskirt', 'confidence': 0.8415197134017944}, {'name': 'outfit', 'confidence': 0.6914616823196411}, {'name': 'dress', 'confidence': 0.5941608548164368}, {'name': 'girl', 'confidence': 0.5880743265151978}, {'name': 'ground', 'confidence': 0.5754156112670898}, {'name': 'sunglasses', 'confidence': 0.5375186204910278}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 180, 'y': 123, 'w': 49, 'h': 33}, 'tags': [{'name': 'Sunglasses', 'confidence': 0.501}]}, {'boundingBox': {'x': 127, 'y': 337, 'w': 128, 'h': 196}, 'tags': [{'name': 'Miniskirt', 'confidence': 0.708}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 112, 'y': 93, 'w': 181, 'h': 676}, 'confidence': 0.9425415992736816}, {'boundingBox': {'x': 118, 'y': 334, 'w': 164, 'h': 203}, 'confidence': 0.0031426239293068647}]}}\n",
      "Caption: a woman in a brown jacket and black skirt (Confidence: 0.7816101312637329)\n",
      "Tags:\n",
      "- outdoor (Confidence: 0.9901933670043945)\n",
      "- person (Confidence: 0.9858798980712891)\n",
      "- fashion (Confidence: 0.9683690071105957)\n",
      "- fashion accessory (Confidence: 0.9656189680099487)\n",
      "- footwear (Confidence: 0.9519913196563721)\n",
      "- clothing (Confidence: 0.949988603591919)\n",
      "- street fashion (Confidence: 0.9480686187744141)\n",
      "- high heels (Confidence: 0.9367468953132629)\n",
      "- fashion model (Confidence: 0.9364528656005859)\n",
      "- waist (Confidence: 0.9321730136871338)\n",
      "- shoulder (Confidence: 0.9227896928787231)\n",
      "- sandal (Confidence: 0.9002746343612671)\n",
      "- casual dress (Confidence: 0.8995646238327026)\n",
      "- day dress (Confidence: 0.8924291133880615)\n",
      "- fashion design (Confidence: 0.8795159459114075)\n",
      "- handbag (Confidence: 0.8647733330726624)\n",
      "- woman (Confidence: 0.857376217842102)\n",
      "- cocktail dress (Confidence: 0.8526679277420044)\n",
      "- miniskirt (Confidence: 0.8415197134017944)\n",
      "- outfit (Confidence: 0.6914616823196411)\n",
      "- dress (Confidence: 0.5941608548164368)\n",
      "- girl (Confidence: 0.5880743265151978)\n",
      "- ground (Confidence: 0.5754156112670898)\n",
      "- sunglasses (Confidence: 0.5375186204910278)\n",
      "Dense Captions:\n",
      "- a woman in a brown jacket and black skirt (Confidence: 0.7816101312637329)\n",
      "- a woman wearing a black skirt (Confidence: 0.8111545443534851)\n",
      "- a blurry image of a person's arm (Confidence: 0.764485239982605)\n",
      "- a woman wearing a black skirt (Confidence: 0.7295424938201904)\n",
      "- a woman wearing sunglasses smiling (Confidence: 0.7680703401565552)\n",
      "- a blurry image of a field of green plants (Confidence: 0.6710941791534424)\n",
      "- a black purse on a person's hand (Confidence: 0.7432492971420288)\n"
     ]
    }
   ],
   "source": [
    "analyze_images_in_directory(r\"D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\resized_image\\women\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "683ae237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 10.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a grey suit with a tie', 'confidence': 0.669489324092865}, 'denseCaptionsResult': {'values': [{'text': 'a grey suit with a tie', 'confidence': 0.6688581109046936, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a grey suit with a tie', 'confidence': 0.629467248916626, 'boundingBox': {'x': 45, 'y': 24, 'w': 293, 'h': 762}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9997642040252686}, {'name': 'coat', 'confidence': 0.9712584018707275}, {'name': 'blazer', 'confidence': 0.9567273259162903}, {'name': 'collar', 'confidence': 0.9550732374191284}, {'name': 'outerwear', 'confidence': 0.9547118544578552}, {'name': 'button', 'confidence': 0.9463926553726196}, {'name': 'person', 'confidence': 0.9451237916946411}, {'name': 'formal wear', 'confidence': 0.9027454853057861}, {'name': 'pocket', 'confidence': 0.8844742774963379}, {'name': 'suit', 'confidence': 0.7364696264266968}, {'name': 'wearing', 'confidence': 0.6684258580207825}, {'name': 'fabric', 'confidence': 0.5339710712432861}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 51, 'y': 12, 'w': 299, 'h': 787}, 'confidence': 0.8757866621017456}, {'boundingBox': {'x': 69, 'y': 317, 'w': 259, 'h': 216}, 'confidence': 0.0015477539272978902}]}}\n",
      "Caption: a grey suit with a tie (Confidence: 0.669489324092865)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9997642040252686)\n",
      "- coat (Confidence: 0.9712584018707275)\n",
      "- blazer (Confidence: 0.9567273259162903)\n",
      "- collar (Confidence: 0.9550732374191284)\n",
      "- outerwear (Confidence: 0.9547118544578552)\n",
      "- button (Confidence: 0.9463926553726196)\n",
      "- person (Confidence: 0.9451237916946411)\n",
      "- formal wear (Confidence: 0.9027454853057861)\n",
      "- pocket (Confidence: 0.8844742774963379)\n",
      "- suit (Confidence: 0.7364696264266968)\n",
      "- wearing (Confidence: 0.6684258580207825)\n",
      "- fabric (Confidence: 0.5339710712432861)\n",
      "Dense Captions:\n",
      "- a grey suit with a tie (Confidence: 0.6688581109046936)\n",
      "- a grey suit with a tie (Confidence: 0.629467248916626)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 14.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man standing with his hands in his pockets', 'confidence': 0.8382658362388611}, 'denseCaptionsResult': {'values': [{'text': 'a man standing with his hands in his pockets', 'confidence': 0.8382658362388611, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a person with hands in pockets', 'confidence': 0.7822921276092529, 'boundingBox': {'x': 117, 'y': 33, 'w': 153, 'h': 734}}, {'text': 'a person wearing a green jacket', 'confidence': 0.7733785510063171, 'boundingBox': {'x': 110, 'y': 128, 'w': 161, 'h': 311}}, {'text': \"a person's legs in white sneakers\", 'confidence': 0.6947526931762695, 'boundingBox': {'x': 171, 'y': 661, 'w': 71, 'h': 109}}, {'text': \"a person's legs in black pants\", 'confidence': 0.7653020024299622, 'boundingBox': {'x': 135, 'y': 393, 'w': 122, 'h': 297}}, {'text': 'a man smiling with a beard', 'confidence': 0.6266832947731018, 'boundingBox': {'x': 145, 'y': 22, 'w': 68, 'h': 121}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9975486993789673}, {'name': 'person', 'confidence': 0.9903882145881653}, {'name': 'outerwear', 'confidence': 0.954484760761261}, {'name': 'trousers', 'confidence': 0.9353430271148682}, {'name': 'top', 'confidence': 0.9347115159034729}, {'name': 'standing', 'confidence': 0.9347018003463745}, {'name': 'sleeve', 'confidence': 0.9342751502990723}, {'name': 'jacket', 'confidence': 0.9325709939002991}, {'name': 'pocket', 'confidence': 0.9295914769172668}, {'name': 'footwear', 'confidence': 0.9274622201919556}, {'name': 'man', 'confidence': 0.9203983545303345}, {'name': 'collar', 'confidence': 0.9039355516433716}, {'name': 'casual dress', 'confidence': 0.9016308784484863}, {'name': 'fabric', 'confidence': 0.882149338722229}, {'name': 'jeans', 'confidence': 0.8593311309814453}, {'name': 'coat', 'confidence': 0.8528797626495361}, {'name': 'fashion', 'confidence': 0.669136106967926}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 113, 'y': 24, 'w': 164, 'h': 748}, 'confidence': 0.6720331907272339}, {'boundingBox': {'x': 123, 'y': 291, 'w': 154, 'h': 212}, 'confidence': 0.0013774981489405036}]}}\n",
      "Caption: a man standing with his hands in his pockets (Confidence: 0.8382658362388611)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9975486993789673)\n",
      "- person (Confidence: 0.9903882145881653)\n",
      "- outerwear (Confidence: 0.954484760761261)\n",
      "- trousers (Confidence: 0.9353430271148682)\n",
      "- top (Confidence: 0.9347115159034729)\n",
      "- standing (Confidence: 0.9347018003463745)\n",
      "- sleeve (Confidence: 0.9342751502990723)\n",
      "- jacket (Confidence: 0.9325709939002991)\n",
      "- pocket (Confidence: 0.9295914769172668)\n",
      "- footwear (Confidence: 0.9274622201919556)\n",
      "- man (Confidence: 0.9203983545303345)\n",
      "- collar (Confidence: 0.9039355516433716)\n",
      "- casual dress (Confidence: 0.9016308784484863)\n",
      "- fabric (Confidence: 0.882149338722229)\n",
      "- jeans (Confidence: 0.8593311309814453)\n",
      "- coat (Confidence: 0.8528797626495361)\n",
      "- fashion (Confidence: 0.669136106967926)\n",
      "Dense Captions:\n",
      "- a man standing with his hands in his pockets (Confidence: 0.8382658362388611)\n",
      "- a person with hands in pockets (Confidence: 0.7822921276092529)\n",
      "- a person wearing a green jacket (Confidence: 0.7733785510063171)\n",
      "- a person's legs in white sneakers (Confidence: 0.6947526931762695)\n",
      "- a person's legs in black pants (Confidence: 0.7653020024299622)\n",
      "- a man smiling with a beard (Confidence: 0.6266832947731018)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 15.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in a beige jacket', 'confidence': 0.7155494689941406}, 'denseCaptionsResult': {'values': [{'text': 'a man in a beige jacket', 'confidence': 0.7155494689941406, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man in a jacket and jeans', 'confidence': 0.6846636533737183, 'boundingBox': {'x': 107, 'y': 0, 'w': 216, 'h': 776}}, {'text': 'a close up of a pair of feet', 'confidence': 0.7491361498832703, 'boundingBox': {'x': 124, 'y': 717, 'w': 162, 'h': 70}}, {'text': \"a person's legs in light blue jeans\", 'confidence': 0.7200524806976318, 'boundingBox': {'x': 137, 'y': 324, 'w': 154, 'h': 417}}, {'text': 'a man in a jacket', 'confidence': 0.717704176902771, 'boundingBox': {'x': 107, 'y': 95, 'w': 220, 'h': 296}}, {'text': \"a close-up of a man's face\", 'confidence': 0.890577495098114, 'boundingBox': {'x': 167, 'y': 3, 'w': 91, 'h': 104}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'person', 'confidence': 0.994360089302063}, {'name': 'clothing', 'confidence': 0.9923176169395447}, {'name': 'shirt', 'confidence': 0.9757299423217773}, {'name': 'jeans', 'confidence': 0.9736469388008118}, {'name': 'collar', 'confidence': 0.9577500820159912}, {'name': 'trousers', 'confidence': 0.9542645215988159}, {'name': 'standing', 'confidence': 0.9524590969085693}, {'name': 'pocket', 'confidence': 0.9471036195755005}, {'name': 'outerwear', 'confidence': 0.9439026713371277}, {'name': 'casual dress', 'confidence': 0.9337993264198303}, {'name': 'sleeve', 'confidence': 0.9336011409759521}, {'name': 'top', 'confidence': 0.9244399070739746}, {'name': 'denim', 'confidence': 0.9119040369987488}, {'name': 'dress shirt', 'confidence': 0.9082640409469604}, {'name': 'man', 'confidence': 0.8963422775268555}, {'name': 'button', 'confidence': 0.8742570877075195}, {'name': 'footwear', 'confidence': 0.8509981632232666}, {'name': 'fashion', 'confidence': 0.7879011631011963}, {'name': 'wearing', 'confidence': 0.7399848103523254}, {'name': 'indoor', 'confidence': 0.535625159740448}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 105, 'y': 39, 'w': 230, 'h': 455}, 'tags': [{'name': 'person', 'confidence': 0.715}]}, {'boundingBox': {'x': 130, 'y': 334, 'w': 167, 'h': 451}, 'tags': [{'name': 'Jeans', 'confidence': 0.78}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 110, 'y': 2, 'w': 225, 'h': 784}, 'confidence': 0.9487478733062744}, {'boundingBox': {'x': 127, 'y': 290, 'w': 199, 'h': 206}, 'confidence': 0.0020674641709774733}]}}\n",
      "Caption: a man in a beige jacket (Confidence: 0.7155494689941406)\n",
      "Tags:\n",
      "- person (Confidence: 0.994360089302063)\n",
      "- clothing (Confidence: 0.9923176169395447)\n",
      "- shirt (Confidence: 0.9757299423217773)\n",
      "- jeans (Confidence: 0.9736469388008118)\n",
      "- collar (Confidence: 0.9577500820159912)\n",
      "- trousers (Confidence: 0.9542645215988159)\n",
      "- standing (Confidence: 0.9524590969085693)\n",
      "- pocket (Confidence: 0.9471036195755005)\n",
      "- outerwear (Confidence: 0.9439026713371277)\n",
      "- casual dress (Confidence: 0.9337993264198303)\n",
      "- sleeve (Confidence: 0.9336011409759521)\n",
      "- top (Confidence: 0.9244399070739746)\n",
      "- denim (Confidence: 0.9119040369987488)\n",
      "- dress shirt (Confidence: 0.9082640409469604)\n",
      "- man (Confidence: 0.8963422775268555)\n",
      "- button (Confidence: 0.8742570877075195)\n",
      "- footwear (Confidence: 0.8509981632232666)\n",
      "- fashion (Confidence: 0.7879011631011963)\n",
      "- wearing (Confidence: 0.7399848103523254)\n",
      "- indoor (Confidence: 0.535625159740448)\n",
      "Dense Captions:\n",
      "- a man in a beige jacket (Confidence: 0.7155494689941406)\n",
      "- a man in a jacket and jeans (Confidence: 0.6846636533737183)\n",
      "- a close up of a pair of feet (Confidence: 0.7491361498832703)\n",
      "- a person's legs in light blue jeans (Confidence: 0.7200524806976318)\n",
      "- a man in a jacket (Confidence: 0.717704176902771)\n",
      "- a close-up of a man's face (Confidence: 0.890577495098114)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 18.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man with tattoos on his arm', 'confidence': 0.6974959373474121}, 'denseCaptionsResult': {'values': [{'text': 'a man with tattoos on his arm', 'confidence': 0.6976024508476257, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man with tattoos on his arm', 'confidence': 0.7556692361831665, 'boundingBox': {'x': 95, 'y': 76, 'w': 198, 'h': 598}}, {'text': 'a man wearing a grey shirt', 'confidence': 0.7533910870552063, 'boundingBox': {'x': 91, 'y': 150, 'w': 196, 'h': 244}}, {'text': 'a close up of a pair of white shoes', 'confidence': 0.8136254549026489, 'boundingBox': {'x': 167, 'y': 609, 'w': 84, 'h': 76}}, {'text': 'a person wearing tan pants', 'confidence': 0.6922109723091125, 'boundingBox': {'x': 115, 'y': 354, 'w': 130, 'h': 277}}, {'text': 'a blue surface with white lines', 'confidence': 0.6435015797615051, 'boundingBox': {'x': 0, 'y': 712, 'w': 393, 'h': 81}}, {'text': 'a man wearing white sunglasses', 'confidence': 0.816545844078064, 'boundingBox': {'x': 152, 'y': 67, 'w': 79, 'h': 97}}, {'text': \"a person's feet in white shoes\", 'confidence': 0.7975026369094849, 'boundingBox': {'x': 0, 'y': 603, 'w': 396, 'h': 137}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.997430682182312}, {'name': 'person', 'confidence': 0.9962362051010132}, {'name': 'footwear', 'confidence': 0.9939155578613281}, {'name': 'trousers', 'confidence': 0.9572266340255737}, {'name': 'man', 'confidence': 0.9532301425933838}, {'name': 'building', 'confidence': 0.9391978979110718}, {'name': 'sleeve', 'confidence': 0.9171595573425293}, {'name': 'outdoor', 'confidence': 0.8715967535972595}, {'name': 'ground', 'confidence': 0.8640323877334595}, {'name': 'trouser', 'confidence': 0.8133134841918945}, {'name': 'street', 'confidence': 0.673173725605011}, {'name': 'standing', 'confidence': 0.6603250503540039}, {'name': 'wearing', 'confidence': 0.597410261631012}, {'name': 'shirt', 'confidence': 0.5262905955314636}, {'name': 'fashion', 'confidence': 0.48193907737731934}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 94, 'y': 76, 'w': 196, 'h': 480}, 'tags': [{'name': 'person', 'confidence': 0.787}]}]}, 'readResult': {'blocks': [{'lines': [{'text': 'EIGHTY', 'boundingPolygon': [{'x': 210, 'y': 230}, {'x': 240, 'y': 240}, {'x': 236, 'y': 250}, {'x': 207, 'y': 239}], 'words': [{'text': 'EIGHTY', 'boundingPolygon': [{'x': 210, 'y': 231}, {'x': 238, 'y': 241}, {'x': 235, 'y': 249}, {'x': 207, 'y': 240}], 'confidence': 0.948}]}, {'text': 'FIVE', 'boundingPolygon': [{'x': 207, 'y': 261}, {'x': 226, 'y': 267}, {'x': 224, 'y': 276}, {'x': 205, 'y': 270}], 'words': [{'text': 'FIVE', 'boundingPolygon': [{'x': 209, 'y': 262}, {'x': 226, 'y': 267}, {'x': 223, 'y': 275}, {'x': 207, 'y': 270}], 'confidence': 0.505}]}]}]}, 'peopleResult': {'values': [{'boundingBox': {'x': 93, 'y': 68, 'w': 204, 'h': 620}, 'confidence': 0.9509848356246948}, {'boundingBox': {'x': 103, 'y': 293, 'w': 182, 'h': 195}, 'confidence': 0.002701342571526766}]}}\n",
      "Caption: a man with tattoos on his arm (Confidence: 0.6974959373474121)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.997430682182312)\n",
      "- person (Confidence: 0.9962362051010132)\n",
      "- footwear (Confidence: 0.9939155578613281)\n",
      "- trousers (Confidence: 0.9572266340255737)\n",
      "- man (Confidence: 0.9532301425933838)\n",
      "- building (Confidence: 0.9391978979110718)\n",
      "- sleeve (Confidence: 0.9171595573425293)\n",
      "- outdoor (Confidence: 0.8715967535972595)\n",
      "- ground (Confidence: 0.8640323877334595)\n",
      "- trouser (Confidence: 0.8133134841918945)\n",
      "- street (Confidence: 0.673173725605011)\n",
      "- standing (Confidence: 0.6603250503540039)\n",
      "- wearing (Confidence: 0.597410261631012)\n",
      "- shirt (Confidence: 0.5262905955314636)\n",
      "- fashion (Confidence: 0.48193907737731934)\n",
      "Dense Captions:\n",
      "- a man with tattoos on his arm (Confidence: 0.6976024508476257)\n",
      "- a man with tattoos on his arm (Confidence: 0.7556692361831665)\n",
      "- a man wearing a grey shirt (Confidence: 0.7533910870552063)\n",
      "- a close up of a pair of white shoes (Confidence: 0.8136254549026489)\n",
      "- a person wearing tan pants (Confidence: 0.6922109723091125)\n",
      "- a blue surface with white lines (Confidence: 0.6435015797615051)\n",
      "- a man wearing white sunglasses (Confidence: 0.816545844078064)\n",
      "- a person's feet in white shoes (Confidence: 0.7975026369094849)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 2.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in a suit', 'confidence': 0.7981011867523193}, 'denseCaptionsResult': {'values': [{'text': 'a man in a suit', 'confidence': 0.7981011867523193, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man in a suit', 'confidence': 0.7635818719863892, 'boundingBox': {'x': 70, 'y': 26, 'w': 201, 'h': 747}}, {'text': 'a close up of a brown object', 'confidence': 0.7539679408073425, 'boundingBox': {'x': 332, 'y': 593, 'w': 66, 'h': 202}}, {'text': 'a pair of brown boots', 'confidence': 0.7357268929481506, 'boundingBox': {'x': 143, 'y': 703, 'w': 130, 'h': 80}}, {'text': 'a close up of a pair of legs', 'confidence': 0.7440174221992493, 'boundingBox': {'x': 103, 'y': 345, 'w': 157, 'h': 372}}, {'text': 'a man in a suit', 'confidence': 0.7823969125747681, 'boundingBox': {'x': 68, 'y': 129, 'w': 206, 'h': 294}}, {'text': 'a close up of a shoe', 'confidence': 0.7951682806015015, 'boundingBox': {'x': 0, 'y': 712, 'w': 360, 'h': 81}}, {'text': 'a man smiling with a mustache', 'confidence': 0.6950545310974121, 'boundingBox': {'x': 124, 'y': 28, 'w': 78, 'h': 117}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9979716539382935}, {'name': 'person', 'confidence': 0.996508002281189}, {'name': 'wall', 'confidence': 0.9872382879257202}, {'name': 'trousers', 'confidence': 0.9801543951034546}, {'name': 'indoor', 'confidence': 0.9575420618057251}, {'name': 'pocket', 'confidence': 0.9342726469039917}, {'name': 'man', 'confidence': 0.9181774258613586}, {'name': 'dress shirt', 'confidence': 0.904300332069397}, {'name': 'blazer', 'confidence': 0.8965617418289185}, {'name': 'gentleman', 'confidence': 0.8943653106689453}, {'name': 'collar', 'confidence': 0.891432523727417}, {'name': 'shirt', 'confidence': 0.8901636600494385}, {'name': 'top', 'confidence': 0.8898183107376099}, {'name': 'footwear', 'confidence': 0.8882598876953125}, {'name': 'outerwear', 'confidence': 0.8858300447463989}, {'name': 'suit trousers', 'confidence': 0.8784655332565308}, {'name': 'button', 'confidence': 0.8767153024673462}, {'name': 'trouser', 'confidence': 0.8369064331054688}, {'name': 'fashion', 'confidence': 0.7979836463928223}, {'name': 'wearing', 'confidence': 0.7272721529006958}, {'name': 'standing', 'confidence': 0.6915855407714844}, {'name': 'pants', 'confidence': 0.6836409568786621}, {'name': 'fabric', 'confidence': 0.647769570350647}, {'name': 'suit', 'confidence': 0.6436828374862671}, {'name': 'floor', 'confidence': 0.6260820031166077}, {'name': 'menswear', 'confidence': 0.49622517824172974}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 74, 'y': 59, 'w': 209, 'h': 428}, 'tags': [{'name': 'person', 'confidence': 0.692}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 73, 'y': 30, 'w': 208, 'h': 759}, 'confidence': 0.9482870101928711}, {'boundingBox': {'x': 90, 'y': 297, 'w': 183, 'h': 207}, 'confidence': 0.0025259265676140785}]}}\n",
      "Caption: a man in a suit (Confidence: 0.7981011867523193)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9979716539382935)\n",
      "- person (Confidence: 0.996508002281189)\n",
      "- wall (Confidence: 0.9872382879257202)\n",
      "- trousers (Confidence: 0.9801543951034546)\n",
      "- indoor (Confidence: 0.9575420618057251)\n",
      "- pocket (Confidence: 0.9342726469039917)\n",
      "- man (Confidence: 0.9181774258613586)\n",
      "- dress shirt (Confidence: 0.904300332069397)\n",
      "- blazer (Confidence: 0.8965617418289185)\n",
      "- gentleman (Confidence: 0.8943653106689453)\n",
      "- collar (Confidence: 0.891432523727417)\n",
      "- shirt (Confidence: 0.8901636600494385)\n",
      "- top (Confidence: 0.8898183107376099)\n",
      "- footwear (Confidence: 0.8882598876953125)\n",
      "- outerwear (Confidence: 0.8858300447463989)\n",
      "- suit trousers (Confidence: 0.8784655332565308)\n",
      "- button (Confidence: 0.8767153024673462)\n",
      "- trouser (Confidence: 0.8369064331054688)\n",
      "- fashion (Confidence: 0.7979836463928223)\n",
      "- wearing (Confidence: 0.7272721529006958)\n",
      "- standing (Confidence: 0.6915855407714844)\n",
      "- pants (Confidence: 0.6836409568786621)\n",
      "- fabric (Confidence: 0.647769570350647)\n",
      "- suit (Confidence: 0.6436828374862671)\n",
      "- floor (Confidence: 0.6260820031166077)\n",
      "- menswear (Confidence: 0.49622517824172974)\n",
      "Dense Captions:\n",
      "- a man in a suit (Confidence: 0.7981011867523193)\n",
      "- a man in a suit (Confidence: 0.7635818719863892)\n",
      "- a close up of a brown object (Confidence: 0.7539679408073425)\n",
      "- a pair of brown boots (Confidence: 0.7357268929481506)\n",
      "- a close up of a pair of legs (Confidence: 0.7440174221992493)\n",
      "- a man in a suit (Confidence: 0.7823969125747681)\n",
      "- a close up of a shoe (Confidence: 0.7951682806015015)\n",
      "- a man smiling with a mustache (Confidence: 0.6950545310974121)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 20.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man standing in front of a building', 'confidence': 0.7893258929252625}, 'denseCaptionsResult': {'values': [{'text': 'a man standing in front of a building', 'confidence': 0.7893258929252625, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man in a jacket', 'confidence': 0.7097902894020081, 'boundingBox': {'x': 68, 'y': 0, 'w': 245, 'h': 772}}, {'text': 'a man in a jacket', 'confidence': 0.7383793592453003, 'boundingBox': {'x': 82, 'y': 108, 'w': 236, 'h': 292}}, {'text': \"a close-up of a person's foot\", 'confidence': 0.8500773906707764, 'boundingBox': {'x': 73, 'y': 689, 'w': 207, 'h': 90}}, {'text': 'a person wearing khaki pants', 'confidence': 0.766906201839447, 'boundingBox': {'x': 103, 'y': 348, 'w': 179, 'h': 361}}, {'text': 'a man wearing sunglasses', 'confidence': 0.8059613704681396, 'boundingBox': {'x': 154, 'y': 6, 'w': 84, 'h': 119}}, {'text': 'a close up of a shoe', 'confidence': 0.8476327061653137, 'boundingBox': {'x': 75, 'y': 693, 'w': 93, 'h': 81}}, {'text': 'a white tie on a black surface', 'confidence': 0.7625676393508911, 'boundingBox': {'x': 151, 'y': 135, 'w': 76, 'h': 222}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'person', 'confidence': 0.9974430799484253}, {'name': 'clothing', 'confidence': 0.9953157901763916}, {'name': 'trousers', 'confidence': 0.9758003354072571}, {'name': 'man', 'confidence': 0.9718943238258362}, {'name': 'pocket', 'confidence': 0.9489836692810059}, {'name': 'outerwear', 'confidence': 0.9444917440414429}, {'name': 'street fashion', 'confidence': 0.9357362985610962}, {'name': 'collar', 'confidence': 0.9224632978439331}, {'name': 'gentleman', 'confidence': 0.9048012495040894}, {'name': 'casual dress', 'confidence': 0.8972394466400146}, {'name': 'sleeve', 'confidence': 0.8958249092102051}, {'name': 'outdoor', 'confidence': 0.8863974213600159}, {'name': 'shirt', 'confidence': 0.8858609199523926}, {'name': 'ground', 'confidence': 0.8851946592330933}, {'name': 'footwear', 'confidence': 0.8826649188995361}, {'name': 'coat', 'confidence': 0.8729465007781982}, {'name': 'top', 'confidence': 0.8706584572792053}, {'name': 'wearing', 'confidence': 0.8448717594146729}, {'name': 'fashion', 'confidence': 0.8439569473266602}, {'name': 'sunglasses', 'confidence': 0.7923665046691895}, {'name': 'trouser', 'confidence': 0.7506989240646362}, {'name': 'standing', 'confidence': 0.711398720741272}, {'name': 'menswear', 'confidence': 0.6408277153968811}, {'name': 'sidewalk', 'confidence': 0.630599856376648}, {'name': 'glasses', 'confidence': 0.6275168061256409}, {'name': 'fabric', 'confidence': 0.5970538854598999}, {'name': 'street', 'confidence': 0.5870660543441772}, {'name': 'suit', 'confidence': 0.48637154698371887}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 89, 'y': 39, 'w': 235, 'h': 386}, 'tags': [{'name': 'person', 'confidence': 0.611}]}, {'boundingBox': {'x': 95, 'y': 332, 'w': 193, 'h': 408}, 'tags': [{'name': 'Jeans', 'confidence': 0.684}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 76, 'y': 6, 'w': 248, 'h': 779}, 'confidence': 0.9545024037361145}, {'boundingBox': {'x': 93, 'y': 294, 'w': 211, 'h': 204}, 'confidence': 0.0026291003450751305}]}}\n",
      "Caption: a man standing in front of a building (Confidence: 0.7893258929252625)\n",
      "Tags:\n",
      "- person (Confidence: 0.9974430799484253)\n",
      "- clothing (Confidence: 0.9953157901763916)\n",
      "- trousers (Confidence: 0.9758003354072571)\n",
      "- man (Confidence: 0.9718943238258362)\n",
      "- pocket (Confidence: 0.9489836692810059)\n",
      "- outerwear (Confidence: 0.9444917440414429)\n",
      "- street fashion (Confidence: 0.9357362985610962)\n",
      "- collar (Confidence: 0.9224632978439331)\n",
      "- gentleman (Confidence: 0.9048012495040894)\n",
      "- casual dress (Confidence: 0.8972394466400146)\n",
      "- sleeve (Confidence: 0.8958249092102051)\n",
      "- outdoor (Confidence: 0.8863974213600159)\n",
      "- shirt (Confidence: 0.8858609199523926)\n",
      "- ground (Confidence: 0.8851946592330933)\n",
      "- footwear (Confidence: 0.8826649188995361)\n",
      "- coat (Confidence: 0.8729465007781982)\n",
      "- top (Confidence: 0.8706584572792053)\n",
      "- wearing (Confidence: 0.8448717594146729)\n",
      "- fashion (Confidence: 0.8439569473266602)\n",
      "- sunglasses (Confidence: 0.7923665046691895)\n",
      "- trouser (Confidence: 0.7506989240646362)\n",
      "- standing (Confidence: 0.711398720741272)\n",
      "- menswear (Confidence: 0.6408277153968811)\n",
      "- sidewalk (Confidence: 0.630599856376648)\n",
      "- glasses (Confidence: 0.6275168061256409)\n",
      "- fabric (Confidence: 0.5970538854598999)\n",
      "- street (Confidence: 0.5870660543441772)\n",
      "- suit (Confidence: 0.48637154698371887)\n",
      "Dense Captions:\n",
      "- a man standing in front of a building (Confidence: 0.7893258929252625)\n",
      "- a man in a jacket (Confidence: 0.7097902894020081)\n",
      "- a man in a jacket (Confidence: 0.7383793592453003)\n",
      "- a close-up of a person's foot (Confidence: 0.8500773906707764)\n",
      "- a person wearing khaki pants (Confidence: 0.766906201839447)\n",
      "- a man wearing sunglasses (Confidence: 0.8059613704681396)\n",
      "- a close up of a shoe (Confidence: 0.8476327061653137)\n",
      "- a white tie on a black surface (Confidence: 0.7625676393508911)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 21.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man standing with his hand in his pocket', 'confidence': 0.8117260932922363}, 'denseCaptionsResult': {'values': [{'text': 'a man standing with his hands in his pockets', 'confidence': 0.8142633438110352, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man in a brown shirt and white pants', 'confidence': 0.7075698375701904, 'boundingBox': {'x': 48, 'y': 90, 'w': 249, 'h': 672}}, {'text': 'a close up of a shoe', 'confidence': 0.7649908661842346, 'boundingBox': {'x': 52, 'y': 690, 'w': 182, 'h': 80}}, {'text': 'a person wearing white pants', 'confidence': 0.6883822679519653, 'boundingBox': {'x': 102, 'y': 358, 'w': 155, 'h': 365}}, {'text': 'a man wearing a brown shirt', 'confidence': 0.7689677476882935, 'boundingBox': {'x': 154, 'y': 178, 'w': 143, 'h': 209}}, {'text': 'a close up of a shoe', 'confidence': 0.8283780217170715, 'boundingBox': {'x': 53, 'y': 690, 'w': 83, 'h': 63}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'person', 'confidence': 0.9932528138160706}, {'name': 'clothing', 'confidence': 0.984386682510376}, {'name': 'trousers', 'confidence': 0.9586454033851624}, {'name': 'shoulder', 'confidence': 0.9175005555152893}, {'name': 'man', 'confidence': 0.9122477173805237}, {'name': 'joint', 'confidence': 0.9110821485519409}, {'name': 'elbow', 'confidence': 0.9103351831436157}, {'name': 'pocket', 'confidence': 0.90030837059021}, {'name': 'footwear', 'confidence': 0.9001544713973999}, {'name': 'casual dress', 'confidence': 0.8969335556030273}, {'name': 'top', 'confidence': 0.8902777433395386}, {'name': 'sleeve', 'confidence': 0.8661054372787476}, {'name': 'waist', 'confidence': 0.8630763292312622}, {'name': 'trouser', 'confidence': 0.8354625701904297}, {'name': 'wall', 'confidence': 0.825616717338562}, {'name': 'fabric', 'confidence': 0.7884881496429443}, {'name': 'fashion', 'confidence': 0.6591812968254089}, {'name': 'standing', 'confidence': 0.5830546021461487}, {'name': 'indoor', 'confidence': 0.5459743142127991}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 124, 'y': 112, 'w': 182, 'h': 480}, 'tags': [{'name': 'person', 'confidence': 0.524}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 57, 'y': 100, 'w': 249, 'h': 671}, 'confidence': 0.9500350952148438}, {'boundingBox': {'x': 65, 'y': 337, 'w': 220, 'h': 201}, 'confidence': 0.002739398740231991}]}}\n",
      "Caption: a man standing with his hand in his pocket (Confidence: 0.8117260932922363)\n",
      "Tags:\n",
      "- person (Confidence: 0.9932528138160706)\n",
      "- clothing (Confidence: 0.984386682510376)\n",
      "- trousers (Confidence: 0.9586454033851624)\n",
      "- shoulder (Confidence: 0.9175005555152893)\n",
      "- man (Confidence: 0.9122477173805237)\n",
      "- joint (Confidence: 0.9110821485519409)\n",
      "- elbow (Confidence: 0.9103351831436157)\n",
      "- pocket (Confidence: 0.90030837059021)\n",
      "- footwear (Confidence: 0.9001544713973999)\n",
      "- casual dress (Confidence: 0.8969335556030273)\n",
      "- top (Confidence: 0.8902777433395386)\n",
      "- sleeve (Confidence: 0.8661054372787476)\n",
      "- waist (Confidence: 0.8630763292312622)\n",
      "- trouser (Confidence: 0.8354625701904297)\n",
      "- wall (Confidence: 0.825616717338562)\n",
      "- fabric (Confidence: 0.7884881496429443)\n",
      "- fashion (Confidence: 0.6591812968254089)\n",
      "- standing (Confidence: 0.5830546021461487)\n",
      "- indoor (Confidence: 0.5459743142127991)\n",
      "Dense Captions:\n",
      "- a man standing with his hands in his pockets (Confidence: 0.8142633438110352)\n",
      "- a man in a brown shirt and white pants (Confidence: 0.7075698375701904)\n",
      "- a close up of a shoe (Confidence: 0.7649908661842346)\n",
      "- a person wearing white pants (Confidence: 0.6883822679519653)\n",
      "- a man wearing a brown shirt (Confidence: 0.7689677476882935)\n",
      "- a close up of a shoe (Confidence: 0.8283780217170715)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 22.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in white shirt and white pants standing in front of a mirror', 'confidence': 0.7741627097129822}, 'denseCaptionsResult': {'values': [{'text': 'a man in white shirt and white pants standing in front of a mirror', 'confidence': 0.7741110920906067, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man wearing white pants', 'confidence': 0.7883489727973938, 'boundingBox': {'x': 144, 'y': 112, 'w': 183, 'h': 648}}, {'text': 'a wooden leg next to a chair', 'confidence': 0.7015265226364136, 'boundingBox': {'x': 55, 'y': 434, 'w': 130, 'h': 294}}, {'text': 'a white stool with a hole in the middle', 'confidence': 0.600471019744873, 'boundingBox': {'x': 135, 'y': 548, 'w': 55, 'h': 160}}, {'text': \"a person's shadow on the wall\", 'confidence': 0.6498269438743591, 'boundingBox': {'x': 130, 'y': 157, 'w': 90, 'h': 165}}, {'text': 'a white curtain with a brown border', 'confidence': 0.5506270527839661, 'boundingBox': {'x': 0, 'y': 0, 'w': 83, 'h': 714}}, {'text': 'a close up of a pair of shoes', 'confidence': 0.7619830965995789, 'boundingBox': {'x': 153, 'y': 695, 'w': 95, 'h': 68}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'person', 'confidence': 0.9966249465942383}, {'name': 'clothing', 'confidence': 0.9930475354194641}, {'name': 'wall', 'confidence': 0.9871711730957031}, {'name': 'indoor', 'confidence': 0.9819614291191101}, {'name': 'furniture', 'confidence': 0.9647060632705688}, {'name': 'footwear', 'confidence': 0.9158928394317627}, {'name': 'trousers', 'confidence': 0.9073073863983154}, {'name': 'joint', 'confidence': 0.9014332294464111}, {'name': 'man', 'confidence': 0.8996748328208923}, {'name': 'table', 'confidence': 0.8551928997039795}, {'name': 'standing', 'confidence': 0.7025822401046753}, {'name': 'floor', 'confidence': 0.7020223140716553}, {'name': 'pants', 'confidence': 0.5717838406562805}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 154, 'y': 123, 'w': 188, 'h': 463}, 'tags': [{'name': 'person', 'confidence': 0.593}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 155, 'y': 114, 'w': 186, 'h': 651}, 'confidence': 0.9505650401115417}, {'boundingBox': {'x': 178, 'y': 341, 'w': 159, 'h': 198}, 'confidence': 0.0031494845170527697}]}}\n",
      "Caption: a man in white shirt and white pants standing in front of a mirror (Confidence: 0.7741627097129822)\n",
      "Tags:\n",
      "- person (Confidence: 0.9966249465942383)\n",
      "- clothing (Confidence: 0.9930475354194641)\n",
      "- wall (Confidence: 0.9871711730957031)\n",
      "- indoor (Confidence: 0.9819614291191101)\n",
      "- furniture (Confidence: 0.9647060632705688)\n",
      "- footwear (Confidence: 0.9158928394317627)\n",
      "- trousers (Confidence: 0.9073073863983154)\n",
      "- joint (Confidence: 0.9014332294464111)\n",
      "- man (Confidence: 0.8996748328208923)\n",
      "- table (Confidence: 0.8551928997039795)\n",
      "- standing (Confidence: 0.7025822401046753)\n",
      "- floor (Confidence: 0.7020223140716553)\n",
      "- pants (Confidence: 0.5717838406562805)\n",
      "Dense Captions:\n",
      "- a man in white shirt and white pants standing in front of a mirror (Confidence: 0.7741110920906067)\n",
      "- a man wearing white pants (Confidence: 0.7883489727973938)\n",
      "- a wooden leg next to a chair (Confidence: 0.7015265226364136)\n",
      "- a white stool with a hole in the middle (Confidence: 0.600471019744873)\n",
      "- a person's shadow on the wall (Confidence: 0.6498269438743591)\n",
      "- a white curtain with a brown border (Confidence: 0.5506270527839661)\n",
      "- a close up of a pair of shoes (Confidence: 0.7619830965995789)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 23.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in a green shirt and blue jeans', 'confidence': 0.72101229429245}, 'denseCaptionsResult': {'values': [{'text': 'a man in a green shirt and blue jeans', 'confidence': 0.7211149334907532, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a person with his hands in his pockets', 'confidence': 0.8088062405586243, 'boundingBox': {'x': 49, 'y': 0, 'w': 264, 'h': 761}}, {'text': 'a close up of a pair of white shoes', 'confidence': 0.7786448001861572, 'boundingBox': {'x': 135, 'y': 674, 'w': 147, 'h': 99}}, {'text': 'a man wearing a necklace', 'confidence': 0.7476472854614258, 'boundingBox': {'x': 45, 'y': 0, 'w': 267, 'h': 300}}, {'text': 'a person wearing jeans', 'confidence': 0.6658124327659607, 'boundingBox': {'x': 92, 'y': 254, 'w': 195, 'h': 450}}, {'text': 'a person wearing jeans and white shoes', 'confidence': 0.7561767101287842, 'boundingBox': {'x': 0, 'y': 298, 'w': 393, 'h': 481}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'person', 'confidence': 0.9952824115753174}, {'name': 'clothing', 'confidence': 0.9900434613227844}, {'name': 'footwear', 'confidence': 0.976939857006073}, {'name': 'trousers', 'confidence': 0.9750921130180359}, {'name': 'jeans', 'confidence': 0.9725151062011719}, {'name': 'denim', 'confidence': 0.9459222555160522}, {'name': 'pocket', 'confidence': 0.9369151592254639}, {'name': 'casual dress', 'confidence': 0.9279735088348389}, {'name': 'sleeve', 'confidence': 0.9252532720565796}, {'name': 'outerwear', 'confidence': 0.9153743386268616}, {'name': 'man', 'confidence': 0.893600583076477}, {'name': 'top', 'confidence': 0.8899530172348022}, {'name': 'trouser', 'confidence': 0.8273889422416687}, {'name': 'ground', 'confidence': 0.8191363215446472}, {'name': 'standing', 'confidence': 0.8068674802780151}, {'name': 'wall', 'confidence': 0.7845536470413208}, {'name': 'indoor', 'confidence': 0.6677992343902588}, {'name': 'wearing', 'confidence': 0.6012066602706909}, {'name': 'jean', 'confidence': 0.44965824484825134}, {'name': 'fashion', 'confidence': 0.4449448883533478}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 91, 'y': 296, 'w': 205, 'h': 459}, 'tags': [{'name': 'Jeans', 'confidence': 0.681}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 53, 'y': 0, 'w': 272, 'h': 773}, 'confidence': 0.95146244764328}, {'boundingBox': {'x': 75, 'y': 262, 'w': 231, 'h': 208}, 'confidence': 0.002008180832490325}]}}\n",
      "Caption: a man in a green shirt and blue jeans (Confidence: 0.72101229429245)\n",
      "Tags:\n",
      "- person (Confidence: 0.9952824115753174)\n",
      "- clothing (Confidence: 0.9900434613227844)\n",
      "- footwear (Confidence: 0.976939857006073)\n",
      "- trousers (Confidence: 0.9750921130180359)\n",
      "- jeans (Confidence: 0.9725151062011719)\n",
      "- denim (Confidence: 0.9459222555160522)\n",
      "- pocket (Confidence: 0.9369151592254639)\n",
      "- casual dress (Confidence: 0.9279735088348389)\n",
      "- sleeve (Confidence: 0.9252532720565796)\n",
      "- outerwear (Confidence: 0.9153743386268616)\n",
      "- man (Confidence: 0.893600583076477)\n",
      "- top (Confidence: 0.8899530172348022)\n",
      "- trouser (Confidence: 0.8273889422416687)\n",
      "- ground (Confidence: 0.8191363215446472)\n",
      "- standing (Confidence: 0.8068674802780151)\n",
      "- wall (Confidence: 0.7845536470413208)\n",
      "- indoor (Confidence: 0.6677992343902588)\n",
      "- wearing (Confidence: 0.6012066602706909)\n",
      "- jean (Confidence: 0.44965824484825134)\n",
      "- fashion (Confidence: 0.4449448883533478)\n",
      "Dense Captions:\n",
      "- a man in a green shirt and blue jeans (Confidence: 0.7211149334907532)\n",
      "- a person with his hands in his pockets (Confidence: 0.8088062405586243)\n",
      "- a close up of a pair of white shoes (Confidence: 0.7786448001861572)\n",
      "- a man wearing a necklace (Confidence: 0.7476472854614258)\n",
      "- a person wearing jeans (Confidence: 0.6658124327659607)\n",
      "- a person wearing jeans and white shoes (Confidence: 0.7561767101287842)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 3.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in a black suit', 'confidence': 0.8617950677871704}, 'denseCaptionsResult': {'values': [{'text': 'a man in a black suit', 'confidence': 0.8616803884506226, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man in a suit', 'confidence': 0.8108239769935608, 'boundingBox': {'x': 126, 'y': 55, 'w': 142, 'h': 709}}, {'text': 'a close up of a pair of black shoes', 'confidence': 0.7323178648948669, 'boundingBox': {'x': 136, 'y': 706, 'w': 94, 'h': 60}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9973741173744202}, {'name': 'suit', 'confidence': 0.9913375377655029}, {'name': 'person', 'confidence': 0.9885080456733704}, {'name': 'blazer', 'confidence': 0.9585599899291992}, {'name': 'standing', 'confidence': 0.9482458233833313}, {'name': 'formal wear', 'confidence': 0.9341282844543457}, {'name': 'collar', 'confidence': 0.921428918838501}, {'name': 'man', 'confidence': 0.9203108549118042}, {'name': 'gentleman', 'confidence': 0.9139178991317749}, {'name': 'suit trousers', 'confidence': 0.9009290933609009}, {'name': 'trousers', 'confidence': 0.8841636180877686}, {'name': 'footwear', 'confidence': 0.8678570985794067}, {'name': 'outerwear', 'confidence': 0.8649723529815674}, {'name': 'pocket', 'confidence': 0.8625030517578125}, {'name': 'sleeve', 'confidence': 0.8415034413337708}, {'name': 'fashion', 'confidence': 0.7541985511779785}, {'name': 'wearing', 'confidence': 0.6347800493240356}, {'name': 'menswear', 'confidence': 0.4331973195075989}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 125, 'y': 43, 'w': 148, 'h': 730}, 'confidence': 0.8570912480354309}]}}\n",
      "Caption: a man in a black suit (Confidence: 0.8617950677871704)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9973741173744202)\n",
      "- suit (Confidence: 0.9913375377655029)\n",
      "- person (Confidence: 0.9885080456733704)\n",
      "- blazer (Confidence: 0.9585599899291992)\n",
      "- standing (Confidence: 0.9482458233833313)\n",
      "- formal wear (Confidence: 0.9341282844543457)\n",
      "- collar (Confidence: 0.921428918838501)\n",
      "- man (Confidence: 0.9203108549118042)\n",
      "- gentleman (Confidence: 0.9139178991317749)\n",
      "- suit trousers (Confidence: 0.9009290933609009)\n",
      "- trousers (Confidence: 0.8841636180877686)\n",
      "- footwear (Confidence: 0.8678570985794067)\n",
      "- outerwear (Confidence: 0.8649723529815674)\n",
      "- pocket (Confidence: 0.8625030517578125)\n",
      "- sleeve (Confidence: 0.8415034413337708)\n",
      "- fashion (Confidence: 0.7541985511779785)\n",
      "- wearing (Confidence: 0.6347800493240356)\n",
      "- menswear (Confidence: 0.4331973195075989)\n",
      "Dense Captions:\n",
      "- a man in a black suit (Confidence: 0.8616803884506226)\n",
      "- a man in a suit (Confidence: 0.8108239769935608)\n",
      "- a close up of a pair of black shoes (Confidence: 0.7323178648948669)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 4.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man standing with his hands in his pockets', 'confidence': 0.8229567408561707}, 'denseCaptionsResult': {'values': [{'text': 'a man standing with his hands in his pockets', 'confidence': 0.8231262564659119, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man in a pink shirt and white pants', 'confidence': 0.7539868354797363, 'boundingBox': {'x': 110, 'y': 14, 'w': 165, 'h': 760}}, {'text': 'a man in a pink shirt', 'confidence': 0.8183074593544006, 'boundingBox': {'x': 111, 'y': 123, 'w': 165, 'h': 208}}, {'text': 'a close up of a pair of boots', 'confidence': 0.8018085956573486, 'boundingBox': {'x': 176, 'y': 728, 'w': 95, 'h': 54}}, {'text': \"a close up of a person's legs\", 'confidence': 0.7844464778900146, 'boundingBox': {'x': 133, 'y': 314, 'w': 136, 'h': 449}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9994328618049622}, {'name': 'person', 'confidence': 0.9904530048370361}, {'name': 'trousers', 'confidence': 0.9751688241958618}, {'name': 'shirt', 'confidence': 0.9458872079849243}, {'name': 'trouser', 'confidence': 0.9301143884658813}, {'name': 'pocket', 'confidence': 0.9209855794906616}, {'name': 'casual dress', 'confidence': 0.9181965589523315}, {'name': 'sleeve', 'confidence': 0.9057955741882324}, {'name': 'fabric', 'confidence': 0.9035808444023132}, {'name': 'top', 'confidence': 0.8730406165122986}, {'name': 'collar', 'confidence': 0.8704959154129028}, {'name': 'shoulder', 'confidence': 0.8410294651985168}, {'name': 'pants', 'confidence': 0.6110646724700928}, {'name': 'standing', 'confidence': 0.6070533394813538}, {'name': 'fashion', 'confidence': 0.5477150678634644}, {'name': 'indoor', 'confidence': 0.5269810557365417}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 110, 'y': 37, 'w': 172, 'h': 473}, 'tags': [{'name': 'person', 'confidence': 0.671}]}, {'boundingBox': {'x': 130, 'y': 295, 'w': 156, 'h': 476}, 'tags': [{'name': 'Jeans', 'confidence': 0.584}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 111, 'y': 20, 'w': 175, 'h': 766}, 'confidence': 0.8369060754776001}, {'boundingBox': {'x': 169, 'y': 0, 'w': 55, 'h': 15}, 'confidence': 0.036125894635915756}, {'boundingBox': {'x': 210, 'y': 0, 'w': 63, 'h': 15}, 'confidence': 0.023617412894964218}, {'boundingBox': {'x': 117, 'y': 294, 'w': 153, 'h': 207}, 'confidence': 0.00196420238353312}]}}\n",
      "Caption: a man standing with his hands in his pockets (Confidence: 0.8229567408561707)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9994328618049622)\n",
      "- person (Confidence: 0.9904530048370361)\n",
      "- trousers (Confidence: 0.9751688241958618)\n",
      "- shirt (Confidence: 0.9458872079849243)\n",
      "- trouser (Confidence: 0.9301143884658813)\n",
      "- pocket (Confidence: 0.9209855794906616)\n",
      "- casual dress (Confidence: 0.9181965589523315)\n",
      "- sleeve (Confidence: 0.9057955741882324)\n",
      "- fabric (Confidence: 0.9035808444023132)\n",
      "- top (Confidence: 0.8730406165122986)\n",
      "- collar (Confidence: 0.8704959154129028)\n",
      "- shoulder (Confidence: 0.8410294651985168)\n",
      "- pants (Confidence: 0.6110646724700928)\n",
      "- standing (Confidence: 0.6070533394813538)\n",
      "- fashion (Confidence: 0.5477150678634644)\n",
      "- indoor (Confidence: 0.5269810557365417)\n",
      "Dense Captions:\n",
      "- a man standing with his hands in his pockets (Confidence: 0.8231262564659119)\n",
      "- a man in a pink shirt and white pants (Confidence: 0.7539868354797363)\n",
      "- a man in a pink shirt (Confidence: 0.8183074593544006)\n",
      "- a close up of a pair of boots (Confidence: 0.8018085956573486)\n",
      "- a close up of a person's legs (Confidence: 0.7844464778900146)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 5.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in a suit', 'confidence': 0.8640285730361938}, 'denseCaptionsResult': {'values': [{'text': 'a man in a suit', 'confidence': 0.8640285730361938, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man in a suit', 'confidence': 0.819415271282196, 'boundingBox': {'x': 137, 'y': 54, 'w': 168, 'h': 731}}, {'text': 'a close up of a pair of black shoes', 'confidence': 0.7681585550308228, 'boundingBox': {'x': 166, 'y': 730, 'w': 68, 'h': 65}}, {'text': \"a person's legs in a pair of pants\", 'confidence': 0.7270292639732361, 'boundingBox': {'x': 145, 'y': 350, 'w': 115, 'h': 396}}, {'text': \"a close up of a man's shirt\", 'confidence': 0.7801890969276428, 'boundingBox': {'x': 168, 'y': 159, 'w': 68, 'h': 205}}, {'text': 'a man with brown hair', 'confidence': 0.6836757063865662, 'boundingBox': {'x': 170, 'y': 45, 'w': 68, 'h': 118}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9990206956863403}, {'name': 'person', 'confidence': 0.9930124282836914}, {'name': 'suit', 'confidence': 0.9744380712509155}, {'name': 'blazer', 'confidence': 0.9525253176689148}, {'name': 'man', 'confidence': 0.9475572109222412}, {'name': 'wall', 'confidence': 0.9456985592842102}, {'name': 'collar', 'confidence': 0.9429130554199219}, {'name': 'trousers', 'confidence': 0.9360318183898926}, {'name': 'standing', 'confidence': 0.934307336807251}, {'name': 'necktie', 'confidence': 0.9318840503692627}, {'name': 'formal wear', 'confidence': 0.9301879405975342}, {'name': 'pocket', 'confidence': 0.922856330871582}, {'name': 'gentleman', 'confidence': 0.9120960831642151}, {'name': 'suit trousers', 'confidence': 0.9112876653671265}, {'name': 'outerwear', 'confidence': 0.9041565656661987}, {'name': 'indoor', 'confidence': 0.888945460319519}, {'name': 'dress shirt', 'confidence': 0.8834800124168396}, {'name': 'footwear', 'confidence': 0.8785079717636108}, {'name': 'top', 'confidence': 0.8537788987159729}, {'name': 'fashion design', 'confidence': 0.84981369972229}, {'name': 'button', 'confidence': 0.8493918180465698}, {'name': 'fashion', 'confidence': 0.84523606300354}, {'name': 'wearing', 'confidence': 0.7620233297348022}, {'name': 'menswear', 'confidence': 0.6808153390884399}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 135, 'y': 69, 'w': 168, 'h': 486}, 'tags': [{'name': 'person', 'confidence': 0.759}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 139, 'y': 48, 'w': 174, 'h': 749}, 'confidence': 0.9000834226608276}, {'boundingBox': {'x': 151, 'y': 302, 'w': 156, 'h': 207}, 'confidence': 0.0018476296681910753}]}}\n",
      "Caption: a man in a suit (Confidence: 0.8640285730361938)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9990206956863403)\n",
      "- person (Confidence: 0.9930124282836914)\n",
      "- suit (Confidence: 0.9744380712509155)\n",
      "- blazer (Confidence: 0.9525253176689148)\n",
      "- man (Confidence: 0.9475572109222412)\n",
      "- wall (Confidence: 0.9456985592842102)\n",
      "- collar (Confidence: 0.9429130554199219)\n",
      "- trousers (Confidence: 0.9360318183898926)\n",
      "- standing (Confidence: 0.934307336807251)\n",
      "- necktie (Confidence: 0.9318840503692627)\n",
      "- formal wear (Confidence: 0.9301879405975342)\n",
      "- pocket (Confidence: 0.922856330871582)\n",
      "- gentleman (Confidence: 0.9120960831642151)\n",
      "- suit trousers (Confidence: 0.9112876653671265)\n",
      "- outerwear (Confidence: 0.9041565656661987)\n",
      "- indoor (Confidence: 0.888945460319519)\n",
      "- dress shirt (Confidence: 0.8834800124168396)\n",
      "- footwear (Confidence: 0.8785079717636108)\n",
      "- top (Confidence: 0.8537788987159729)\n",
      "- fashion design (Confidence: 0.84981369972229)\n",
      "- button (Confidence: 0.8493918180465698)\n",
      "- fashion (Confidence: 0.84523606300354)\n",
      "- wearing (Confidence: 0.7620233297348022)\n",
      "- menswear (Confidence: 0.6808153390884399)\n",
      "Dense Captions:\n",
      "- a man in a suit (Confidence: 0.8640285730361938)\n",
      "- a man in a suit (Confidence: 0.819415271282196)\n",
      "- a close up of a pair of black shoes (Confidence: 0.7681585550308228)\n",
      "- a person's legs in a pair of pants (Confidence: 0.7270292639732361)\n",
      "- a close up of a man's shirt (Confidence: 0.7801890969276428)\n",
      "- a man with brown hair (Confidence: 0.6836757063865662)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 6.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in a suit', 'confidence': 0.7820921540260315}, 'denseCaptionsResult': {'values': [{'text': 'a man in a suit', 'confidence': 0.7820921540260315, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man wearing a suit and tie', 'confidence': 0.702163815498352, 'boundingBox': {'x': 39, 'y': 0, 'w': 305, 'h': 784}}, {'text': 'a man in a suit', 'confidence': 0.839548647403717, 'boundingBox': {'x': 0, 'y': 362, 'w': 388, 'h': 426}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'person', 'confidence': 0.996666431427002}, {'name': 'clothing', 'confidence': 0.991281270980835}, {'name': 'outdoor', 'confidence': 0.9756213426589966}, {'name': 'man', 'confidence': 0.9740966558456421}, {'name': 'suit', 'confidence': 0.9720923900604248}, {'name': 'blazer', 'confidence': 0.9334625601768494}, {'name': 'gentleman', 'confidence': 0.9145369529724121}, {'name': 'formal wear', 'confidence': 0.9129096269607544}, {'name': 'collar', 'confidence': 0.907935380935669}, {'name': 'shirt', 'confidence': 0.8979162573814392}, {'name': 'fashion accessory', 'confidence': 0.8794976472854614}, {'name': 'outerwear', 'confidence': 0.8585892915725708}, {'name': 'dress shirt', 'confidence': 0.8537249565124512}, {'name': 'coat', 'confidence': 0.850587785243988}, {'name': 'standing', 'confidence': 0.7987867593765259}, {'name': 'wearing', 'confidence': 0.7985002994537354}, {'name': 'white', 'confidence': 0.7085981965065002}, {'name': 'ground', 'confidence': 0.5942603945732117}, {'name': 'fashion', 'confidence': 0.5065504312515259}]}, 'objectsResult': {'values': []}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 43, 'y': 0, 'w': 323, 'h': 799}, 'confidence': 0.933403730392456}, {'boundingBox': {'x': 64, 'y': 294, 'w': 272, 'h': 203}, 'confidence': 0.0023040398955345154}, {'boundingBox': {'x': 331, 'y': 190, 'w': 68, 'h': 189}, 'confidence': 0.0010410323739051819}]}}\n",
      "Caption: a man in a suit (Confidence: 0.7820921540260315)\n",
      "Tags:\n",
      "- person (Confidence: 0.996666431427002)\n",
      "- clothing (Confidence: 0.991281270980835)\n",
      "- outdoor (Confidence: 0.9756213426589966)\n",
      "- man (Confidence: 0.9740966558456421)\n",
      "- suit (Confidence: 0.9720923900604248)\n",
      "- blazer (Confidence: 0.9334625601768494)\n",
      "- gentleman (Confidence: 0.9145369529724121)\n",
      "- formal wear (Confidence: 0.9129096269607544)\n",
      "- collar (Confidence: 0.907935380935669)\n",
      "- shirt (Confidence: 0.8979162573814392)\n",
      "- fashion accessory (Confidence: 0.8794976472854614)\n",
      "- outerwear (Confidence: 0.8585892915725708)\n",
      "- dress shirt (Confidence: 0.8537249565124512)\n",
      "- coat (Confidence: 0.850587785243988)\n",
      "- standing (Confidence: 0.7987867593765259)\n",
      "- wearing (Confidence: 0.7985002994537354)\n",
      "- white (Confidence: 0.7085981965065002)\n",
      "- ground (Confidence: 0.5942603945732117)\n",
      "- fashion (Confidence: 0.5065504312515259)\n",
      "Dense Captions:\n",
      "- a man in a suit (Confidence: 0.7820921540260315)\n",
      "- a man wearing a suit and tie (Confidence: 0.702163815498352)\n",
      "- a man in a suit (Confidence: 0.839548647403717)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 7.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in a suit', 'confidence': 0.8461505770683289}, 'denseCaptionsResult': {'values': [{'text': 'a man in a suit', 'confidence': 0.8461505770683289, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man in a suit', 'confidence': 0.8403139114379883, 'boundingBox': {'x': 109, 'y': 138, 'w': 159, 'h': 625}}, {'text': 'a close up of a chair', 'confidence': 0.8212524652481079, 'boundingBox': {'x': 0, 'y': 415, 'w': 84, 'h': 357}}, {'text': 'a black object in front of a white wall', 'confidence': 0.7286672592163086, 'boundingBox': {'x': 41, 'y': 163, 'w': 72, 'h': 510}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9973832964897156}, {'name': 'indoor', 'confidence': 0.9903216361999512}, {'name': 'person', 'confidence': 0.9897655844688416}, {'name': 'wall', 'confidence': 0.9836821556091309}, {'name': 'suit', 'confidence': 0.9770491123199463}, {'name': 'man', 'confidence': 0.9404483437538147}, {'name': 'blazer', 'confidence': 0.9391982555389404}, {'name': 'trousers', 'confidence': 0.9342745542526245}, {'name': 'furniture', 'confidence': 0.9317083358764648}, {'name': 'shirt', 'confidence': 0.910835862159729}, {'name': 'formal wear', 'confidence': 0.9074974060058594}, {'name': 'suit trousers', 'confidence': 0.9065719246864319}, {'name': 'collar', 'confidence': 0.8973782062530518}, {'name': 'gentleman', 'confidence': 0.8838005065917969}, {'name': 'pocket', 'confidence': 0.882184624671936}, {'name': 'outerwear', 'confidence': 0.8790204524993896}, {'name': 'dress shirt', 'confidence': 0.8735983371734619}, {'name': 'footwear', 'confidence': 0.846306562423706}, {'name': 'floor', 'confidence': 0.8375529646873474}, {'name': 'room', 'confidence': 0.790534496307373}, {'name': 'standing', 'confidence': 0.7242317199707031}, {'name': 'fashion', 'confidence': 0.7216430902481079}, {'name': 'wearing', 'confidence': 0.6424834728240967}, {'name': 'chair', 'confidence': 0.5273957848548889}, {'name': 'menswear', 'confidence': 0.5170302987098694}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 0, 'y': 407, 'w': 81, 'h': 393}, 'tags': [{'name': 'chair', 'confidence': 0.664}]}, {'boundingBox': {'x': 105, 'y': 152, 'w': 173, 'h': 485}, 'tags': [{'name': 'person', 'confidence': 0.538}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 106, 'y': 132, 'w': 168, 'h': 667}, 'confidence': 0.9379640221595764}, {'boundingBox': {'x': 114, 'y': 374, 'w': 153, 'h': 201}, 'confidence': 0.0021117066498845816}]}}\n",
      "Caption: a man in a suit (Confidence: 0.8461505770683289)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9973832964897156)\n",
      "- indoor (Confidence: 0.9903216361999512)\n",
      "- person (Confidence: 0.9897655844688416)\n",
      "- wall (Confidence: 0.9836821556091309)\n",
      "- suit (Confidence: 0.9770491123199463)\n",
      "- man (Confidence: 0.9404483437538147)\n",
      "- blazer (Confidence: 0.9391982555389404)\n",
      "- trousers (Confidence: 0.9342745542526245)\n",
      "- furniture (Confidence: 0.9317083358764648)\n",
      "- shirt (Confidence: 0.910835862159729)\n",
      "- formal wear (Confidence: 0.9074974060058594)\n",
      "- suit trousers (Confidence: 0.9065719246864319)\n",
      "- collar (Confidence: 0.8973782062530518)\n",
      "- gentleman (Confidence: 0.8838005065917969)\n",
      "- pocket (Confidence: 0.882184624671936)\n",
      "- outerwear (Confidence: 0.8790204524993896)\n",
      "- dress shirt (Confidence: 0.8735983371734619)\n",
      "- footwear (Confidence: 0.846306562423706)\n",
      "- floor (Confidence: 0.8375529646873474)\n",
      "- room (Confidence: 0.790534496307373)\n",
      "- standing (Confidence: 0.7242317199707031)\n",
      "- fashion (Confidence: 0.7216430902481079)\n",
      "- wearing (Confidence: 0.6424834728240967)\n",
      "- chair (Confidence: 0.5273957848548889)\n",
      "- menswear (Confidence: 0.5170302987098694)\n",
      "Dense Captions:\n",
      "- a man in a suit (Confidence: 0.8461505770683289)\n",
      "- a man in a suit (Confidence: 0.8403139114379883)\n",
      "- a close up of a chair (Confidence: 0.8212524652481079)\n",
      "- a black object in front of a white wall (Confidence: 0.7286672592163086)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 8.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in a suit', 'confidence': 0.881773054599762}, 'denseCaptionsResult': {'values': [{'text': 'a man in a suit', 'confidence': 0.8818448185920715, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man in a suit', 'confidence': 0.8662225604057312, 'boundingBox': {'x': 125, 'y': 75, 'w': 210, 'h': 714}}, {'text': 'a person wearing black pants', 'confidence': 0.6719877123832703, 'boundingBox': {'x': 0, 'y': 238, 'w': 68, 'h': 338}}, {'text': \"a close-up of a man's legs\", 'confidence': 0.7995632290840149, 'boundingBox': {'x': 133, 'y': 472, 'w': 155, 'h': 321}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'person', 'confidence': 0.9984865784645081}, {'name': 'clothing', 'confidence': 0.9972033500671387}, {'name': 'building', 'confidence': 0.9913473129272461}, {'name': 'coat', 'confidence': 0.9761331081390381}, {'name': 'man', 'confidence': 0.9705456495285034}, {'name': 'gentleman', 'confidence': 0.9662587642669678}, {'name': 'trousers', 'confidence': 0.9624036550521851}, {'name': 'blazer', 'confidence': 0.9584953188896179}, {'name': 'outdoor', 'confidence': 0.9579280614852905}, {'name': 'tie', 'confidence': 0.9553841352462769}, {'name': 'shirt', 'confidence': 0.9552898406982422}, {'name': 'footwear', 'confidence': 0.9506089687347412}, {'name': 'outerwear', 'confidence': 0.949158787727356}, {'name': 'dress shirt', 'confidence': 0.9466843605041504}, {'name': 'formal wear', 'confidence': 0.9466584920883179}, {'name': 'collar', 'confidence': 0.9249334931373596}, {'name': 'human face', 'confidence': 0.9085783362388611}, {'name': 'pocket', 'confidence': 0.9026728868484497}, {'name': 'male person', 'confidence': 0.882767915725708}, {'name': 'street fashion', 'confidence': 0.8784067630767822}, {'name': 'suit trousers', 'confidence': 0.8626120090484619}, {'name': 'suit', 'confidence': 0.8385573029518127}, {'name': 'street', 'confidence': 0.7347384691238403}, {'name': 'wearing', 'confidence': 0.7346521615982056}, {'name': 'ground', 'confidence': 0.7339761853218079}, {'name': 'standing', 'confidence': 0.6985669136047363}, {'name': 'sidewalk', 'confidence': 0.6323421001434326}, {'name': 'necktie', 'confidence': 0.5931529998779297}, {'name': 'business', 'confidence': 0.5851102471351624}, {'name': 'city', 'confidence': 0.5821739435195923}, {'name': 'fashion', 'confidence': 0.4305209517478943}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 77, 'y': 241, 'w': 46, 'h': 178}, 'tags': [{'name': 'person', 'confidence': 0.581}]}, {'boundingBox': {'x': 197, 'y': 252, 'w': 46, 'h': 232}, 'tags': [{'name': 'tie', 'confidence': 0.648}]}, {'boundingBox': {'x': 0, 'y': 243, 'w': 72, 'h': 333}, 'tags': [{'name': 'person', 'confidence': 0.748}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 128, 'y': 86, 'w': 220, 'h': 713}, 'confidence': 0.932407021522522}, {'boundingBox': {'x': 0, 'y': 234, 'w': 74, 'h': 351}, 'confidence': 0.9048269987106323}, {'boundingBox': {'x': 80, 'y': 244, 'w': 45, 'h': 172}, 'confidence': 0.8057278394699097}, {'boundingBox': {'x': 124, 'y': 229, 'w': 34, 'h': 193}, 'confidence': 0.7596610188484192}, {'boundingBox': {'x': 36, 'y': 244, 'w': 45, 'h': 113}, 'confidence': 0.5372406244277954}, {'boundingBox': {'x': 69, 'y': 236, 'w': 28, 'h': 105}, 'confidence': 0.14718976616859436}, {'boundingBox': {'x': 70, 'y': 236, 'w': 23, 'h': 43}, 'confidence': 0.011311307549476624}, {'boundingBox': {'x': 64, 'y': 239, 'w': 21, 'h': 57}, 'confidence': 0.006017862819135189}, {'boundingBox': {'x': 37, 'y': 244, 'w': 35, 'h': 54}, 'confidence': 0.0051613859832286835}, {'boundingBox': {'x': 141, 'y': 331, 'w': 190, 'h': 217}, 'confidence': 0.003051676321774721}, {'boundingBox': {'x': 53, 'y': 243, 'w': 19, 'h': 33}, 'confidence': 0.0021378023084253073}, {'boundingBox': {'x': 65, 'y': 237, 'w': 18, 'h': 28}, 'confidence': 0.001919430447742343}, {'boundingBox': {'x': 86, 'y': 244, 'w': 27, 'h': 29}, 'confidence': 0.0014134077355265617}, {'boundingBox': {'x': 0, 'y': 233, 'w': 147, 'h': 350}, 'confidence': 0.001027188147418201}]}}\n",
      "Caption: a man in a suit (Confidence: 0.881773054599762)\n",
      "Tags:\n",
      "- person (Confidence: 0.9984865784645081)\n",
      "- clothing (Confidence: 0.9972033500671387)\n",
      "- building (Confidence: 0.9913473129272461)\n",
      "- coat (Confidence: 0.9761331081390381)\n",
      "- man (Confidence: 0.9705456495285034)\n",
      "- gentleman (Confidence: 0.9662587642669678)\n",
      "- trousers (Confidence: 0.9624036550521851)\n",
      "- blazer (Confidence: 0.9584953188896179)\n",
      "- outdoor (Confidence: 0.9579280614852905)\n",
      "- tie (Confidence: 0.9553841352462769)\n",
      "- shirt (Confidence: 0.9552898406982422)\n",
      "- footwear (Confidence: 0.9506089687347412)\n",
      "- outerwear (Confidence: 0.949158787727356)\n",
      "- dress shirt (Confidence: 0.9466843605041504)\n",
      "- formal wear (Confidence: 0.9466584920883179)\n",
      "- collar (Confidence: 0.9249334931373596)\n",
      "- human face (Confidence: 0.9085783362388611)\n",
      "- pocket (Confidence: 0.9026728868484497)\n",
      "- male person (Confidence: 0.882767915725708)\n",
      "- street fashion (Confidence: 0.8784067630767822)\n",
      "- suit trousers (Confidence: 0.8626120090484619)\n",
      "- suit (Confidence: 0.8385573029518127)\n",
      "- street (Confidence: 0.7347384691238403)\n",
      "- wearing (Confidence: 0.7346521615982056)\n",
      "- ground (Confidence: 0.7339761853218079)\n",
      "- standing (Confidence: 0.6985669136047363)\n",
      "- sidewalk (Confidence: 0.6323421001434326)\n",
      "- necktie (Confidence: 0.5931529998779297)\n",
      "- business (Confidence: 0.5851102471351624)\n",
      "- city (Confidence: 0.5821739435195923)\n",
      "- fashion (Confidence: 0.4305209517478943)\n",
      "Dense Captions:\n",
      "- a man in a suit (Confidence: 0.8818448185920715)\n",
      "- a man in a suit (Confidence: 0.8662225604057312)\n",
      "- a person wearing black pants (Confidence: 0.6719877123832703)\n",
      "- a close-up of a man's legs (Confidence: 0.7995632290840149)\n",
      "Analyzing image: D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\M 9.jpg\n",
      "{'modelVersion': '2023-10-01', 'captionResult': {'text': 'a man in a suit and tie', 'confidence': 0.8414282202720642}, 'denseCaptionsResult': {'values': [{'text': 'a man in a suit and tie', 'confidence': 0.8410835862159729, 'boundingBox': {'x': 0, 'y': 0, 'w': 400, 'h': 800}}, {'text': 'a man in a suit and tie', 'confidence': 0.819883406162262, 'boundingBox': {'x': 32, 'y': 14, 'w': 353, 'h': 773}}]}, 'metadata': {'width': 400, 'height': 800}, 'tagsResult': {'values': [{'name': 'clothing', 'confidence': 0.9990479946136475}, {'name': 'person', 'confidence': 0.9952018857002258}, {'name': 'suit', 'confidence': 0.9850441217422485}, {'name': 'collar', 'confidence': 0.979427695274353}, {'name': 'formal wear', 'confidence': 0.977556049823761}, {'name': 'necktie', 'confidence': 0.9727516770362854}, {'name': 'blazer', 'confidence': 0.9725394248962402}, {'name': 'man', 'confidence': 0.9458683729171753}, {'name': 'dress shirt', 'confidence': 0.9349653720855713}, {'name': 'gentleman', 'confidence': 0.9325920343399048}, {'name': 'outerwear', 'confidence': 0.931464433670044}, {'name': 'wall', 'confidence': 0.9289332628250122}, {'name': 'human face', 'confidence': 0.9192281365394592}, {'name': 'tie', 'confidence': 0.9114400744438171}, {'name': 'coat', 'confidence': 0.9093471765518188}, {'name': 'wearing', 'confidence': 0.8966959714889526}, {'name': 'button', 'confidence': 0.8937205672264099}, {'name': 'shirt', 'confidence': 0.8814051151275635}, {'name': 'top', 'confidence': 0.879986047744751}, {'name': 'white-collar worker', 'confidence': 0.86800217628479}, {'name': 'pocket', 'confidence': 0.8576827645301819}, {'name': 'male person', 'confidence': 0.840800404548645}, {'name': 'indoor', 'confidence': 0.8332623243331909}, {'name': 'young', 'confidence': 0.611929714679718}, {'name': 'business', 'confidence': 0.5861786603927612}]}, 'objectsResult': {'values': [{'boundingBox': {'x': 153, 'y': 314, 'w': 52, 'h': 197}, 'tags': [{'name': 'tie', 'confidence': 0.712}]}]}, 'readResult': {'blocks': []}, 'peopleResult': {'values': [{'boundingBox': {'x': 39, 'y': 28, 'w': 360, 'h': 771}, 'confidence': 0.9461219906806946}, {'boundingBox': {'x': 71, 'y': 321, 'w': 305, 'h': 216}, 'confidence': 0.0027501294389367104}]}}\n",
      "Caption: a man in a suit and tie (Confidence: 0.8414282202720642)\n",
      "Tags:\n",
      "- clothing (Confidence: 0.9990479946136475)\n",
      "- person (Confidence: 0.9952018857002258)\n",
      "- suit (Confidence: 0.9850441217422485)\n",
      "- collar (Confidence: 0.979427695274353)\n",
      "- formal wear (Confidence: 0.977556049823761)\n",
      "- necktie (Confidence: 0.9727516770362854)\n",
      "- blazer (Confidence: 0.9725394248962402)\n",
      "- man (Confidence: 0.9458683729171753)\n",
      "- dress shirt (Confidence: 0.9349653720855713)\n",
      "- gentleman (Confidence: 0.9325920343399048)\n",
      "- outerwear (Confidence: 0.931464433670044)\n",
      "- wall (Confidence: 0.9289332628250122)\n",
      "- human face (Confidence: 0.9192281365394592)\n",
      "- tie (Confidence: 0.9114400744438171)\n",
      "- coat (Confidence: 0.9093471765518188)\n",
      "- wearing (Confidence: 0.8966959714889526)\n",
      "- button (Confidence: 0.8937205672264099)\n",
      "- shirt (Confidence: 0.8814051151275635)\n",
      "- top (Confidence: 0.879986047744751)\n",
      "- white-collar worker (Confidence: 0.86800217628479)\n",
      "- pocket (Confidence: 0.8576827645301819)\n",
      "- male person (Confidence: 0.840800404548645)\n",
      "- indoor (Confidence: 0.8332623243331909)\n",
      "- young (Confidence: 0.611929714679718)\n",
      "- business (Confidence: 0.5861786603927612)\n",
      "Dense Captions:\n",
      "- a man in a suit and tie (Confidence: 0.8410835862159729)\n",
      "- a man in a suit and tie (Confidence: 0.819883406162262)\n"
     ]
    }
   ],
   "source": [
    "analyze_images_in_directory(r\"D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965fa1f7",
   "metadata": {},
   "source": [
    "### Mens Tag Generation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e3bfc26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ImageName                                            Caption  \\\n",
      "0   M 10.jpg                             a grey suit with a tie   \n",
      "1   M 14.jpg       a man standing with his hands in his pockets   \n",
      "2   M 15.jpg                            a man in a beige jacket   \n",
      "3   M 18.jpg                      a man with tattoos on his arm   \n",
      "4    M 2.jpg                                    a man in a suit   \n",
      "5   M 20.jpg              a man standing in front of a building   \n",
      "6   M 21.jpg         a man standing with his hand in his pocket   \n",
      "7   M 22.jpg  a man in white shirt and white pants standing ...   \n",
      "8   M 23.jpg              a man in a green shirt and blue jeans   \n",
      "9    M 3.jpg                              a man in a black suit   \n",
      "10   M 4.jpg       a man standing with his hands in his pockets   \n",
      "11   M 5.jpg                                    a man in a suit   \n",
      "12   M 6.jpg                                    a man in a suit   \n",
      "13   M 7.jpg                                    a man in a suit   \n",
      "14   M 8.jpg                                    a man in a suit   \n",
      "15   M 9.jpg                            a man in a suit and tie   \n",
      "\n",
      "    CaptionConfidence                                               Tags  \\\n",
      "0            0.669489  [clothing, coat, blazer, collar, outerwear, bu...   \n",
      "1            0.838266  [clothing, person, outerwear, trousers, jacket...   \n",
      "2            0.715549  [person, clothing, shirt, jeans, collar, trous...   \n",
      "3            0.697496  [clothing, person, footwear, trousers, man, sl...   \n",
      "4            0.798101  [trousers, formal wear, blazer, gentleman, whi...   \n",
      "5            0.789326  [person, clothing, trousers, man, pocket, oute...   \n",
      "6            0.811726  [trousers, man, footwear, casual dress, brown ...   \n",
      "7            0.774163  [person, clothing, footwear, trousers, pants, ...   \n",
      "8            0.721012  [footwear, trousers, jeans, denim, green shirt...   \n",
      "9            0.861795  [suit, blazer, formal wear, trousers, footwear...   \n",
      "10           0.822957  [trousers, shirt, casual dress, pants, pink sh...   \n",
      "11           0.864029  [suit, blazer, trousers, necktie, formal wear,...   \n",
      "12           0.782092  [person, suit, blazer, gentleman, formal wear,...   \n",
      "13           0.846151  [clothing, suit, formal wear, blazer, trousers...   \n",
      "14           0.881773  [person, trousers, blazer, outdoor, tie, shirt...   \n",
      "15           0.841428  [clothing, person, suit, formal wear, necktie,...   \n",
      "\n",
      "                                       TagConfidences  \\\n",
      "0   [1.0, 0.971, 0.957, 0.955, 0.955, 0.946, 0.945...   \n",
      "1   [0.998, 0.99, 0.954, 0.935, 0.933, 0.927, 0.92...   \n",
      "2   [0.994, 0.992, 0.976, 0.974, 0.958, 0.954, 0.9...   \n",
      "3   [0.997, 0.996, 0.994, 0.957, 0.953, 0.917, 0.8...   \n",
      "4   [0.98, 0.934, 0.897, 0.894, 0.89, 0.888, 0.886...   \n",
      "5   [0.997, 0.995, 0.976, 0.972, 0.949, 0.944, 0.8...   \n",
      "6            [0.959, 0.912, 0.9, 0.897, 0.708, 0.708]   \n",
      "7   [0.997, 0.993, 0.916, 0.907, 0.572, 0.788, 0.7...   \n",
      "8   [0.977, 0.975, 0.973, 0.946, 0.721, 0.721, 0.9...   \n",
      "9   [0.991, 0.959, 0.934, 0.884, 0.868, 0.862, 0.732]   \n",
      "10         [0.975, 0.946, 0.918, 0.611, 0.818, 0.754]   \n",
      "11   [0.974, 0.953, 0.936, 0.932, 0.93, 0.768, 0.684]   \n",
      "12  [0.997, 0.972, 0.933, 0.915, 0.913, 0.851, 0.7...   \n",
      "13         [0.997, 0.977, 0.923, 0.939, 0.934, 0.846]   \n",
      "14  [0.998, 0.962, 0.958, 0.958, 0.955, 0.955, 0.9...   \n",
      "15  [0.999, 0.995, 0.985, 0.978, 0.973, 0.973, 0.9...   \n",
      "\n",
      "                                     TagConfidenceMap  \n",
      "0   {'clothing': 1.0, 'coat': 0.971, 'blazer': 0.9...  \n",
      "1   {'clothing': 0.998, 'person': 0.99, 'outerwear...  \n",
      "2   {'person': 0.994, 'clothing': 0.992, 'shirt': ...  \n",
      "3   {'clothing': 0.997, 'person': 0.996, 'footwear...  \n",
      "4   {'trousers': 0.98, 'formal wear': 0.934, 'blaz...  \n",
      "5   {'person': 0.997, 'clothing': 0.995, 'trousers...  \n",
      "6   {'trousers': 0.959, 'man': 0.912, 'footwear': ...  \n",
      "7   {'person': 0.997, 'clothing': 0.993, 'footwear...  \n",
      "8   {'footwear': 0.977, 'trousers': 0.975, 'jeans'...  \n",
      "9   {'suit': 0.991, 'blazer': 0.959, 'formal wear'...  \n",
      "10  {'trousers': 0.975, 'shirt': 0.946, 'casual dr...  \n",
      "11  {'suit': 0.974, 'blazer': 0.953, 'trousers': 0...  \n",
      "12  {'person': 0.997, 'suit': 0.972, 'blazer': 0.9...  \n",
      "13  {'clothing': 0.997, 'suit': 0.977, 'formal wea...  \n",
      "14  {'person': 0.998, 'trousers': 0.962, 'blazer':...  \n",
      "15  {'clothing': 0.999, 'person': 0.995, 'suit': 0...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from pathlib import Path, PureWindowsPath\n",
    "\n",
    "file_path = Path(r\"D:\\Dinesh\\Tech\\GitHub\\virtual-assistant\\images\\American\\resized_images\\men\\Men_Tags\")  # Change this to your actual file path\n",
    "\n",
    "data = []\n",
    "current = {}\n",
    "\n",
    "def extract_confidence(text):\n",
    "    match = re.search(r\"\\(Confidence:\\s*([0-9.]+)\\)\", text)\n",
    "    return float(match.group(1)) if match else None\n",
    "\n",
    "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        stripped = line.strip()\n",
    "\n",
    "        if stripped.startswith(\"Analyzing image:\"):\n",
    "            if current:\n",
    "                data.append(current)\n",
    "            current = {\n",
    "                \"ImagePath\": stripped.replace(\"Analyzing image:\", \"\").strip(),\n",
    "                \"Caption\": \"\",\n",
    "                \"CaptionConfidence\": None,\n",
    "                \"Tags\": [],\n",
    "                \"TagConfidences\": []\n",
    "            }\n",
    "\n",
    "        elif stripped.startswith(\"Caption:\"):\n",
    "            caption_line = stripped.replace(\"Caption:\", \"\").strip()\n",
    "            current[\"CaptionConfidence\"] = extract_confidence(caption_line)\n",
    "            current[\"Caption\"] = caption_line[:caption_line.rfind(\"(\")].strip() if \"(\" in caption_line else caption_line\n",
    "\n",
    "        elif stripped.startswith(\"-\"):\n",
    "            tag_line = stripped[1:].strip()\n",
    "            confidence = extract_confidence(tag_line)\n",
    "            tag = tag_line[:tag_line.rfind(\"(\")].strip() if \"(\" in tag_line else tag_line\n",
    "            current[\"Tags\"].append(tag)\n",
    "            current[\"TagConfidences\"].append(round(confidence, 3) if confidence else None)\n",
    "\n",
    "# Add last parsed item\n",
    "if current:\n",
    "    data.append(current)\n",
    "\n",
    "# Add TagConfidenceMap and ImageName\n",
    "for entry in data:\n",
    "    entry[\"TagConfidenceMap\"] = {\n",
    "        tag: conf for tag, conf in zip(entry[\"Tags\"], entry[\"TagConfidences\"])\n",
    "    }\n",
    "    entry[\"ImageName\"] = PureWindowsPath(entry[\"ImagePath\"]).name\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Optional: Save to file\n",
    "# df.to_csv(\"parsed_image_tags.csv\", index=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b7bb821",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ImageName                                            Caption  \\\n",
      "0   M 10.jpg                             a grey suit with a tie   \n",
      "1   M 14.jpg       a man standing with his hands in his pockets   \n",
      "2   M 15.jpg                            a man in a beige jacket   \n",
      "3   M 18.jpg                      a man with tattoos on his arm   \n",
      "4    M 2.jpg                                    a man in a suit   \n",
      "5   M 20.jpg              a man standing in front of a building   \n",
      "6   M 21.jpg         a man standing with his hand in his pocket   \n",
      "7   M 22.jpg  a man in white shirt and white pants standing ...   \n",
      "8   M 23.jpg              a man in a green shirt and blue jeans   \n",
      "9    M 3.jpg                              a man in a black suit   \n",
      "10   M 4.jpg       a man standing with his hands in his pockets   \n",
      "11   M 5.jpg                                    a man in a suit   \n",
      "12   M 6.jpg                                    a man in a suit   \n",
      "13   M 7.jpg                                    a man in a suit   \n",
      "14   M 8.jpg                                    a man in a suit   \n",
      "15   M 9.jpg                            a man in a suit and tie   \n",
      "\n",
      "    CaptionConfidence                                               Tags  \\\n",
      "0            0.669489  [clothing, coat, blazer, collar, outerwear, bu...   \n",
      "1            0.838266  [clothing, person, outerwear, trousers, jacket...   \n",
      "2            0.715549  [person, clothing, shirt, jeans, collar, trous...   \n",
      "3            0.697496  [clothing, person, footwear, trousers, man, sl...   \n",
      "4            0.798101  [trousers, formal wear, blazer, gentleman, whi...   \n",
      "5            0.789326  [person, clothing, trousers, man, pocket, oute...   \n",
      "6            0.811726  [trousers, man, footwear, casual dress, brown ...   \n",
      "7            0.774163  [person, clothing, footwear, trousers, pants, ...   \n",
      "8            0.721012  [footwear, trousers, jeans, denim, green shirt...   \n",
      "9            0.861795  [suit, blazer, formal wear, trousers, footwear...   \n",
      "10           0.822957  [trousers, shirt, casual dress, pants, pink sh...   \n",
      "11           0.864029  [suit, blazer, trousers, necktie, formal wear,...   \n",
      "12           0.782092  [person, suit, blazer, gentleman, formal wear,...   \n",
      "13           0.846151  [clothing, suit, formal wear, blazer, trousers...   \n",
      "14           0.881773  [person, trousers, blazer, outdoor, tie, shirt...   \n",
      "15           0.841428  [clothing, person, suit, formal wear, necktie,...   \n",
      "\n",
      "                                     TagConfidenceMap  \n",
      "0   {'clothing': 1.0, 'coat': 0.971, 'blazer': 0.9...  \n",
      "1   {'clothing': 0.998, 'person': 0.99, 'outerwear...  \n",
      "2   {'person': 0.994, 'clothing': 0.992, 'shirt': ...  \n",
      "3   {'clothing': 0.997, 'person': 0.996, 'footwear...  \n",
      "4   {'trousers': 0.98, 'formal wear': 0.934, 'blaz...  \n",
      "5   {'person': 0.997, 'clothing': 0.995, 'trousers...  \n",
      "6   {'trousers': 0.959, 'man': 0.912, 'footwear': ...  \n",
      "7   {'person': 0.997, 'clothing': 0.993, 'footwear...  \n",
      "8   {'footwear': 0.977, 'trousers': 0.975, 'jeans'...  \n",
      "9   {'suit': 0.991, 'blazer': 0.959, 'formal wear'...  \n",
      "10  {'trousers': 0.975, 'shirt': 0.946, 'casual dr...  \n",
      "11  {'suit': 0.974, 'blazer': 0.953, 'trousers': 0...  \n",
      "12  {'person': 0.997, 'suit': 0.972, 'blazer': 0.9...  \n",
      "13  {'clothing': 0.997, 'suit': 0.977, 'formal wea...  \n",
      "14  {'person': 0.998, 'trousers': 0.962, 'blazer':...  \n",
      "15  {'clothing': 0.999, 'person': 0.995, 'suit': 0...  \n"
     ]
    }
   ],
   "source": [
    "print(df[[\"ImageName\", \"Caption\", \"CaptionConfidence\", \"Tags\", \"TagConfidenceMap\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078b1ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
